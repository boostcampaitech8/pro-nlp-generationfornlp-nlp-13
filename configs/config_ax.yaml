model:
  model_name_or_path: "skt/A.X-4.0-Light"
  use_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  compute_dtype: "float16"
  device_map: "auto"
  use_gradient_checkpointing: true
  trust_remote_code: true

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: "all-linear"
  bias: "none"
  task_type: "CAUSAL_LM"

trainer:
  output_dir: "./models/A.X-4.0-Light"
  max_seq_length: 2048
  packing: false

  num_train_epochs: 4
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4

  learning_rate: 0.0002
  lr_scheduler_type: "cosine"
  optim: "paged_adamw_32bit"
  weight_decay: 0.01
  warmup_ratio: 0.05
  max_grad_norm: 1.0

  fp16: true
  bf16: false

  eval_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 10
  eval_steps: 40
  save_steps: 40

  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: "eval_macro_f1"
  greater_is_better: true

  report_to: "wandb"
  seed: 42
  remove_unused_columns: true

token_format:
  response_template: "<|im_start|><|assistant|>"
  label_pos_from_tail: 2
  logit_pos_from_tail: 3

data:
  train_path: "./data/train.csv"
  train_augment_paths:
    - "./data/train_augment_LSAT_cut2.csv"
    - "./data/train_augment_SAT_4.csv"
    - "./data/train_augment_korea_history.csv"
  test_path: "./data/test.csv"
  valid_ratio: 0.1
  seed: 42
  do_split: true

prompt:
  templates_dir: "./src/prompt/templates"
  verbose: false
  policy:
    system:
      4: "expand"
      5: "v1"
    user:
      4: "eng"
      5: "v1"

tokenizer:
  train:
    max_length: 2048
    padding: false
    truncation: true
    add_generation_prompt: false
  gen:
    max_length: 2048
    padding: "max_length"
    truncation: true
    add_generation_prompt: true

wandb:
  enabled: true
  run_name: "A.X-4.0-Light-basic"
  project_name: "A.X-4.0-Light"
  entity: "pro-nlp-generationfornlp-nlp-13"
  dir: "./wandb"

inference:
  model:
    use_gradient_checkpointing: false

  adapter_path: "./models/A.X-4.0-Light/final_model"
  test_data_path: "./data/test.csv"
  output_path: "./submission_ax_v1.csv"
  output_logits_path : "./ax_probs_v1.csv"
  max_new_tokens: 30
  do_sample: false
  retry_quantile: 0.25

  prompt:
    policy:
      system:
        4: "expand"
        5: "v1"
      user:
        4: "eng"
        5: "v1"
  retry_prompt:
    policy:
      system:
        4: "retryv1"
        5: "retryv1"
      user:
        4: "retryv1"
        5: "retryv1"

dpo:
  sft_adapter_path: "./models/qwen3_14B_eng_aug2/final_model"

  trainer:
    output_dir: "./models/qwen3_14B_eng_aug_dpo"

    num_train_epochs: 2
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 4

    learning_rate: 0.000005
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1

    beta: 0.1
    label_smoothing: 0.0
    loss_type: "sigmoid"

    fp16: true
    bf16: false

    eval_strategy: "steps"
    save_strategy: "steps"
    logging_steps: 20
    eval_steps: 60
    save_steps: 60

    load_best_model_at_end: true
    metric_for_best_model: "eval_rewards/accuracies"
    greater_is_better: true
    save_total_limit: 2

    max_length: 2048
    max_prompt_length: 1820
    max_completion_length: 30

    remove_unused_columns: false
    report_to: "wandb"
    seed: 42

  data:
    train_path: "./data/dpo_train.jsonl"
    eval_path: "./data/dpo_eval.jsonl"
    seed: 42
    csv_dir: "./data/dpo_outputs"
    margin_threshold: 0.995
    eval_ratio: 0.1