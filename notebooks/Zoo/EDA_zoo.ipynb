{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3.10 -m venv --system-site-packages /data/ephemeral/home/py310\n",
    "source /data/ephemeral/home/py310/bin/activate\n",
    "pip install --upgrade pip\n",
    "```\n",
    "위에 커맨드를 사용하여 가상환경을 만들고 IDE의 커널을 생성한 가상환경으로 변경해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir torch==2.9.1+cu128 --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('/data/ephemeral/pro-nlp-generationfornlp-nlp-13/data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)\n",
    "df['paragraph_length'] = df['paragraph'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['paragraph_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Paragraph Lengths')\n",
    "plt.xlabel('Paragraph Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 질문 길이와 본문 길이 상관성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_comparison(df):\n",
    "    \"\"\"질문 길이 vs paragraph 길이 산점도\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 기본 산점도\n",
    "    axes[0].scatter(df['question_length'], df['paragraph_length'], \n",
    "                   alpha=0.5, s=10)\n",
    "    axes[0].set_xlabel('Question Length (characters)')\n",
    "    axes[0].set_ylabel('Paragraph Length (characters)')\n",
    "    axes[0].set_title('Question Length vs Paragraph Length')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 상관관계 표시\n",
    "    correlation = df['question_length'].corr(df['paragraph_length'])\n",
    "    axes[0].text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
    "                transform=axes[0].transAxes, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 밀도 산점도 (hexbin)\n",
    "    axes[1].hexbin(df['question_length'], df['paragraph_length'], \n",
    "                   gridsize=30, cmap='YlOrRd')\n",
    "    axes[1].set_xlabel('Question Length (characters)')\n",
    "    axes[1].set_ylabel('Paragraph Length (characters)')\n",
    "    axes[1].set_title('Density Plot (Hexbin)')\n",
    "    plt.colorbar(axes[1].collections[0], ax=axes[1], label='Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('scatter_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter_comparison(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층 EDA Frequency of Words in Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy\n",
    "# !pip install wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def Q_korean_word_freq(df):\n",
    "    okt = Okt()\n",
    "    stopwords = {\n",
    "        '것', '수', '등', '및', '제', '때', '대한',\n",
    "        '옳은', '않은', '틀린', '설명', '인가', '무엇',\n",
    "        '다음', '보기', '문제', '답', '번', '있는' ,'가장',\n",
    "    }\n",
    "    # 모든 질문 합치기\n",
    "    all_text = ' '.join(df['question'].astype(str))\n",
    "    \n",
    "    # 형태소 분석 (명사만 추출)\n",
    "    nouns = okt.nouns(all_text)\n",
    "    \n",
    "    # 한 글자 단어 제거\n",
    "    nouns = [word for word in nouns if len(word) > 1 and word not in stopwords]\n",
    "    \n",
    "    # 빈도수 계산\n",
    "    word_freq = Counter(nouns)\n",
    "    \n",
    "    # 상위 30개\n",
    "    print(\"=== 상위 30개 명사 ===\")\n",
    "    for word, count in word_freq.most_common(30):\n",
    "        print(f\"{word}: {count}번\")\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "Q_freq = Q_korean_word_freq(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud_from_freq(word_freq, title='단어 빈도 워드클라우드'):\n",
    "    \"\"\"Counter 객체로 워드클라우드 생성\"\"\"\n",
    "    \n",
    "    # 한글 폰트 설정 (환경에 맞게 선택)\n",
    "    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  # Linux\n",
    "    # font_path = 'C:/Windows/Fonts/malgun.ttf'  # Windows\n",
    "    # font_path = '/System/Library/Fonts/AppleGothic.ttf'  # Mac\n",
    "    \n",
    "    # 워드클라우드 생성\n",
    "    wordcloud = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        relative_scaling=0.3,\n",
    "        colormap='viridis'\n",
    "    ).generate_from_frequencies(word_freq)  # ← Counter 객체 직접 사용!\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 사용\n",
    "create_wordcloud_from_freq(Q_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층 EDA Frequency of Words in Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def P_korean_word_freq(df):\n",
    "    okt = Okt()\n",
    "    stopwords = {\n",
    "        '것', '수', '등', '및', '제', '때', '대한',\n",
    "        '옳은', '않은', '틀린', '설명', '인가', '무엇',\n",
    "        '다음', '보기', '문제', '답', '번', '있는' ,'가장',\n",
    "    }\n",
    "    # 모든 질문 합치기\n",
    "    all_text = ' '.join(df['paragraph'].astype(str))\n",
    "    \n",
    "    # 형태소 분석 (명사만 추출)\n",
    "    nouns = okt.nouns(all_text)\n",
    "    \n",
    "    # 한 글자 단어 제거\n",
    "    nouns = [word for word in nouns if len(word) > 1 and word not in stopwords]\n",
    "    \n",
    "    # 빈도수 계산\n",
    "    word_freq = Counter(nouns)\n",
    "    \n",
    "    # 상위 30개\n",
    "    print(\"=== 상위 30개 명사 ===\")\n",
    "    for word, count in word_freq.most_common(30):\n",
    "        print(f\"{word}: {count}번\")\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "P_freq = P_korean_word_freq(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud_from_freq(P_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-venv)",
   "language": "python",
   "name": "my-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
