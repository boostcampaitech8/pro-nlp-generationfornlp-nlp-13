{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc67ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84552978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b2f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/data/ephemeral/pro-nlp-generationfornlp-nlp-13'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb35ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             2031 non-null   object \n",
      " 1   paragraph      2031 non-null   object \n",
      " 2   problems       2031 non-null   object \n",
      " 3   question_plus  0 non-null      float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 63.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d886406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(\n",
    "    df,\n",
    "    problems_col=\"problems\",\n",
    "):\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        problems = literal_eval(row[problems_col])\n",
    "\n",
    "        record = {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"paragraph\": row[\"paragraph\"],\n",
    "            \"question\": problems[\"question\"],\n",
    "            \"choices\": problems[\"choices\"],\n",
    "            \"answer\": problems.get(\"answer\", None),\n",
    "            \"question_plus\": problems.get(\"question_plus\", None),\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcee66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flatten_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a98f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2031 non-null   object\n",
      " 1   paragraph      2031 non-null   object\n",
      " 2   question       2031 non-null   object\n",
      " 3   choices        2031 non-null   object\n",
      " 4   answer         2031 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      " 6   choices_len    2031 non-null   int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 111.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df[\"choices_len\"] = df[\"choices\"].apply(len)\n",
    "df['choices_len'].value_counts(dropna=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7791aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df[df.choices_len == 4]\n",
    "df5 = df[df.choices_len == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61b557b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "4    221\n",
       "3    203\n",
       "2    185\n",
       "1    183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.answer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1015c3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "1    765\n",
       "2    265\n",
       "3    116\n",
       "4     62\n",
       "5     31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.answer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b22959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298516/1841385885.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5[\"new_choices\"] = [c for c, _ in shuffled_df5]\n",
      "/tmp/ipykernel_1298516/1841385885.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5[\"new_answer\"]  = [a for _, a in shuffled_df5]\n"
     ]
    }
   ],
   "source": [
    "def shuffle_choices(choices, answer, seed=42):\n",
    "    idx = answer - 1\n",
    "    zipped = list(enumerate(choices))\n",
    "    \n",
    "    rng = random.Random(seed)  # 시드 고정\n",
    "    rng.shuffle(zipped)\n",
    "\n",
    "    new_choices = [c for _, c in zipped]\n",
    "    new_answer = [i for i, (orig_i, _) in enumerate(zipped) if orig_i == idx][0] + 1\n",
    "\n",
    "    return new_choices, new_answer\n",
    "\n",
    "# 시드 고정\n",
    "seed = 42\n",
    "shuffled_df5 = [\n",
    "    shuffle_choices(c, a, seed + i)  # i를 더해서 각 행마다 shuffle이 달라지도록\n",
    "    for i, (c, a) in enumerate(zip(df5[\"choices\"], df5[\"answer\"]))\n",
    "]\n",
    "\n",
    "df5[\"new_choices\"] = [c for c, _ in shuffled_df5]\n",
    "df5[\"new_answer\"]  = [a for _, a in shuffled_df5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac47cdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_answer\n",
       "5    262\n",
       "2    255\n",
       "3    248\n",
       "4    246\n",
       "1    228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.new_answer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed04e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    1239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df = df5.assign(\n",
    "    original_answer_text = df5.apply(\n",
    "        lambda r: r[\"choices\"][r[\"answer\"] - 1], axis=1\n",
    "    ),\n",
    "    shuffled_answer_text = df5.apply(\n",
    "        lambda r: r[\"new_choices\"][r[\"new_answer\"] - 1], axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "(check_df[\"original_answer_text\"] == check_df[\"shuffled_answer_text\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624a778b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1298516/3925679504.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5.choices = df5.new_choices\n",
      "/tmp/ipykernel_1298516/3925679504.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5.answer = df5.new_answer\n"
     ]
    }
   ],
   "source": [
    "df5.choices = df5.new_choices\n",
    "df5.answer = df5.new_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca2c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5[list(df4.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d530553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus',\n",
       "       'choices_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5f9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df4,df5]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc81138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,        \n",
    "    random_state=42,\n",
    "    stratify=df[\"choices_len\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a04b1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "4    35\n",
       "1    34\n",
       "2    26\n",
       "3    24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df[valid_df.choices_len==4].answer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09aad38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "valid_ds = Dataset.from_pandas(valid_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b8cd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6051aa309a244d59f6c3483f2a04d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\"\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb7c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 System Prompt 함수\n",
    "def get_system_message(row, system_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 하나의 데이터 행\n",
    "    system_prompts: {choices_len: {version: prompt_text}}\n",
    "    prompt_policy: {choices_len: version}\n",
    "    \"\"\"\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    return system_prompts[choices_len][version]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2cb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt 관리 구조 설명\n",
    "\n",
    "# SYSTEM_PROMPTS:\n",
    "# - 실제 system prompt 문구들을 모두 정의해 두는 \"저장소\"\n",
    "# - 문제 유형(choices_len = 4 / 5)별로 나누고, 각 유형 안에서 v1, v2 처럼 여러 버전의 prompt를 보관\n",
    "# - 즉, 여기에는 \"쓸 수 있는 모든 프롬프트 후보\"가 들어 있다.\n",
    "#\n",
    "# 예:\n",
    "# SYSTEM_PROMPTS = {\n",
    "#     4: { \"v1\": \"...\", \"v2\": \"...\" },\n",
    "#     5: { \"v1\": \"...\", \"v2\": \"...\" }\n",
    "# }\n",
    "\n",
    "SYSTEM_PROMPT_4_V1 = (\n",
    "    \"당신은 **지식 추론(Knowledge Inference) 전문가**입니다. \"\n",
    "    \"이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. \"\n",
    "    \"지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 \"\n",
    "    \"가장 타당한 선택지 하나를 고르십시오.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_5_V1 = (\n",
    "    \"당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. \"\n",
    "    \"이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. \"\n",
    "    \"당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\\n\\n\"\n",
    ")\n",
    "\n",
    "# PROMPT_POLICY:\n",
    "# - 이번 실험(run)에서 \"어떤 system prompt 버전을 사용할지\"를 결정하는 설정값\n",
    "# - choices_len(4 or 5) → 사용할 prompt 버전(v1, v2, ...)\n",
    "# - 실험을 바꿀 때는 이 딕셔너리만 수정\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": SYSTEM_PROMPT_4_V1,\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": SYSTEM_PROMPT_5_V1,\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6bc09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 User Prompt 함수\n",
    "def get_user_message(row, user_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 데이터 행\n",
    "    user_prompts: 템플릿 저장소\n",
    "    prompt_policy: 버전 정책\n",
    "    \"\"\"\n",
    "    # 메타 데이터 확인\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    \n",
    "    # 해당 버전의 템플릿 세트 가져오기 (plus, no_plus가 들어있음)\n",
    "    template_set = user_prompts[choices_len][version]\n",
    "    \n",
    "    # 데이터 준비\n",
    "    paragraph = row['paragraph']\n",
    "    question = row['question']\n",
    "    choices_str = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(row['choices'])])\n",
    "    q_plus = row.get('question_plus', None)\n",
    "    \n",
    "    # 분기 처리 및 포맷팅 (여기가 핵심!)\n",
    "    # q_plus가 존재하고, nan이 아닐 때 -> Plus 템플릿 사용\n",
    "    if q_plus and str(q_plus) != 'nan':\n",
    "        return template_set[\"plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question_plus=q_plus, # 여기 들어감\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    # q_plus가 없을 때 -> No Plus 템플릿 사용\n",
    "    else:\n",
    "        return template_set[\"no_plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30fd8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# User Prompt Templates (V1)\n",
    "# =========================\n",
    "\n",
    "# 4지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 4지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "USER_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_5_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_5_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    }\n",
    "}\n",
    "\n",
    "USER_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77699618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Assistant Prompt 함수\n",
    "def get_assistant_message(row):\n",
    "    \"\"\"\n",
    "    Assistant 메시지 생성 함수.\n",
    "    Qwen3 모델의 토크나이저 템플릿이 자동으로 <think> 태그를 처리하므로,\n",
    "    여기서는 순수한 정답(Label) 텍스트만 반환\n",
    "    \"\"\"\n",
    "    return str(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e714cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    \"\"\"\n",
    "    원본 example(row)로부터 학습용 chat messages를 구성한다.\n",
    "    - choices_len(4/5) 및 question_plus 유무에 따라 system/user 프롬프트를 선택\n",
    "    - assistant는 정답 숫자만\n",
    "    - 이후 평가/추적용으로 id, label도 함께 유지\n",
    "    \"\"\"\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "    asst_msg = get_assistant_message(example)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ],\n",
    "        \"label\": int(example[\"answer\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5edd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(example):\n",
    "    \"\"\"\n",
    "    messages(list[dict])를 tokenizer의 chat_template 규칙에 따라\n",
    "    단일 텍스트로 직렬화한다.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  \n",
    "    )\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7514656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example, truncation=True, max_length=2048, padding=False):\n",
    "    \"\"\"\n",
    "    batched=True면 example[\"text\"]는 List[str]\n",
    "    batched=False면 example[\"text\"]는 str\n",
    "    \"\"\"\n",
    "    tok_kwargs = dict(truncation=truncation, padding=padding)\n",
    "    if truncation is True:\n",
    "        tok_kwargs[\"max_length\"] = max_length\n",
    "\n",
    "    out = tokenizer(example[\"text\"], **tok_kwargs)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": out[\"input_ids\"],\n",
    "        \"attention_mask\": out[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c74e1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len']\n"
     ]
    }
   ],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "print(orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64c8fa2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6032eb56cc64103a42e697b8ce16f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b8f53d304f46a4b07e78e1fea85ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "359c4c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e049868954c541cab28da26ad5cb3158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972ee8d4b297482cb3f406cc93998d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8be22b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4796189b1845d8b448f1ef7f04f3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9384fe025a0642d7bc109ab2b4782ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c157e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 변환 완료 ===\n",
      "Train 개수: 1726\n",
      "첫 번째 샘플 Keys: dict_keys(['id', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 변환 완료 ===\")\n",
    "print(\"Train 개수:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"첫 번째 샘플 Keys:\", tokenized_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c3a5a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55dbd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "340479ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' -> ids=[16], toks=['1']\n",
      "'2' -> ids=[17], toks=['2']\n",
      "'3' -> ids=[18], toks=['3']\n",
      "'4' -> ids=[19], toks=['4']\n",
      "'5' -> ids=[20], toks=['5']\n",
      "' 1' -> ids=[220, 16], toks=['Ġ', '1']\n",
      "' 2' -> ids=[220, 17], toks=['Ġ', '2']\n",
      "' 3' -> ids=[220, 18], toks=['Ġ', '3']\n",
      "' 4' -> ids=[220, 19], toks=['Ġ', '4']\n",
      "' 5' -> ids=[220, 20], toks=['Ġ', '5']\n",
      "1 -> 16\n",
      "2 -> 17\n",
      "3 -> 18\n",
      "4 -> 19\n",
      "5 -> 20\n"
     ]
    }
   ],
   "source": [
    "### Qwen3 1,2,3,4,5 토큰 id 확인\n",
    "# 가장 확실: encode 결과 보기\n",
    "for s in [\"1\",\"2\",\"3\",\"4\",\"5\",\" 1\",\" 2\",\" 3\",\" 4\",\" 5\"]:\n",
    "    ids = tokenizer.encode(s, add_special_tokens=False)\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"{s!r} -> ids={ids}, toks={toks}\")\n",
    "\n",
    "# 단일 토큰 매핑을 기대한다면 (토크나이저에 따라 None/-1일 수 있음)\n",
    "for tok in [\"1\",\"2\",\"3\",\"4\",\"5\"]:\n",
    "    print(tok, \"->\", tokenizer.convert_tokens_to_ids(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e6894a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 검증용 / 미리 사전 체크\n",
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def assert_answer_pos_is_digit(\n",
    "    ds,                    # train_ds or eval_ds (collator 전에 remove_columns 한 ds)\n",
    "    data_collator,         # DataCollatorForCompletionOnlyLM\n",
    "    n_samples=1000,\n",
    "    seed=42,\n",
    "    pos_from_tail=3,       # -3 규칙\n",
    "    batch_size=16,\n",
    "    verbose_fail=5,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "    n_samples = min(n_samples, len(ds))\n",
    "    idxs = rng.sample(range(len(ds)), n_samples)\n",
    "\n",
    "    fail = 0\n",
    "    shown = 0\n",
    "\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        batch_idxs = idxs[start:start+batch_size]\n",
    "        sample_batch = [ds[i] for i in batch_idxs]\n",
    "        collated = data_collator(sample_batch)\n",
    "\n",
    "        input_ids = collated[\"input_ids\"]          # (B, L)\n",
    "        attn = collated[\"attention_mask\"]          # (B, L)\n",
    "        labels = collated[\"labels\"]                # (B, L)\n",
    "\n",
    "        real_len = attn.sum(dim=1)                 # (B,)\n",
    "        pos = real_len - pos_from_tail             # (B,)\n",
    "\n",
    "        bsz = input_ids.size(0)\n",
    "        for b in range(bsz):\n",
    "            p = int(pos[b].item())\n",
    "            tok = int(input_ids[b, p].item())\n",
    "            lab = int(labels[b, p].item())\n",
    "\n",
    "            ok = (tok in DIGIT_IDS) and (lab in DIGIT_IDS)\n",
    "            if not ok:\n",
    "                fail += 1\n",
    "                if shown < verbose_fail:\n",
    "                    shown += 1\n",
    "                    print(f\"[FAIL] idx={batch_idxs[b]} real_len={int(real_len[b])} pos={p} tok={tok} lab={lab}\")\n",
    "                    # 주변 토큰도 같이 확인\n",
    "                    left = max(0, p-6)\n",
    "                    right = min(input_ids.size(1), p+6)\n",
    "                    print(\"  decoded window:\", tokenizer.decode(input_ids[b, left:right], skip_special_tokens=False))\n",
    "                    # labels에서 digit 토큰이 아예 있는지 확인\n",
    "                    digit_mask = torch.isin(labels[b], torch.tensor(DIGIT_IDS, device=labels.device))\n",
    "                    if digit_mask.any():\n",
    "                        digit_positions = torch.where(digit_mask)[0].tolist()\n",
    "                        print(\"  digit_positions_in_labels:\", digit_positions)\n",
    "                    else:\n",
    "                        print(\"  digit_positions_in_labels: NONE\")\n",
    "\n",
    "    assert fail == 0, f\"pos=real_len-{pos_from_tail} 규칙이 깨진 샘플이 {fail}개 있습니다.\"\n",
    "    print(f\"OK: {n_samples} samples passed. (pos=real_len-{pos_from_tail} is digit token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "609641d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels, pos_from_tail=4):\n",
    "    \"\"\"\n",
    "    반환: (batch, 5)  -> '1'~'5'에 해당하는 logits만 뽑아서 metrics 단계로 전달\n",
    "    \"\"\"\n",
    "    # Trainer가 (logits, ...) 튜플을 줄 때가 있어서 정리\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # (B, L, V)\n",
    "\n",
    "    # labels: (B, L), pad/무시 영역은 -100일 가능성이 큼\n",
    "    # real_len = 마지막으로 labels != -100 인 위치 + 1 로 복원\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "    not_ignored = (labels_t != -100)\n",
    "\n",
    "    # 샘플별로 마지막 not_ignored 위치 찾기\n",
    "    # (뒤에서부터 True 찾기)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)          # (B,)\n",
    "    has_any = not_ignored.any(dim=1)                             # (B,)\n",
    "    # real_len = seq_len - last_true_from_end\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "\n",
    "    # 만약 labels가 전부 -100인 샘플이 있으면(비정상) 그냥 seq_len로 처리\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos = (real_len - pos_from_tail).clamp(min=0, max=seq_len-1) # (B,)\n",
    "\n",
    "    # (B, V)로 해당 위치의 logits만 gather\n",
    "    logits_t = torch.as_tensor(logits)                           # (B, L, V)\n",
    "    batch_idx = torch.arange(logits_t.size(0), device=logits_t.device)\n",
    "    picked = logits_t[batch_idx, pos, :]                         # (B, V)\n",
    "\n",
    "    # digit ids만 슬라이스 -> (B, 5)\n",
    "    picked_digits = picked[:, DIGIT_IDS]\n",
    "    return picked_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c236201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, label_pos_from_tail=3):\n",
    "    \"\"\"\n",
    "    eval_pred:\n",
    "      - (predictions, label_ids) 튜플 형태가 가장 흔함\n",
    "      - predictions: preprocess_logits_for_metrics가 반환한 (B, 5)\n",
    "      - label_ids: (B, L) with -100 ignored\n",
    "    반환: {\"accuracy\": ..., \"macro_f1\": ...}\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "    preds_t = torch.as_tensor(preds)\n",
    "    pred_cls = torch.argmax(preds_t, dim=-1).cpu().numpy().astype(np.int64)  # (B,)\n",
    "\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "\n",
    "    not_ignored = (labels_t != -100)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)\n",
    "    has_any = not_ignored.any(dim=1)\n",
    "\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos_label = (real_len - label_pos_from_tail).clamp(min=0, max=seq_len - 1)\n",
    "    batch_idx = torch.arange(labels_t.size(0), device=labels_t.device)\n",
    "    gold_tok = labels_t[batch_idx, pos_label].cpu().numpy().astype(np.int64) \n",
    "\n",
    "    gold_cls = gold_tok - DIGIT_IDS[0]  \n",
    "\n",
    "    valid = (gold_cls >= 0) & (gold_cls < 5)\n",
    "    pred_cls = pred_cls[valid]\n",
    "    gold_cls = gold_cls[valid]\n",
    "\n",
    "    acc = (pred_cls == gold_cls).mean() if len(gold_cls) > 0 else 0.0\n",
    "\n",
    "    f1s = []\n",
    "    for c in range(5):\n",
    "        tp = np.sum((pred_cls == c) & (gold_cls == c))\n",
    "        fp = np.sum((pred_cls == c) & (gold_cls != c))\n",
    "        fn = np.sum((pred_cls != c) & (gold_cls == c))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_f1 = float(np.mean(f1s)) if len(f1s) > 0 else 0.0\n",
    "\n",
    "    return {\"accuracy\": float(acc), \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5eb4f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fbce44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 전에는 반드시 지워야 하는것 -> 'input_ids', 'attention_mask'\n",
    "# 아래 예시처럼\n",
    "train_dataset = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_dataset = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99900285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|> 151643\n",
      "eos_token: <|im_end|> 151645\n",
      "use_cache: False\n",
      "grad_ckpt: True\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"eos_token:\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "# pad_token 없으면 eos로 대체\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 학습 안정/메모리 옵션\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"use_cache:\", model.config.use_cache)\n",
    "print(\"grad_ckpt:\", model.is_gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b78496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5719f76808bc4d81ab31c5cbe4569b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # V100이면 fp16 추천\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9a81332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer에 pad_token이 이미 있는 상태라면\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9db60135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attention proj만\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Attention + MLP까지 (성능 더 노리되 trainable 조금 증가)\n",
    "# \"gate_proj\", \"up_proj\", \"down_proj\" -> FFN\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# target_modules = [\"q_proj\", \"k_proj\"]\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7995917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../../qwen-sft-results\",\n",
    "    \n",
    "    # 데이터 및 배치 설정\n",
    "    num_train_epochs=1,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    #  학습률 및 Optimizer (V100)\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                  # V100 필수\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_32bit\",  #  32bit 사용 (안정성) / paged_adamw_8bit -> 더 안된다면 이걸로!\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 검증(Eval) 및 저장(Save) 전략 수정\n",
    "    eval_strategy=\"steps\",      # Epoch 대신 Steps 단위로 검증\n",
    "    eval_steps=20,              # 50 스텝마다 검증 (데이터 양에 따라 조절하세요)\n",
    "    \n",
    "    # 실제 사용 시 주석 해제\n",
    "    save_strategy=\"steps\",      # 검증 주기와 맞춰서 저장 전략도 steps로\n",
    "    save_steps=20,              # 50 스텝마다 저장 시도\n",
    "    \n",
    "    # 저장 용량 관리\n",
    "    save_total_limit=2,         # 체크포인트를 딱 2개만 남기고 옛날 건 자동 삭제!\n",
    "    load_best_model_at_end=True, # 학습 끝나면 \"가장 성능 좋았던 모델\"을 자동으로 로드\n",
    "    metric_for_best_model=\"eval_loss\", # 무엇을 기준으로 최고를 뽑을지 (accuracy 추천)\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # 기타 로깅\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # 시험용이라면???\n",
    "    # max_steps = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59947732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e03394d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 1:00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.068857</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.815175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.059181</td>\n",
       "      <td>0.829508</td>\n",
       "      <td>0.838813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.058127</td>\n",
       "      <td>0.839344</td>\n",
       "      <td>0.849308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.839344</td>\n",
       "      <td>0.851055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.059215</td>\n",
       "      <td>0.829508</td>\n",
       "      <td>0.838910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.064588</td>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.836263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.058025</td>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.832729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.056023</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.830165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.060395</td>\n",
       "      <td>0.845902</td>\n",
       "      <td>0.854808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.059426</td>\n",
       "      <td>0.859016</td>\n",
       "      <td>0.867189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11efa020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     869 non-null    int64 \n",
      " 1   id             869 non-null    object\n",
      " 2   paragraph      869 non-null    object\n",
      " 3   problems       869 non-null    object\n",
      " 4   question_plus  44 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 34.1+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65154677764d464f87c06a87a2b6e6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec87196e2e14716843a091468e098a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc059244be4a85b74154bb56f2bb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "test_df.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)\n",
    "test_df[\"choices_len\"] = test_df[\"choices\"].apply(len)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Model load\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "adapter_path = \"../../qwen-sft-results/checkpoint-216\"# 실제 변경\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "def build_test_messages(example):\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def to_test_text(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, \n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "test_ds_msg = test_dataset.map(\n",
    "    build_test_messages,\n",
    "    batched=False,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "test_ds_text = test_ds_msg.map(\n",
    "    to_test_text,\n",
    "    batched=False,\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a8c0cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Logits 기반 추론 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 869/869 [18:36<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료! 총 869개 결과 생성됨\n",
      "📊 Logits 성공: 869 (100.0%)\n",
      "📊 Fallback 사용: 0 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 이걸로 해보면...??? ###\n",
    "\n",
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "THINK_END_ID = 151668  # </think>\n",
    "\n",
    "def get_answer_from_logits(outputs, input_len):\n",
    "    \"\"\"\n",
    "    output_scores에서 </think> 이후 첫 digit의 logits 확인\n",
    "    \"\"\"\n",
    "    if not hasattr(outputs, 'scores') or not outputs.scores:\n",
    "        return None\n",
    "    \n",
    "    generated_ids = outputs.sequences[0]  # (total_len,)\n",
    "    \n",
    "    # </think> 위치 찾기\n",
    "    think_end_positions = (generated_ids == THINK_END_ID).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    if len(think_end_positions) == 0:\n",
    "        # </think>가 없으면 생성된 토큰 중 첫 digit 찾기\n",
    "        for i in range(input_len, len(generated_ids)):\n",
    "            token_id = generated_ids[i].item()\n",
    "            if token_id in DIGIT_IDS:\n",
    "                step_idx = i - input_len\n",
    "                if 0 <= step_idx < len(outputs.scores):\n",
    "                    step_logits = outputs.scores[step_idx][0]  # (V,)\n",
    "                    digit_logits = step_logits[DIGIT_IDS]  # (5,)\n",
    "                    return digit_logits.argmax().item() + 1  # 1~5\n",
    "        return None\n",
    "    \n",
    "    # </think> 이후 첫 digit 토큰 찾기\n",
    "    think_end_pos = think_end_positions[-1].item()\n",
    "    \n",
    "    for i in range(think_end_pos + 1, len(generated_ids)):\n",
    "        token_id = generated_ids[i].item()\n",
    "        if token_id in DIGIT_IDS:\n",
    "            # 이 토큰이 생성된 step의 logits 확인\n",
    "            step_idx = i - input_len\n",
    "            if 0 <= step_idx < len(outputs.scores):\n",
    "                step_logits = outputs.scores[step_idx][0]  # (V,)\n",
    "                digit_logits = step_logits[DIGIT_IDS]  # (5,)\n",
    "                # 가장 높은 확률의 digit 선택\n",
    "                predicted = digit_logits.argmax().item() + 1  # 1~5\n",
    "                return predicted\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_pred_fallback(text):\n",
    "    \"\"\"\n",
    "    logits에서 실패 시 텍스트 파싱 (fallback)\n",
    "    \"\"\"\n",
    "    if \"</think>\" in text:\n",
    "        after_think = text.split(\"</think>\")[-1]\n",
    "        for char in after_think:\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                return char\n",
    "    \n",
    "    for keyword in [\"정답:\", \"정답은\", \"Answer:\"]:\n",
    "        if keyword in text:\n",
    "            after_keyword = text.split(keyword)[-1]\n",
    "            for char in after_keyword:\n",
    "                if char in ['1', '2', '3', '4', '5']:\n",
    "                    return char\n",
    "    \n",
    "    for char in reversed(text):\n",
    "        if char in ['1', '2', '3', '4', '5']:\n",
    "            return char\n",
    "    \n",
    "    return '1'\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 추론 실행 (개선 버전)\n",
    "# ==========================================\n",
    "infer_results = []\n",
    "logits_success = 0\n",
    "fallback_used = 0\n",
    "\n",
    "print(\"🚀 Logits 기반 추론 시작...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for ex in tqdm(test_ds_text):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,       \n",
    "        )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        \n",
    "        # 1차 시도: logits 기반\n",
    "        pred = get_answer_from_logits(outputs, input_len)\n",
    "        \n",
    "        if pred is not None:\n",
    "            logits_success += 1\n",
    "            pred_str = str(pred)\n",
    "        else:\n",
    "            # 2차 시도: 텍스트 파싱\n",
    "            fallback_used += 1\n",
    "            gen_ids = outputs.sequences[0][input_len:]\n",
    "            gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            pred_str = parse_pred_fallback(gen_text)\n",
    "        \n",
    "        infer_results.append({\n",
    "            \"id\": _id,\n",
    "            \"answer\": pred_str\n",
    "        })\n",
    "\n",
    "print(f\"✅ 완료! 총 {len(infer_results)}개 결과 생성됨\")\n",
    "print(f\"📊 Logits 성공: {logits_success} ({logits_success/len(infer_results)*100:.1f}%)\")\n",
    "print(f\"📊 Fallback 사용: {fallback_used} ({fallback_used/len(infer_results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c5e7364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv('shuffled_answer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afcd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
