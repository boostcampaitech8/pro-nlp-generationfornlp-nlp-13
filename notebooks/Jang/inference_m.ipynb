{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a4cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: /data/ephemeral/pro-nlp-generationfornlp-nlp-13\n",
      "sys.path[0]: /data/ephemeral/pro-nlp-generationfornlp-nlp-13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path(os.getcwd())\n",
    "\n",
    "project_root = nb_dir.parents[1]\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"sys.path[0]:\", sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc35c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digit_logits(\n",
    "    outputs,\n",
    "    generated_ids: torch.Tensor,\n",
    "    digit_token_ids: list[int],\n",
    "):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "      digit_pos: int | None\n",
    "      subset_logits: List[float] | None   # [l1, l2, ..., lk]\n",
    "      chosen_digit: str | None\n",
    "    \"\"\"\n",
    "    digit_pos = None\n",
    "    chosen_token_id = None\n",
    "\n",
    "    # 숫자 토큰이 처음 등장한 위치\n",
    "    for i, tid in enumerate(generated_ids.tolist()):\n",
    "        if tid in digit_token_ids:\n",
    "            digit_pos = i\n",
    "            chosen_token_id = tid\n",
    "            break\n",
    "\n",
    "    if digit_pos is None:\n",
    "        return None, None, None\n",
    "\n",
    "    step_logits = outputs.scores[digit_pos][0]  # (vocab,)\n",
    "    subset_logits = step_logits[digit_token_ids]  # (k,)\n",
    "\n",
    "    chosen_digit = str(digit_token_ids.index(chosen_token_id) + 1)\n",
    "\n",
    "    return (\n",
    "        digit_pos,\n",
    "        subset_logits.detach().cpu().tolist(),\n",
    "        chosen_digit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(\n",
    "    row_dict: Dict,\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 100,\n",
    ") -> Dict:\n",
    "\n",
    "    output = builder.build_message(row_dict)\n",
    "    messages = output[\"messages\"]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(device)\n",
    "\n",
    "    k = int(row_dict[\"choices_len\"])\n",
    "    digit_token_ids = [\n",
    "        tokenizer.encode(str(i), add_special_tokens=False)[0]\n",
    "        for i in range(1, k + 1)\n",
    "    ]\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs.sequences[0][input_len:]\n",
    "    full_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    digit_pos, digit_logits, chosen_digit = extract_digit_logits(\n",
    "        outputs,\n",
    "        generated_ids,\n",
    "        digit_token_ids,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"id\": row_dict.get(\"id\"),\n",
    "        \"predicted_answer\": extract_answer(full_text),  # 기존 방식\n",
    "        \"chosen_digit\": chosen_digit,                   # logit 기반\n",
    "        \"digit_logits\": digit_logits,                   # ⭐ 핵심\n",
    "        \"digit_pos\": digit_pos,\n",
    "        \"full_output\": full_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ad839",
   "metadata": {},
   "source": [
    "## 시험용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83cf89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from src.data.preprocessor import parse_problems_column, add_choices_len\n",
    "from src.prompt.prompt_builder import PromptBuilder, PromptConfig\n",
    "from src.training.model_loader import ModelConfig, load_model_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660832d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully!\n",
      "Adapter Path: ./models/qwen3_14B_eng_aug2/final_model\n",
      "Test Data Path: ./data/test.csv\n",
      "Max New Tokens: 30\n",
      "Device: cuda\n",
      "Loading test data from /data/ephemeral/pro-nlp-generationfornlp-nlp-13/data/test.csv...\n",
      "Loaded 869 rows\n",
      "Loading tokenizer from Qwen/Qwen3-14B...\n",
      "Tokenizer loaded successfully!\n",
      "Loading model from ./models/qwen3_14B_eng_aug2/final_model...\n",
      "Loading Base Model for Inference: Qwen/Qwen3-14B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073ad549fba34d8d9c553736187974a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA Adapter from: /data/ephemeral/pro-nlp-generationfornlp-nlp-13/models/qwen3_14B_eng_aug2/final_model\n",
      "Model loaded successfully!\n",
      "PromptBuilder ready!\n"
     ]
    }
   ],
   "source": [
    "config_path = \"../../config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg_dict = yaml.safe_load(f)\n",
    "\n",
    "print(\"Config loaded successfully!\")\n",
    "\n",
    "# Model Config\n",
    "model_cfg_dict = cfg_dict[\"model\"].copy()\n",
    "model_cfg_dict[\"use_gradient_checkpointing\"] = False\n",
    "model_cfg = ModelConfig(**model_cfg_dict)\n",
    "\n",
    "# Prompt Config\n",
    "prompt_dict = cfg_dict[\"inference\"][\"prompt\"]\n",
    "prompt_cfg = PromptConfig(\n",
    "    policy=prompt_dict[\"policy\"],\n",
    "    mode=\"test\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Inference Config\n",
    "inference_cfg = cfg_dict.get(\"inference\", {})\n",
    "adapter_path = inference_cfg[\"adapter_path\"]\n",
    "test_data_path = inference_cfg[\"test_data_path\"]\n",
    "max_new_tokens = inference_cfg.get(\"max_new_tokens\", 100)\n",
    "\n",
    "print(f\"Adapter Path: {adapter_path}\")\n",
    "print(f\"Test Data Path: {test_data_path}\")\n",
    "print(f\"Max New Tokens: {max_new_tokens}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "### 데이터 경로 설정\n",
    "print(f\"Loading test data from {project_root / test_data_path}...\")\n",
    "test_df = pd.read_csv(project_root / test_data_path)\n",
    "test_df = parse_problems_column(test_df)\n",
    "test_df = add_choices_len(test_df)\n",
    "\n",
    "print(f\"Loaded {len(test_df)} rows\")\n",
    "test_df.head()\n",
    "\n",
    "print(f\"Loading tokenizer from {model_cfg.model_name_or_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_cfg.model_name_or_path,\n",
    "    trust_remote_code=model_cfg.trust_remote_code,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "print(f\"Loading model from {adapter_path}...\")\n",
    "model = load_model_inference(model_cfg, project_root / adapter_path)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "builder = PromptBuilder(prompt_cfg)\n",
    "print(\"PromptBuilder ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278dc857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages:\n",
      "\n",
      "[system]\n",
      "당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다.\n",
      "이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다.\n",
      "당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\n",
      "\n",
      "[user]\n",
      "### 지문\n",
      "사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 여러 가지가 있겠지만 그 중심에는 ‘소통의  즐거움’이 있다. 독자는 독서를 통해 책과 소통하는 즐거움을 경험한다 .  독서는   필자와 간접적으로 대화하는 소통 행위이다 .  독자는 자신이 속한   사회나 시대의 영향 아래 필자가 속해 있거나 드러내고자 하는  사회나 시대를 경험한다.  직접 경험하지 못했던 다양한 삶을  필자를 매개로 만나고 이해하면서 독자는 더 넓은 시야로 세계를   바라볼 수 있다.  이때 같은 책을 읽은 독자라도 독자의 배경 지식이나 관점 등의 독자 요인,  읽기 환경이나 과제 등의 상황  요인이 다르므로,  필자가 보여 주는 세계를 그대로 수용하지  않고 저마다 소통 과정에서 다른 의미를 구성할 수 있다 . 이러한 소통은 독자가 책의 내용에 대해 질문하고 답을  찾아내는 과정에서 가능해진다.  독자는 책에서 답을 찾는  질문 ,  독자 자신에게서 답을 찾는 질문 등을 제기할 수 있다 .   전자의 경우 책에 명시된 내용에서 답을 발견할 수 있고,   책의 내용들을 관계 지으며 답에 해당하는 내용을 스스로  구성할 수도 있다.  또한 후자의 경우 책에는 없는 독자의  경험에서 답을 찾을 수 있다.  이런 질문들을 풍부히 생성 하고 주체적으로 답을 찾을 때 소통의 즐거움은 더 커진다 . 한편 독자는 ㉠다른 독자와 소통하는 즐거움 을 경험할 수도  있다 .  책과의 소통을 통해 개인적으로 형성한 의미를 독서 모임 이나 독서 동아리 등에서 다른 독자들과 나누는 일이 이에 해당 한다.  비슷한 해석에 서로 공감하며 기존 인식을 강화하거나  관점의 차이를 확인하고 기존 인식을 조정하는 과정에서 ,  독자는   자신의 인식을 심화 ･확장할 수 있다.  최근 소통 공간이 온라인 으로 확대되면서 독서를 통해 다른 독자들과 소통하며 즐거움을   누리는 양상이 더 다양해지고 있다.  자신의 독서 경험을 담은  글이나 동영상을 생산･ 공유함으로써 ,  책을 읽지 않은  타인이  책과 소통하도록 돕는 것도 책을 통한 소통의 즐거움을 나누는  일이다. \n",
      "\n",
      "### 질문\n",
      "윗글의 내용과 일치하지 않는  것은?\n",
      "\n",
      "### 선택지\n",
      "1. 같은 책을 읽은 독자라도 서로 다른 의미를 구성할 수 있다 .\n",
      "2. 다른 독자와의 소통은 독자가 인식의 폭을 확장하도록 돕는다 .\n",
      "3. 독자는 직접 경험해 보지 못했던 다양한 삶을 책의 필자를  매개로 접할 수 있다.\n",
      "4. 독자의 배경지식,  관점 ,  읽기 환경 ,  과제는 독자의 의미 구성에   영향을 주는 독자 요인이다.\n",
      "5. 독자는 책을 읽을 때 자신이 속한 사회나 시대의 영향을  받으며 필자와 간접적으로 대화한다.\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
      "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
      "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
      "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
      "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
      "\n",
      "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:\n",
      "Prompt Text:\n",
      "<|im_start|>system\n",
      "당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다.\n",
      "이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다.\n",
      "당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.<|im_end|>\n",
      "<|im_start|>user\n",
      "### 지문\n",
      "사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 여러 가지가 있겠지만 그 중심에는 ‘소통의  즐거움’이 있다. 독자는 독서를 통해 책과 소통하는 즐거움을 경험한다 .  독서는   필자와 간접적으로 대화하는 소통 행위이다 .  독자는 자신이 속한   사회나 시대의 영향 아래 필자가 속해 있거나 드러내고자 하는  사회나 시대를 경험한다.  직접 경험하지 못했던 다양한 삶을  필자를 매개로 만나고 이해하면서 독자는 더 넓은 시야로 세계를   바라볼 수 있다.  이때 같은 책을 읽은 독자라도 독자의 배경 지식이나 관점 등의 독자 요인,  읽기 환경이나 과제 등의 상황  요인이 다르므로,  필자가 보여 주는 세계를 그대로 수용하지  않고 저마다 소통 과정에서 다른 의미를 구성할 수 있다 . 이러한 소통은 독자가 책의 내용에 대해 질문하고 답을  찾아내는 과정에서 가능해진다.  독자는 책에서 답을 찾는  질문 ,  독자 자신에게서 답을 찾는 질문 등을 제기할 수 있다 .   전자의 경우 책에 명시된 내용에서 답을 발견할 수 있고,   책의 내용들을 관계 지으며 답에 해당하는 내용을 스스로  구성할 수도 있다.  또한 후자의 경우 책에는 없는 독자의  경험에서 답을 찾을 수 있다.  이런 질문들을 풍부히 생성 하고 주체적으로 답을 찾을 때 소통의 즐거움은 더 커진다 . 한편 독자는 ㉠다른 독자와 소통하는 즐거움 을 경험할 수도  있다 .  책과의 소통을 통해 개인적으로 형성한 의미를 독서 모임 이나 독서 동아리 등에서 다른 독자들과 나누는 일이 이에 해당 한다.  비슷한 해석에 서로 공감하며 기존 인식을 강화하거나  관점의 차이를 확인하고 기존 인식을 조정하는 과정에서 ,  독자는   자신의 인식을 심화 ･확장할 수 있다.  최근 소통 공간이 온라인 으로 확대되면서 독서를 통해 다른 독자들과 소통하며 즐거움을   누리는 양상이 더 다양해지고 있다.  자신의 독서 경험을 담은  글이나 동영상을 생산･ 공유함으로써 ,  책을 읽지 않은  타인이  책과 소통하도록 돕는 것도 책을 통한 소통의 즐거움을 나누는  일이다. \n",
      "\n",
      "### 질문\n",
      "윗글의 내용과 일치하지 않는  것은?\n",
      "\n",
      "### 선택지\n",
      "1. 같은 책을 읽은 독자라도 서로 다른 의미를 구성할 수 있다 .\n",
      "2. 다른 독자와의 소통은 독자가 인식의 폭을 확장하도록 돕는다 .\n",
      "3. 독자는 직접 경험해 보지 못했던 다양한 삶을 책의 필자를  매개로 접할 수 있다.\n",
      "4. 독자의 배경지식,  관점 ,  읽기 환경 ,  과제는 독자의 의미 구성에   영향을 주는 독자 요인이다.\n",
      "5. 독자는 책을 읽을 때 자신이 속한 사회나 시대의 영향을  받으며 필자와 간접적으로 대화한다.\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
      "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
      "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
      "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
      "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
      "\n",
      "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "... (total length: 1801 chars)\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플 가져오기\n",
    "row_dict = test_df.iloc[0].to_dict()\n",
    "\n",
    "# Prompt 생성\n",
    "output = builder.build_message(row_dict)\n",
    "messages = output[\"messages\"]\n",
    "\n",
    "print(\"Messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"\\n[{msg['role']}]\")\n",
    "    print(msg['content'] if len(msg['content']) > 500 else msg['content'])\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "print(\"Prompt Text:\")\n",
    "print(prompt_text)\n",
    "print(f\"\\n... (total length: {len(prompt_text)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 실제 사용.\n",
    "def extract_answer(text: str) -> str:\n",
    "    numbers = re.findall(r'[1-5]', text)\n",
    "    return numbers[-1] if numbers else \"no\"\n",
    "\n",
    "\n",
    "def process_row(\n",
    "    row_dict: Dict,\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 30,\n",
    ") -> Dict:\n",
    "\n",
    "    output = builder.build_message(row_dict)\n",
    "    messages = output[\"messages\"]\n",
    "    \n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(device)\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0][input_len:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    step_logits = outputs.scores[-2][0]\n",
    "    \n",
    "    k = int(row_dict.get(\"choices_len\", 5))\n",
    "    \n",
    "    topk_values, topk_indices = torch.topk(step_logits, k=k)\n",
    "    probs_full = torch.softmax(step_logits, dim=-1)\n",
    "    \n",
    "    topk_candidates = []\n",
    "    for rank, (logit_val, token_id) in enumerate(zip(topk_values, topk_indices)):\n",
    "        topk_candidates.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"token_id\": token_id.item(),\n",
    "            \"token\": tokenizer.decode([token_id.item()]),\n",
    "            \"logit\": logit_val.item(),\n",
    "            \"prob\": probs_full[token_id].item(),\n",
    "        })\n",
    "    \n",
    "    answer = extract_answer(generated_text)\n",
    "    \n",
    "    return {\n",
    "        \"id\": row_dict.get(\"id\"),\n",
    "        \"answer\": answer,\n",
    "        \"top_candidates\": topk_candidates,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"prompt\": prompt_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ab8e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: generation-for-nlp-1565\n",
      "Answer: 1\n",
      "\n",
      "Top 5 Candidates:\n",
      "  Rank 1: Token='1' | Logit=32.5000 | Prob=0.9999\n",
      "  Rank 2: Token='2' | Logit=22.5469 | Prob=0.0000\n",
      "  Rank 3: Token='3' | Logit=20.4219 | Prob=0.0000\n",
      "  Rank 4: Token='0' | Logit=19.2188 | Prob=0.0000\n",
      "  Rank 5: Token='4' | Logit=18.8125 | Prob=0.0000\n",
      "\n",
      "Generated Text: 1\n"
     ]
    }
   ],
   "source": [
    "row_dict = test_df.iloc[800].to_dict()\n",
    "result = process_row_with_logits(\n",
    "    row_dict=row_dict,\n",
    "    builder=builder,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"ID: {result['id']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nTop 5 Candidates:\")\n",
    "for cand in result['top5_candidates']:\n",
    "    print(f\"  Rank {cand['rank']}: Token='{cand['token']}' | Logit={cand['logit']:.4f} | Prob={cand['prob']:.4f}\")\n",
    "print(f\"\\nGenerated Text: {result['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7c55db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-1565',\n",
       " 'answer': '1',\n",
       " 'top5_candidates': [{'rank': 1,\n",
       "   'token_id': 16,\n",
       "   'token': '1',\n",
       "   'logit': 32.5,\n",
       "   'prob': 0.9999415874481201},\n",
       "  {'rank': 2,\n",
       "   'token_id': 17,\n",
       "   'token': '2',\n",
       "   'logit': 22.546875,\n",
       "   'prob': 4.757594069815241e-05},\n",
       "  {'rank': 3,\n",
       "   'token_id': 18,\n",
       "   'token': '3',\n",
       "   'logit': 20.421875,\n",
       "   'prob': 5.6821354519343e-06},\n",
       "  {'rank': 4,\n",
       "   'token_id': 15,\n",
       "   'token': '0',\n",
       "   'logit': 19.21875,\n",
       "   'prob': 1.706086436570331e-06},\n",
       "  {'rank': 5,\n",
       "   'token_id': 19,\n",
       "   'token': '4',\n",
       "   'logit': 18.8125,\n",
       "   'prob': 1.1364986676198896e-06}],\n",
       " 'generated_text': '1',\n",
       " 'prompt': '<|im_start|>system\\n당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다.\\n이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다.\\n당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.<|im_end|>\\n<|im_start|>user\\n### 지문\\n아파트 2300여가구를 지을 수 있는 경기 수원 광교신도시 주상복합용지 C2블록이 지난 3일 최고가 매각 입찰에서 내정가격 5644억원의 133%에 달하는 7507억원을 써낸 중흥건설에 돌아갔다. 현대건설 등 대형 건설사 컨소시엄 및 호반건설과 반도건설, 아이에스동서 등 중견 건설사는 물론 개발업체인 엠디엠 등 나머지 업체들도 6000억~7000억원을 써냈을 정도로 경쟁이 치열했다. 인근 수원 삼성디지털시티 근무자들이 새 집을 찾으면서 2010년 4억8000만원대에 분양한 ‘광교 자연앤 자이’ 전용 101㎡는 이달 분양가보다 2억원가량 비싼 7억원으로 올랐다. 경기도청 이전 재추진과 서울 강남을 잇는 신분당선 연장선 개통 등을 앞둔 광교신도시로 건설업계와 실수요자들이 몰리고 있다.◆수도권 신도시 중 집값 상승률 1위광교신도시 인기는 집값에서 확인된다. 부동산114에 따르면 광교신도시는 최근 1년간(2013년 10월~2014년 10월) 아파트값이 7.68% 오르며 15개 수도권 1·2기 신도시 중 상승률 1위를 기록했다. 2위인 안양 평촌(3.49%)과 3위인 성남 분당(3.32%) 아파트값 상승률의 2배에 달한다. 1순위 신도시로 꼽혀온 분당 집값도 뛰어넘었다. 지난 3월 말 3.3㎡당 1484만원으로 분당(1473만원)을 넘어선 광교 아파트값이 지난달에는 1542만원까지 올라 1494만원을 기록한 분당과의 격차를 더 벌렸다.입주 3년차를 맞아 편의시설이 확충됨에 따라 집값이 분양가보다 2억원 이상 뛴 단지들도 등장했다. ‘자연앤힐스테이트’ 전용 84㎡는 분양가가 3억8000만원대였지만 이달 거래가격은 5억5000만~5억9000만원까지 올랐다. 조망이 좋은 호수공원 일대는 프리미엄이 더 높다. 호수와 맞닿은 ‘에일린의 뜰’ 연립주택 테라스하우스 전용 134㎡ 매매가격은 13억~15억원으로 호수 조망이 안 되는 같은 면적 집보다 3억~4억원 비싸다. 여기에 경기도가 2017년 신청사 준공을 목표로 연말까지 설계를 확정할 예정인 데다 신분당선 연장선도 2016년 초 개통 예정이다.◆5000여가구 주상복합 분양‘광교 노른자’로 꼽히는 호수공원 주변에서는 주상복합 아파트 네 곳이 잇따라 분양된다. 네 곳 땅값만 1조4000억원에 달한다. 이달 분양에 들어가는 첫 타자인 ‘힐스테이트 광교’(D3블록) 주상복합은 호수공원과 맞닿아 있어 청약 경쟁이 치열할 것으로 예상되고 있다. 아파트는 지상 49층, 6개동 전용 97~155㎡ 928가구로 구성된다. 오피스텔은 지상 20층, 2개동 전용 45~84㎡ 172실 규모다. 이어 내년 상반기에는 호수공원 남쪽인 C3블록에서 개발업체인 네오밸류가 주상복합 987가구를 분양할 예정이다. 이어 대상산업도 이웃한 C4블록에서 내년 하반기에 주상복합 686가구를 공급한다. 이번에 가구 수가 가장 많은 C2블록 부지를 사들인 중흥건설도 내년 하반기 주상복합 2300가구를 선보인다. 손지호 네오밸류 대표는 “주상복합 상가 점포의 80%가량을 회사가 직접 소유한 채 임대해 상권을 활성화시킬 방침”이라고 말했다.\\n\\n### 질문\\n광교신도시에서 아파트값 상승률 1위를 기록한 기간은 언제인가?\\n\\n### 선택지\\n1. 2013년 10월~2014년 10월\\n2. 2014년 10월~2015년 10월\\n3. 2015년 10월~2016년 10월\\n4. 2016년 10월~2017년 10월\\n5. 2017년 10월~2018년 10월\\n\\n### 문제 해결 가이드라인\\n1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\\n2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\\n3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\\n4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\\n5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\\n\\n정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\\n정답:<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31be7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(\n",
    "    row_dict: Dict,\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 30,\n",
    ") -> Dict:\n",
    "\n",
    "    output = builder.build_message(row_dict)\n",
    "    messages = output[\"messages\"]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(device)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs.sequences[0][input_len:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    step_logits = outputs.scores[-2][0]\n",
    "\n",
    "    k = int(row_dict.get(\"choices_len\", 5))\n",
    "\n",
    "    topk_values, topk_indices = torch.topk(step_logits, k=k)\n",
    "    probs_full = torch.softmax(step_logits, dim=-1)\n",
    "\n",
    "    topk_candidates = []\n",
    "    for rank, (logit_val, token_id) in enumerate(zip(topk_values, topk_indices)):\n",
    "        topk_candidates.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"token_id\": token_id.item(),\n",
    "            \"token\": tokenizer.decode([token_id.item()]),\n",
    "            \"logit\": logit_val.item(),\n",
    "            \"prob\": probs_full[token_id].item(),\n",
    "        })\n",
    "\n",
    "    digit_tokens = [str(i) for i in range(1, k + 1)]\n",
    "    digit_token_ids = []\n",
    "\n",
    "    for digit in digit_tokens:\n",
    "        encoded = tokenizer.encode(digit, add_special_tokens=False)\n",
    "        digit_token_ids.append(encoded[0])\n",
    "\n",
    "    digit_logits = torch.tensor([step_logits[tid].item() for tid in digit_token_ids])\n",
    "    digit_probs = torch.softmax(digit_logits, dim=-1)\n",
    "\n",
    "    top2_digit_values, top2_digit_indices = torch.topk(digit_logits, k=min(2, k))\n",
    "    if k >= 2:\n",
    "        logit_gap = (top2_digit_values[0] - top2_digit_values[1]).item()\n",
    "    else:\n",
    "        logit_gap = 0.0\n",
    "\n",
    "    answer = extract_answer(generated_text)\n",
    "\n",
    "    return {\n",
    "        \"id\": row_dict.get(\"id\"),\n",
    "        \"answer\": answer,\n",
    "        \"digit_probs\": digit_probs.tolist(),\n",
    "        \"logit_gap\": logit_gap,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"prompt\": prompt_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "781e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = test_df.iloc[123].to_dict()\n",
    "result = process_row(\n",
    "    row_dict=row_dict,\n",
    "    builder=builder,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "869a5e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 123,\n",
       " 'id': 'generation-for-nlp-123',\n",
       " 'paragraph': '(가 ) 동녁 두던 밧긔 크나큰 너븐 들\\uf550 만경(萬頃)  황운(黃雲)이 \\uf53a 빗치 되야 잇다 중양이 거의로다 내노리 \\uf537 쟈스라 블근 게  여믈고 눌은 \\ue390기 \\ue982져시니 술이 니글션졍 버디야 업\\ue982 소냐 전가(田家)  흥미\\ue285 날로 기퍼 가노매라 살여흘 긴 몰래예 밤블이 \\ue64f 가시니 ㉠게 잡\\ue285 아\\uf550\\ue38f이 그\\ue566\\u200b \\u200b을 흣텨 잇고 호두포* 엔 구븨 예 아젹 믈이  미러오니 ㉡돗\\ue38d\\u200b\\u200b\\ue668 애내성(欸 乃聲)*이 고기 \\uf48e\\ue285 댱\\ue999로다 경 (景 )도 됴커니와 생리(生理)라  괴로오랴 (중략) 어와 이 청경( 淸景)  갑시 이실 거시런\\ue38f 적막히 다든 문애 내  분으로 드려오랴 사조(私照)* 업다 호미 거즌말 아니로다 ㉢모재(茅齋)*예 빗쵠 빗치 옥루( 玉樓)라 다\\ue470 소냐 청준(淸樽)을 밧\\ueb82\\u200b  열고 큰 잔의 \\ue1a7\\ue38b 브어 ㉣죽엽(竹葉)  \\ue1a7\\u200b \\ue285\\u200b\\u200b  술\\ue470 \\ue38f빗 조차 거후로니 표연\\uf53a 일흥( 逸興)이 져기면 \\ue288리로다 이적선( 李謫仙)  이려\\uf537야 \\ue38f을 보고 밋치닷다 춘하추동애 경물이 아름답고 주야조모( 晝夜朝暮) 애 완상이 새로오니 ㉤몸이 한가\\uf537 나 귀 눈은 겨\\ue470 업다 여생이 언마치리 백발이 날로 기니 세상 공명은 계륵이나 다\\ue470 소냐 ⓐ강호 어조( 魚鳥)애 새 \\ue587셰 깁퍼시니 옥당금마( 玉堂金馬)*의 몽혼( 夢魂)*이 섯긔엿다 초당연월( 草堂煙月) 의 시\\ue477 업시 누워 이셔 촌주강어( 村酒江魚) 로 장일취( 長日醉) \\ue470 원 (願 )\\uf537노라 이 몸이 이러구롬도 역군은( 亦君恩) 이샷다 -신계영,  ｢월선헌십육경가｣ - *호두포 :예산현의 무한천 하류. *애내성 :어부가 노를 저으면서 부르는 노랫소리 . *사조 :사사로이 비춤 . *모재 :띠로 지붕을 이어 지은 집. *옥당금마 :관직 생활 . *몽혼 :꿈 . (나 ) 어촌(漁村)은 나의 벗 공백공의 자호( 自號)다 .  백공은 나와  태어난 해는 같으나 생일이 뒤이기 때문에 내가 아우라고 한다.   풍채와 인품이 소탈하고 명랑하여 사랑할 만하다.  대과에 급제 하고 좋은 벼슬에 올라,  갓끈을 나부끼고 인끈을 두르고 필기를   위한 붓을 귀에 꽂고 나라의 옥새를 주관하니 ,  사람들은 진실로   그에게 원대한 기대를 하였으나 ,  담담하게 강호의 취미를 지니고   있다.  가끔 흥이 무르익으면,  ｢어부사｣ 를 노래한다.  그 음성이 맑고 밝아서 천지에 가득 찰 것 같다.  증자가 상송 (商頌 )을 노래 하는 것을 듣는 듯하여,  사람의 가슴으로 하여금 멀리 강호에  있는 것 같게 만든다.  이것은 그의 마음에 사욕이 없 어 사물에  초탈하였기 때문에 소리의 나타남이 이와 같은 것이다 . 하루는 나에게 말하기를, “나의 뜻은 어부( 漁父 )에 있다.  그대는 어부의 즐거움을 아는가.   강태공은 성인이니 내가  감히  그가 주 문왕을 만난 것과 같은   그런 만남을 기약할 수 없다.  엄자릉은 현인이니 내가 감히   그의 깨끗함을 바랄 수는 없다.  ㉥아이와 어른들을 데리고  갈매기와 백로를 벗하며  어떤 때는 낚싯대를 잡고 ,  ㉦외로운  배를 노 저어 조류를 따라 오르고 내리면서 가는 대로 맡겨  두고 ,  모래가 깨끗하면 뱃줄을 매어 두고 산이 좋으면 그 가 운데를 흘러간다 .  ㉧구운 고기와 신선한 생선회로 술잔을 들어   주고받다가  해가 지고 달이 떠오르며 바람은 잔잔하고 물결이   고요한 때에는 배에 기대어 길게 휘파람을 불며 ,  돛대를 치고   큰 소리로 노래를 부른다.  ㉨흰 물결을 일으키고 맑은 빛을  헤치면 ,  멀고 멀어서 마치 성사*를 타고 하늘에 오르는 것 같다.   강의 연기가 자욱하고 짙은 안개가 내리면,  도롱이와 삿갓을  걸치고 그물을 걷어 올리면 금빛 같은 비늘과 옥같이 흰 꼬리의   물고기가 제멋대로 펄떡거리며 뛰는 모습은 ㉩넉넉히 눈을  즐겁게 하고 마음을 기쁘게 한다 .  밤이 깊어 구름은 어둡고  하늘이 캄캄하면 사방은 아득하기만 하다 .  어촌의 등불은 가물 거리는데 배의 지붕에 빗소리는 울어 느리다가 빠르다가 우수수 하는 소리가 차갑고도 슬프다 .  …(중략 )…  여름날 뜨거운 햇빛에   더위가 쏟아질 적엔 버드나무 늘어진 낚시터에 미풍이 불고 ,   겨울 하늘에 눈이 날릴 때면 차가운 강물에서 홀로 낚시를  드리운다.  사계절이 차례로 바뀌건만 어부의 즐거움은 없는  때가 없다. 저 영달에 얽매여 벼슬하는 자는 구차하게 영화에 매달리지만   나는 만나는 대로 편안하다.  빈궁하여 고기잡이를 하는 자는  구차하게 이익을 계산하지만 나는 스스로 유유자적을 즐긴다.   성공과 실패는 운명에 맡기고 ,  진퇴도 오직 때를 따를 뿐이다.   부귀 보기를 뜬구름과 같이 하고 공명을 헌신짝 벗어 버리듯   하여 ,  스스로 세상의 물욕 밖에서 방랑하는 것이니 ,  어찌 시세에   영합하여 이름을 낚시질하고 ,  벼슬길에 빠져들어 생명을 가볍게   여기며 이익만 취하다가 스스로 함정에 빠지는 자와 같겠는가 . ⓑ이것이 내가 몸은 벼슬을 하면서도 뜻은 강호에 두어 매양   노래에 의탁하는 것이니,  그대는 어떻게 생각하는가? ” 하니 내가 듣고 즐거워하며 그대로 기록하여 백공에게 보내고 ,   또한 나 자신도 살피고자 한다.  을축년 7월 어느 날. -권근 ,  ｢어촌기｣ - *성사 :옛날 장건이 타고 하늘에 다녀왔다고 하는 배 .[A ] ',\n",
       " 'question': '<보기>를 참고하여 (나 )를 이해한 내용으로 적절하지 않은   것은?',\n",
       " 'choices': ['벗이 ‘영화’와 ‘이익’을 중시하는 삶을 거부한다는 것을 통해  벗의 가치관을 알 수 있군.',\n",
       "  '작가가 벗의 말을 ‘즐거워하며 ’  자신도 살피려 하는 것을 통해   작가는 벗의 생각에 공감하고 있음을 알 수 있군 .',\n",
       "  '작가가 벗을 ‘아우’로 삼고 있다는 것을 통해 벗이 추구하는  삶의 자세가 작가로부터 전해 받은 것임을 알 수 있군 .',\n",
       "  '벗이 ‘강태공’과 ‘엄자릉’을 들어 ‘내가 감히’ 라는 말을 언급한  것을 통해 그들의 삶에 미치지 못함을 스스로 인정하는 벗의 겸손한 성품을 알 수 있군.',\n",
       "  '작가가 벗이 ‘대과에 급제’ 하여 기대를 받고 있는데도 ‘마음에   사욕이 없 ’다고 평한 것을 통해 벗의 말이 기록할 만한 가치가   있다고 여김을 알 수 있군.'],\n",
       " 'answer': '',\n",
       " 'question_plus': ' ｢어촌기｣의 작가는 벗의 말을 인용하여 자신의 생각을 드러 내고 있다.  작가는 벗에 관한 이야기가 기록할 만한 가치가  있다는 근거를 벗과의 관계와 그의 성품에 대한 평을 통해 마련 하고 있다 .  이를 통해 작가는 자신이 추구하는 삶의 방향성과   가치관을 드러내며 벗의 생각에 공감하고 있다 . ',\n",
       " 'choices_len': 5}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af235444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-123',\n",
       " 'answer': '3',\n",
       " 'digit_probs': [0.11904565244913101,\n",
       "  0.0047624255530536175,\n",
       "  0.7884997129440308,\n",
       "  0.0571187287569046,\n",
       "  0.03057345189154148],\n",
       " 'logit_gap': 1.890625,\n",
       " 'generated_text': '3',\n",
       " 'prompt': '<|im_start|>system\\n당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다.\\n이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다.\\n당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.<|im_end|>\\n<|im_start|>user\\n### 지문\\n(가 ) 동녁 두던 밧긔 크나큰 너븐 들\\uf550 만경(萬頃)  황운(黃雲)이 \\uf53a 빗치 되야 잇다 중양이 거의로다 내노리 \\uf537 쟈스라 블근 게  여믈고 눌은 \\ue390기 \\ue982져시니 술이 니글션졍 버디야 업\\ue982 소냐 전가(田家)  흥미\\ue285 날로 기퍼 가노매라 살여흘 긴 몰래예 밤블이 \\ue64f 가시니 ㉠게 잡\\ue285 아\\uf550\\ue38f이 그\\ue566\\u200b \\u200b을 흣텨 잇고 호두포* 엔 구븨 예 아젹 믈이  미러오니 ㉡돗\\ue38d\\u200b\\u200b\\ue668 애내성(欸 乃聲)*이 고기 \\uf48e\\ue285 댱\\ue999로다 경 (景 )도 됴커니와 생리(生理)라  괴로오랴 (중략) 어와 이 청경( 淸景)  갑시 이실 거시런\\ue38f 적막히 다든 문애 내  분으로 드려오랴 사조(私照)* 업다 호미 거즌말 아니로다 ㉢모재(茅齋)*예 빗쵠 빗치 옥루( 玉樓)라 다\\ue470 소냐 청준(淸樽)을 밧\\ueb82\\u200b  열고 큰 잔의 \\ue1a7\\ue38b 브어 ㉣죽엽(竹葉)  \\ue1a7\\u200b \\ue285\\u200b\\u200b  술\\ue470 \\ue38f빗 조차 거후로니 표연\\uf53a 일흥( 逸興)이 져기면 \\ue288리로다 이적선( 李謫仙)  이려\\uf537야 \\ue38f을 보고 밋치닷다 춘하추동애 경물이 아름답고 주야조모( 晝夜朝暮) 애 완상이 새로오니 ㉤몸이 한가\\uf537 나 귀 눈은 겨\\ue470 업다 여생이 언마치리 백발이 날로 기니 세상 공명은 계륵이나 다\\ue470 소냐 ⓐ강호 어조( 魚鳥)애 새 \\ue587셰 깁퍼시니 옥당금마( 玉堂金馬)*의 몽혼( 夢魂)*이 섯긔엿다 초당연월( 草堂煙月) 의 시\\ue477 업시 누워 이셔 촌주강어( 村酒江魚) 로 장일취( 長日醉) \\ue470 원 (願 )\\uf537노라 이 몸이 이러구롬도 역군은( 亦君恩) 이샷다 -신계영,  ｢월선헌십육경가｣ - *호두포 :예산현의 무한천 하류. *애내성 :어부가 노를 저으면서 부르는 노랫소리 . *사조 :사사로이 비춤 . *모재 :띠로 지붕을 이어 지은 집. *옥당금마 :관직 생활 . *몽혼 :꿈 . (나 ) 어촌(漁村)은 나의 벗 공백공의 자호( 自號)다 .  백공은 나와  태어난 해는 같으나 생일이 뒤이기 때문에 내가 아우라고 한다.   풍채와 인품이 소탈하고 명랑하여 사랑할 만하다.  대과에 급제 하고 좋은 벼슬에 올라,  갓끈을 나부끼고 인끈을 두르고 필기를   위한 붓을 귀에 꽂고 나라의 옥새를 주관하니 ,  사람들은 진실로   그에게 원대한 기대를 하였으나 ,  담담하게 강호의 취미를 지니고   있다.  가끔 흥이 무르익으면,  ｢어부사｣ 를 노래한다.  그 음성이 맑고 밝아서 천지에 가득 찰 것 같다.  증자가 상송 (商頌 )을 노래 하는 것을 듣는 듯하여,  사람의 가슴으로 하여금 멀리 강호에  있는 것 같게 만든다.  이것은 그의 마음에 사욕이 없 어 사물에  초탈하였기 때문에 소리의 나타남이 이와 같은 것이다 . 하루는 나에게 말하기를, “나의 뜻은 어부( 漁父 )에 있다.  그대는 어부의 즐거움을 아는가.   강태공은 성인이니 내가  감히  그가 주 문왕을 만난 것과 같은   그런 만남을 기약할 수 없다.  엄자릉은 현인이니 내가 감히   그의 깨끗함을 바랄 수는 없다.  ㉥아이와 어른들을 데리고  갈매기와 백로를 벗하며  어떤 때는 낚싯대를 잡고 ,  ㉦외로운  배를 노 저어 조류를 따라 오르고 내리면서 가는 대로 맡겨  두고 ,  모래가 깨끗하면 뱃줄을 매어 두고 산이 좋으면 그 가 운데를 흘러간다 .  ㉧구운 고기와 신선한 생선회로 술잔을 들어   주고받다가  해가 지고 달이 떠오르며 바람은 잔잔하고 물결이   고요한 때에는 배에 기대어 길게 휘파람을 불며 ,  돛대를 치고   큰 소리로 노래를 부른다.  ㉨흰 물결을 일으키고 맑은 빛을  헤치면 ,  멀고 멀어서 마치 성사*를 타고 하늘에 오르는 것 같다.   강의 연기가 자욱하고 짙은 안개가 내리면,  도롱이와 삿갓을  걸치고 그물을 걷어 올리면 금빛 같은 비늘과 옥같이 흰 꼬리의   물고기가 제멋대로 펄떡거리며 뛰는 모습은 ㉩넉넉히 눈을  즐겁게 하고 마음을 기쁘게 한다 .  밤이 깊어 구름은 어둡고  하늘이 캄캄하면 사방은 아득하기만 하다 .  어촌의 등불은 가물 거리는데 배의 지붕에 빗소리는 울어 느리다가 빠르다가 우수수 하는 소리가 차갑고도 슬프다 .  …(중략 )…  여름날 뜨거운 햇빛에   더위가 쏟아질 적엔 버드나무 늘어진 낚시터에 미풍이 불고 ,   겨울 하늘에 눈이 날릴 때면 차가운 강물에서 홀로 낚시를  드리운다.  사계절이 차례로 바뀌건만 어부의 즐거움은 없는  때가 없다. 저 영달에 얽매여 벼슬하는 자는 구차하게 영화에 매달리지만   나는 만나는 대로 편안하다.  빈궁하여 고기잡이를 하는 자는  구차하게 이익을 계산하지만 나는 스스로 유유자적을 즐긴다.   성공과 실패는 운명에 맡기고 ,  진퇴도 오직 때를 따를 뿐이다.   부귀 보기를 뜬구름과 같이 하고 공명을 헌신짝 벗어 버리듯   하여 ,  스스로 세상의 물욕 밖에서 방랑하는 것이니 ,  어찌 시세에   영합하여 이름을 낚시질하고 ,  벼슬길에 빠져들어 생명을 가볍게   여기며 이익만 취하다가 스스로 함정에 빠지는 자와 같겠는가 . ⓑ이것이 내가 몸은 벼슬을 하면서도 뜻은 강호에 두어 매양   노래에 의탁하는 것이니,  그대는 어떻게 생각하는가? ” 하니 내가 듣고 즐거워하며 그대로 기록하여 백공에게 보내고 ,   또한 나 자신도 살피고자 한다.  을축년 7월 어느 날. -권근 ,  ｢어촌기｣ - *성사 :옛날 장건이 타고 하늘에 다녀왔다고 하는 배 .[A ] \\n\\n### 질문\\n<보기>를 참고하여 (나 )를 이해한 내용으로 적절하지 않은   것은?\\n\\n### 보기\\n ｢어촌기｣의 작가는 벗의 말을 인용하여 자신의 생각을 드러 내고 있다.  작가는 벗에 관한 이야기가 기록할 만한 가치가  있다는 근거를 벗과의 관계와 그의 성품에 대한 평을 통해 마련 하고 있다 .  이를 통해 작가는 자신이 추구하는 삶의 방향성과   가치관을 드러내며 벗의 생각에 공감하고 있다 . \\n\\n### 선택지\\n1. 벗이 ‘영화’와 ‘이익’을 중시하는 삶을 거부한다는 것을 통해  벗의 가치관을 알 수 있군.\\n2. 작가가 벗의 말을 ‘즐거워하며 ’  자신도 살피려 하는 것을 통해   작가는 벗의 생각에 공감하고 있음을 알 수 있군 .\\n3. 작가가 벗을 ‘아우’로 삼고 있다는 것을 통해 벗이 추구하는  삶의 자세가 작가로부터 전해 받은 것임을 알 수 있군 .\\n4. 벗이 ‘강태공’과 ‘엄자릉’을 들어 ‘내가 감히’ 라는 말을 언급한  것을 통해 그들의 삶에 미치지 못함을 스스로 인정하는 벗의 겸손한 성품을 알 수 있군.\\n5. 작가가 벗이 ‘대과에 급제’ 하여 기대를 받고 있는데도 ‘마음에   사욕이 없 ’다고 평한 것을 통해 벗의 말이 기록할 만한 가치가   있다고 여김을 알 수 있군.\\n\\n### 문제 해결 가이드라인\\n1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\\n2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\\n3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\\n4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\\n5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\\n\\n정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\\n정답:<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873eaaa",
   "metadata": {},
   "source": [
    "### 실제 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343f275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pred_df = pd.read_csv(project_root / \"submission_aug2_2step.csv\")\n",
    "sub = pd.read_csv(project_root / \"submission_aug2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82791fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           869 non-null    object \n",
      " 1   answer       869 non-null    object \n",
      " 2   digit_probs  869 non-null    object \n",
      " 3   logit_gap    869 non-null    float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 27.3+ KB\n"
     ]
    }
   ],
   "source": [
    "pred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bdedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      869 non-null    object\n",
      " 1   answer  869 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 13.7+ KB\n"
     ]
    }
   ],
   "source": [
    "sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b3ec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_df.index == sub.index).all()\n",
    "m = pred_df[['id','answer']].merge(\n",
    "    sub[['id','answer']],\n",
    "    on='id',\n",
    "    how='inner',\n",
    "    suffixes=('_pred','_sub')\n",
    ")\n",
    "\n",
    "a = m['answer_pred'].astype(str).str.strip()\n",
    "b = m['answer_sub'].astype(str).str.strip()\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aba4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer_pred</th>\n",
       "      <th>answer_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>generation-for-nlp-741</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id answer_pred  answer_sub\n",
       "517  generation-for-nlp-741          no           4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = (a != b)\n",
    "m.loc[diff, ['id','answer_pred','answer_sub']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d663c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    869.000000\n",
       "mean       4.625827\n",
       "std        3.572176\n",
       "min        0.015625\n",
       "25%        1.500000\n",
       "50%        3.718750\n",
       "75%        7.265625\n",
       "max       13.312500\n",
       "Name: logit_gap, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['logit_gap'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c578c",
   "metadata": {},
   "source": [
    "### 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97a2d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 답변: 25 / 869 (2.88%)\n",
      "                          id  answer_1step  answer_2step\n",
      "36     generation-for-nlp-36             1             3\n",
      "260   generation-for-nlp-260             1             2\n",
      "278   generation-for-nlp-278             2             3\n",
      "308   generation-for-nlp-308             1             2\n",
      "321   generation-for-nlp-321             3             5\n",
      "336   generation-for-nlp-336             1             4\n",
      "367   generation-for-nlp-367             5             1\n",
      "398   generation-for-nlp-398             2             5\n",
      "423   generation-for-nlp-423             2             3\n",
      "440   generation-for-nlp-681             2             4\n",
      "456   generation-for-nlp-614             1             2\n",
      "475  generation-for-nlp-2465             2             5\n",
      "513   generation-for-nlp-633             2             3\n",
      "601   generation-for-nlp-970             4             3\n",
      "606  generation-for-nlp-1061             1             2\n",
      "639  generation-for-nlp-2370             1             4\n",
      "642   generation-for-nlp-649             2             4\n",
      "643   generation-for-nlp-511             1             4\n",
      "646   generation-for-nlp-811             1             4\n",
      "654   generation-for-nlp-549             2             4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(project_root / \"submission_aug2.csv\")  # 1-step\n",
    "df2 = pd.read_csv(project_root / \"submission_aug2_2step.csv\")  # 새로 생성된 2-step\n",
    "\n",
    "merged = df1.merge(df2, on=\"id\", suffixes=(\"_1step\", \"_2step\"))\n",
    "changed = merged[merged[\"answer_1step\"] != merged[\"answer_2step\"]]\n",
    "\n",
    "print(f\"변경된 답변: {len(changed)} / {len(merged)} ({len(changed)/len(merged)*100:.2f}%)\")\n",
    "if len(changed) > 0:\n",
    "    print(changed[[\"id\", \"answer_1step\", \"answer_2step\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3f7888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출용 파일 저장: submission_aug2_2step_submit.csv\n",
      "분석용 파일 저장: submission_aug2_2step_full.csv (digit_probs, logit_gap 포함)\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터 로드 (분석용)\n",
    "df2_full = pd.read_csv(project_root / \"submission_aug2_2step.csv\")\n",
    "\n",
    "# 제출용 (id, answer만)\n",
    "df2_submission = df2_full[[\"id\", \"answer\"]].copy()\n",
    "\n",
    "# 제출용 파일 저장\n",
    "df2_submission.to_csv(project_root / \"submission_aug2_2step_submit.csv\", index=False)\n",
    "print(f\"제출용 파일 저장: submission_aug2_2step_submit.csv\")\n",
    "\n",
    "# 분석용 파일 저장 (전체)\n",
    "df2_full.to_csv(project_root / \"submission_aug2_2step_full.csv\", index=False)\n",
    "print(f\"분석용 파일 저장: submission_aug2_2step_full.csv (digit_probs, logit_gap 포함)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9987b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 답변: 65 / 869 (7.48%)\n",
      "                         id  answer_1step  answer_2step\n",
      "31    generation-for-nlp-31             1             2\n",
      "36    generation-for-nlp-36             1             3\n",
      "41    generation-for-nlp-41             1             3\n",
      "42    generation-for-nlp-42             5             3\n",
      "64    generation-for-nlp-64             1             3\n",
      "67    generation-for-nlp-67             4             5\n",
      "146  generation-for-nlp-146             4             3\n",
      "162  generation-for-nlp-162             2             3\n",
      "194  generation-for-nlp-194             1             2\n",
      "198  generation-for-nlp-198             1             2\n",
      "216  generation-for-nlp-216             4             5\n",
      "260  generation-for-nlp-260             1             2\n",
      "262  generation-for-nlp-262             3             4\n",
      "270  generation-for-nlp-270             3             2\n",
      "273  generation-for-nlp-273             2             5\n",
      "278  generation-for-nlp-278             2             1\n",
      "281  generation-for-nlp-281             2             5\n",
      "283  generation-for-nlp-283             2             5\n",
      "285  generation-for-nlp-285             5             2\n",
      "286  generation-for-nlp-286             1             4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(project_root / \"submission_aug2.csv\")  # 1-step\n",
    "df2 = pd.read_csv(project_root / \"submission_aug2_2step2_full.csv\")  # 새로 생성된 2-step\n",
    "\n",
    "merged = df1.merge(df2, on=\"id\", suffixes=(\"_1step\", \"_2step\"))\n",
    "changed = merged[merged[\"answer_1step\"] != merged[\"answer_2step\"]]\n",
    "\n",
    "print(f\"변경된 답변: {len(changed)} / {len(merged)} ({len(changed)/len(merged)*100:.2f}%)\")\n",
    "if len(changed) > 0:\n",
    "    print(changed[[\"id\", \"answer_1step\", \"answer_2step\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9fb43f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 답변: 30 / 869 (3.45%)\n",
      "                          id  answer_1step  answer_2step\n",
      "36     generation-for-nlp-36             1             3\n",
      "260   generation-for-nlp-260             1             2\n",
      "278   generation-for-nlp-278             2             3\n",
      "294   generation-for-nlp-294             2             5\n",
      "308   generation-for-nlp-308             1             2\n",
      "321   generation-for-nlp-321             3             5\n",
      "336   generation-for-nlp-336             1             4\n",
      "367   generation-for-nlp-367             5             1\n",
      "398   generation-for-nlp-398             2             5\n",
      "423   generation-for-nlp-423             2             3\n",
      "440   generation-for-nlp-681             2             4\n",
      "456   generation-for-nlp-614             1             2\n",
      "475  generation-for-nlp-2465             2             5\n",
      "513   generation-for-nlp-633             2             3\n",
      "594   generation-for-nlp-926             4             2\n",
      "601   generation-for-nlp-970             4             3\n",
      "606  generation-for-nlp-1061             1             2\n",
      "639  generation-for-nlp-2370             1             4\n",
      "642   generation-for-nlp-649             2             4\n",
      "643   generation-for-nlp-511             1             4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(project_root / \"submission_aug2.csv\")  # 1-step\n",
    "df2 = pd.read_csv(project_root / \"submission_aug2_2step3.csv\")  # 새로 생성된 2-step\n",
    "\n",
    "merged = df1.merge(df2, on=\"id\", suffixes=(\"_1step\", \"_2step\"))\n",
    "changed = merged[merged[\"answer_1step\"] != merged[\"answer_2step\"]]\n",
    "\n",
    "print(f\"변경된 답변: {len(changed)} / {len(merged)} ({len(changed)/len(merged)*100:.2f}%)\")\n",
    "if len(changed) > 0:\n",
    "    print(changed[[\"id\", \"answer_1step\", \"answer_2step\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c47356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
