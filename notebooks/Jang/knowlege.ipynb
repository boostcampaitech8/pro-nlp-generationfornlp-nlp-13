{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4992751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d161f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb893e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             2031 non-null   object \n",
      " 1   paragraph      2031 non-null   object \n",
      " 2   problems       2031 non-null   object \n",
      " 3   question_plus  0 non-null      float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 63.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '/data/ephemeral/pro-nlp-generationfornlp-nlp-13'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "dataset.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea49cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     869 non-null    int64 \n",
      " 1   id             869 non-null    object\n",
      " 2   paragraph      869 non-null    object\n",
      " 3   problems       869 non-null    object\n",
      " 4   question_plus  44 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 34.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "test_df.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)\n",
    "test_df[\"choices_len\"] = test_df[\"choices\"].apply(len)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423769b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choices_len\n",
       "5    1239\n",
       "4     792\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"choices_len\"] = df[\"choices\"].apply(len)\n",
    "df['choices_len'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8f839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df[df['choices_len'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857ae3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 673\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    df4,\n",
    "    test_size=0.15,        \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "valid_ds = Dataset.from_pandas(valid_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38073318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d0fb48ad25422982ea28baf83fb1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\"\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3622e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 System Prompt 함수\n",
    "def get_system_message(row, system_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 하나의 데이터 행\n",
    "    system_prompts: {choices_len: {version: prompt_text}}\n",
    "    prompt_policy: {choices_len: version}\n",
    "    \"\"\"\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    return system_prompts[choices_len][version]\n",
    "\n",
    "SYSTEM_PROMPT_4_V1 = (\n",
    "    \"당신은 **지식 추론(Knowledge Inference) 전문가**입니다. \"\n",
    "    \"이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. \"\n",
    "    \"지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 \"\n",
    "    \"가장 타당한 선택지 하나를 고르십시오.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_5_V1 = (\n",
    "    \"당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. \"\n",
    "    \"이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. \"\n",
    "    \"당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\\n\\n\"\n",
    ")\n",
    "\n",
    "# PROMPT_POLICY:\n",
    "# - 이번 실험(run)에서 \"어떤 system prompt 버전을 사용할지\"를 결정하는 설정값\n",
    "# - choices_len(4 or 5) → 사용할 prompt 버전(v1, v2, ...)\n",
    "# - 실험을 바꿀 때는 이 딕셔너리만 수정\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": SYSTEM_PROMPT_4_V1,\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": SYSTEM_PROMPT_5_V1,\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05ff5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 User Prompt 함수\n",
    "def get_user_message(row, user_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 데이터 행\n",
    "    user_prompts: 템플릿 저장소\n",
    "    prompt_policy: 버전 정책\n",
    "    \"\"\"\n",
    "    # 메타 데이터 확인\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    \n",
    "    # 해당 버전의 템플릿 세트 가져오기 (plus, no_plus가 들어있음)\n",
    "    template_set = user_prompts[choices_len][version]\n",
    "    \n",
    "    # 데이터 준비\n",
    "    paragraph = row['paragraph']\n",
    "    question = row['question']\n",
    "    choices_str = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(row['choices'])])\n",
    "    q_plus = row.get('question_plus', None)\n",
    "    \n",
    "    # 분기 처리 및 포맷팅 (여기가 핵심!)\n",
    "    # q_plus가 존재하고, nan이 아닐 때 -> Plus 템플릿 사용\n",
    "    if q_plus and str(q_plus) != 'nan':\n",
    "        return template_set[\"plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question_plus=q_plus, # 여기 들어감\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    # q_plus가 없을 때 -> No Plus 템플릿 사용\n",
    "    else:\n",
    "        return template_set[\"no_plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    \n",
    "# =========================\n",
    "# User Prompt Templates (V1)\n",
    "# =========================\n",
    "\n",
    "# 4지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 4지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "USER_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_5_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_5_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    }\n",
    "}\n",
    "\n",
    "USER_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98fff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Assistant Prompt 함수\n",
    "def get_assistant_message(row):\n",
    "    \"\"\"\n",
    "    Assistant 메시지 생성 함수.\n",
    "    Qwen3 모델의 토크나이저 템플릿이 자동으로 <think> 태그를 처리하므로,\n",
    "    여기서는 순수한 정답(Label) 텍스트만 반환\n",
    "    \"\"\"\n",
    "    return str(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b34b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    \"\"\"\n",
    "    원본 example(row)로부터 학습용 chat messages를 구성한다.\n",
    "    - choices_len(4/5) 및 question_plus 유무에 따라 system/user 프롬프트를 선택\n",
    "    - assistant는 정답 숫자만\n",
    "    - 이후 평가/추적용으로 id, label도 함께 유지\n",
    "    \"\"\"\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "    asst_msg = get_assistant_message(example)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ],\n",
    "        \"label\": int(example[\"answer\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e8dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(example):\n",
    "    \"\"\"\n",
    "    messages(list[dict])를 tokenizer의 chat_template 규칙에 따라\n",
    "    단일 텍스트로 직렬화한다.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  \n",
    "    )\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b4dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example, truncation=True, max_length=2048, padding=False):\n",
    "    \"\"\"\n",
    "    batched=True면 example[\"text\"]는 List[str]\n",
    "    batched=False면 example[\"text\"]는 str\n",
    "    \"\"\"\n",
    "    tok_kwargs = dict(truncation=truncation, padding=padding)\n",
    "    if truncation is True:\n",
    "        tok_kwargs[\"max_length\"] = max_length\n",
    "\n",
    "    out = tokenizer(example[\"text\"], **tok_kwargs)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": out[\"input_ids\"],\n",
    "        \"attention_mask\": out[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f725705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4317ba6c7a4fd3b7e328493d8fad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd16faec216b45879bd94a734df61445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d80708919c4c30b260cbf953461fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b35fc5bcabe4e6393e9cdf7ce6fdbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f596df4d2944c88991b80b73365e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc2fbaa0614a7c903016f1accc9aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"truncation\": True, \"max_length\": 2048, \"padding\": False},\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9f4a2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 변환 완료 ===\n",
      "Train 개수: 673\n",
      "첫 번째 샘플 Keys: dict_keys(['id', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 변환 완료 ===\")\n",
    "print(\"Train 개수:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"첫 번째 샘플 Keys:\", tokenized_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf909b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 673\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c26e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32a2c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels, pos_from_tail=4):\n",
    "    \"\"\"\n",
    "    반환: (batch, 5)  -> '1'~'5'에 해당하는 logits만 뽑아서 metrics 단계로 전달\n",
    "    \"\"\"\n",
    "    # Trainer가 (logits, ...) 튜플을 줄 때가 있어서 정리\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # (B, L, V)\n",
    "\n",
    "    # labels: (B, L), pad/무시 영역은 -100일 가능성이 큼\n",
    "    # real_len = 마지막으로 labels != -100 인 위치 + 1 로 복원\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "    not_ignored = (labels_t != -100)\n",
    "\n",
    "    # 샘플별로 마지막 not_ignored 위치 찾기\n",
    "    # (뒤에서부터 True 찾기)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)          # (B,)\n",
    "    has_any = not_ignored.any(dim=1)                             # (B,)\n",
    "    # real_len = seq_len - last_true_from_end\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "\n",
    "    # 만약 labels가 전부 -100인 샘플이 있으면(비정상) 그냥 seq_len로 처리\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos = (real_len - pos_from_tail).clamp(min=0, max=seq_len-1) # (B,)\n",
    "\n",
    "    # (B, V)로 해당 위치의 logits만 gather\n",
    "    logits_t = torch.as_tensor(logits)                           # (B, L, V)\n",
    "    batch_idx = torch.arange(logits_t.size(0), device=logits_t.device)\n",
    "    picked = logits_t[batch_idx, pos, :]                         # (B, V)\n",
    "\n",
    "    # digit ids만 슬라이스 -> (B, 5)\n",
    "    picked_digits = picked[:, DIGIT_IDS]\n",
    "    return picked_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2be1d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, label_pos_from_tail=3):\n",
    "    \"\"\"\n",
    "    eval_pred:\n",
    "      - (predictions, label_ids) 튜플 형태가 가장 흔함\n",
    "      - predictions: preprocess_logits_for_metrics가 반환한 (B, 5)\n",
    "      - label_ids: (B, L) with -100 ignored\n",
    "    반환: {\"accuracy\": ..., \"macro_f1\": ...}\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "    preds_t = torch.as_tensor(preds)\n",
    "    pred_cls = torch.argmax(preds_t, dim=-1).cpu().numpy().astype(np.int64)  # (B,)\n",
    "\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "\n",
    "    not_ignored = (labels_t != -100)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)\n",
    "    has_any = not_ignored.any(dim=1)\n",
    "\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos_label = (real_len - label_pos_from_tail).clamp(min=0, max=seq_len - 1)\n",
    "    batch_idx = torch.arange(labels_t.size(0), device=labels_t.device)\n",
    "    gold_tok = labels_t[batch_idx, pos_label].cpu().numpy().astype(np.int64) \n",
    "\n",
    "    gold_cls = gold_tok - DIGIT_IDS[0]  \n",
    "\n",
    "    valid = (gold_cls >= 0) & (gold_cls < 5)\n",
    "    pred_cls = pred_cls[valid]\n",
    "    gold_cls = gold_cls[valid]\n",
    "\n",
    "    acc = (pred_cls == gold_cls).mean() if len(gold_cls) > 0 else 0.0\n",
    "\n",
    "    f1s = []\n",
    "    for c in range(5):\n",
    "        tp = np.sum((pred_cls == c) & (gold_cls == c))\n",
    "        fp = np.sum((pred_cls == c) & (gold_cls != c))\n",
    "        fn = np.sum((pred_cls != c) & (gold_cls == c))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_f1 = float(np.mean(f1s)) if len(f1s) > 0 else 0.0\n",
    "\n",
    "    return {\"accuracy\": float(acc), \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb41086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_dataset = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee5e80dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103a6902317b414fa647351e0c4ff1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Pad Token: <|endoftext|> (151643)\n",
      "Use Cache: False, Grad Ckpt: True\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 이름 및 양자화 설정\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 2. 토크나이저 먼저 로드 및 PAD 설정 (중요!)\n",
    "# 모델 로드 전에 PAD 토큰을 확정지어야 나중에 모델 설정에 바로 반영됩니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. 모델 로드 (양자화 및 장치 할당)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 4. 모델 설정 업데이트\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 토크나이저의 PAD 설정을 모델에 동기화\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 확인 출력\n",
    "print(f\"Final Pad Token: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
    "print(f\"Use Cache: {model.config.use_cache}, Grad Ckpt: {model.is_gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21c9b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attention proj만\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Attention + MLP까지 (성능 더 노리되 trainable 조금 증가)\n",
    "# \"gate_proj\", \"up_proj\", \"down_proj\" -> FFN\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# target_modules = [\"q_proj\", \"k_proj\"]\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../../qwen-sft-results\",\n",
    "    \n",
    "    num_train_epochs=2,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 전략 설정\n",
    "    eval_strategy=\"steps\",       # 주석 해제 (활성화)\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",          # 저장 안 함 (비교용)\n",
    "    load_best_model_at_end=False, # 저장 안 할 때는 False가 안전\n",
    "    \n",
    "    # 지표 및 로깅\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "663ffb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3b43909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# 기존 객체 삭제\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'model' in locals(): del model\n",
    "\n",
    "# 가비지 컬렉션 및 CUDA 캐시 비우기\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b3a1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 23:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.122589</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.559377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.111150</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.540536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.108635</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.565741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.111214</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.525989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.123531</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.530332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.120189</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.542736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.112962</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.531787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.112050</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.524727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=0.3064488097148783, metrics={'train_runtime': 1406.8652, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.121, 'total_flos': 4.16753022432768e+16, 'train_loss': 0.3064488097148783, 'epoch': 2.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb76ab8",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97978f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...:   0%|          | 0/119 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Validating...: 100%|██████████| 119/119 [01:08<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "infer_results = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    # test_ds_text 대신 eval_dataset(validation set) 사용\n",
    "    for i in tqdm(range(len(eval_dataset)), desc=\"Validating...\"):\n",
    "        # 1. ID와 정답(Ground Truth) 가져오기 (데이터셋 구조에 따라 확인 필요)\n",
    "        # 보통 tokenized_dataset[\"validation\"]에 'id'와 'label'이 남아있다면 그걸 사용합니다.\n",
    "        _id = tokenized_dataset[\"validation\"][i].get(\"id\", i)\n",
    "        gt_answer = tokenized_dataset[\"validation\"][i].get(\"label\", \"unknown\") \n",
    "        \n",
    "        # 2. 입력 데이터 준비 (이미 토크나이즈된 input_ids 사용)\n",
    "        input_ids = torch.tensor(eval_dataset[i]['input_ids']).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.tensor(eval_dataset[i]['attention_mask']).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # 3. 모델 생성\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # 4. 생성된 부분만 잘라내기\n",
    "        input_len = input_ids.shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # 5. 숫자 추출 로직 (뒤에서부터 찾기)\n",
    "        pred = '1'\n",
    "        found = False\n",
    "        for char in reversed(gen_text):\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                pred = char\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        infer_results.append({\n",
    "            \"id\": _id,\n",
    "            \"ground_truth\": gt_answer, # 정답과 비교하기 위해 추가\n",
    "            \"prediction\": pred,\n",
    "            \"raw_output\": gen_text,\n",
    "            \"is_found\": found\n",
    "        })\n",
    "\n",
    "df_val_results = pd.DataFrame(infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61e3a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 경고: 30개의 데이터에서 숫자를 추출하지 못했습니다. 'raw_output'을 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "# 숫자를 못 찾은 비율 확인\n",
    "fail_count = len([res for res in infer_results if not res['is_found']])\n",
    "if fail_count > 0:\n",
    "    print(f\"⚠️ 경고: {fail_count}개의 데이터에서 숫자를 추출하지 못했습니다. 'raw_output'을 확인하세요.\")\n",
    "\n",
    "df_val_results[['id', 'prediction']].to_csv(\"val_results_no_RAG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20443bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'generation-for-nlp-1261',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1063',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1149',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1160',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-566',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-501',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-767',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1048',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-585',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1137',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-678',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-693',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-710',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-506',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-688',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1333',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-715',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-564',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1281',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1056',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '</think>\\n\\n1',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-915',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-668',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-883',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-968',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-792',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-460',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-941',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-822',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-880',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1317',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1093',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-690',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1046',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1211',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-472',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-452',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-735',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1176',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-795',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-965',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-899',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1077',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-897',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1147',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1212',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1055',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-672',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-999',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1295',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-962',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-830',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-998',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1219',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-529',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-825',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-859',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1178',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-719',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1220',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-894',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1064',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1057',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-507',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-519',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1325',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-484',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '</think>\\n\\n1',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-959',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-607',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1373',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1374',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-843',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-464',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-461',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1209',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-854',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-815',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1135',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-924',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-540',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-513',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-518',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1224',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-750',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-743',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1303',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1361',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-828',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1203',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-673',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-988',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1177',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1158',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1192',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1309',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-491',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1319',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-684',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-541',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1254',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-517',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-555',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-686',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1335',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1131',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1367',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-921',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-955',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-526',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-436',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-771',\n",
       "  'ground_truth': 3,\n",
       "  'prediction': '3',\n",
       "  'raw_output': '</think>\\n\\n3',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-933',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1074',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1282',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-1051',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '4',\n",
       "  'raw_output': '</think>\\n\\n4',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-683',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-1142',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-831',\n",
       "  'ground_truth': 4,\n",
       "  'prediction': '1',\n",
       "  'raw_output': '',\n",
       "  'is_found': False},\n",
       " {'id': 'generation-for-nlp-459',\n",
       "  'ground_truth': 1,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '2',\n",
       "  'is_found': True},\n",
       " {'id': 'generation-for-nlp-627',\n",
       "  'ground_truth': 2,\n",
       "  'prediction': '2',\n",
       "  'raw_output': '</think>\\n\\n2',\n",
       "  'is_found': True}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87e4513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>raw_output</th>\n",
       "      <th>is_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-1261</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1063</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-1149</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-1160</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;/think&gt;\\n\\n3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-566</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;/think&gt;\\n\\n4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>generation-for-nlp-683</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>generation-for-nlp-1142</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>generation-for-nlp-831</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>generation-for-nlp-459</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>generation-for-nlp-627</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;/think&gt;\\n\\n2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  ground_truth prediction     raw_output  is_found\n",
       "0    generation-for-nlp-1261             4          1                    False\n",
       "1    generation-for-nlp-1063             1          4              4      True\n",
       "2    generation-for-nlp-1149             4          1                    False\n",
       "3    generation-for-nlp-1160             3          3  </think>\\n\\n3      True\n",
       "4     generation-for-nlp-566             4          4  </think>\\n\\n4      True\n",
       "..                       ...           ...        ...            ...       ...\n",
       "114   generation-for-nlp-683             2          1                    False\n",
       "115  generation-for-nlp-1142             4          1                    False\n",
       "116   generation-for-nlp-831             4          1                    False\n",
       "117   generation-for-nlp-459             1          2              2      True\n",
       "118   generation-for-nlp-627             2          2  </think>\\n\\n2      True\n",
       "\n",
       "[119 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "596d8a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 119/119 [01:07<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "infer_results2 = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    # test_ds_text 대신 eval_dataset(validation set) 사용\n",
    "    for i in tqdm(range(len(eval_dataset)), desc=\"Validating...\"):\n",
    "        # 1. ID와 정답(Ground Truth) 가져오기 (데이터셋 구조에 따라 확인 필요)\n",
    "        # 보통 tokenized_dataset[\"validation\"]에 'id'와 'label'이 남아있다면 그걸 사용합니다.\n",
    "        _id = tokenized_dataset[\"validation\"][i].get(\"id\", i)\n",
    "        gt_answer = tokenized_dataset[\"validation\"][i].get(\"label\", \"unknown\") \n",
    "        \n",
    "        # 2. 입력 데이터 준비 (이미 토크나이즈된 input_ids 사용)\n",
    "        input_ids = torch.tensor(eval_dataset[i]['input_ids']).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.tensor(eval_dataset[i]['attention_mask']).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # 3. 모델 생성\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # 4. 생성된 부분만 잘라내기\n",
    "        input_len = input_ids.shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # 5. 숫자 추출 로직 (뒤에서부터 찾기)\n",
    "        pred = 'N/A' # 기본값을 숫자가 아닌 것으로 변경 (구분용)\n",
    "        found = False\n",
    "        \n",
    "        for char in reversed(gen_text):\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                pred = char\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        infer_results2.append({\n",
    "            \"id\": _id,\n",
    "            \"ground_truth\": gt_answer,\n",
    "            \"prediction\": pred,\n",
    "            \"is_found\": found,\n",
    "            \"raw_output\": repr(gen_text) # repr()을 쓰면 줄바꿈(\\n) 같은 특수기호가 다 보입니다!\n",
    "        })\n",
    "\n",
    "df_val_results_2 = pd.DataFrame(infer_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65778974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>is_found</th>\n",
       "      <th>raw_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-1261</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1063</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-1149</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-1160</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-566</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>generation-for-nlp-501</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generation-for-nlp-767</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>generation-for-nlp-1048</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>generation-for-nlp-585</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>generation-for-nlp-1137</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>generation-for-nlp-678</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>generation-for-nlp-693</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>generation-for-nlp-710</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>generation-for-nlp-506</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>generation-for-nlp-688</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>generation-for-nlp-1333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>generation-for-nlp-715</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>generation-for-nlp-564</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>generation-for-nlp-1281</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>generation-for-nlp-1056</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>generation-for-nlp-915</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>generation-for-nlp-668</td>\n",
       "      <td>3</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>generation-for-nlp-883</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>generation-for-nlp-968</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>generation-for-nlp-792</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>generation-for-nlp-460</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>generation-for-nlp-941</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>generation-for-nlp-822</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>generation-for-nlp-880</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>generation-for-nlp-1317</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>generation-for-nlp-1093</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>generation-for-nlp-690</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>generation-for-nlp-1046</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>generation-for-nlp-1211</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>generation-for-nlp-472</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>generation-for-nlp-452</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>generation-for-nlp-735</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>generation-for-nlp-1176</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>generation-for-nlp-795</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>generation-for-nlp-965</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>generation-for-nlp-899</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>generation-for-nlp-1077</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>generation-for-nlp-897</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>generation-for-nlp-1147</td>\n",
       "      <td>2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>generation-for-nlp-1212</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>generation-for-nlp-1055</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>generation-for-nlp-672</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>generation-for-nlp-999</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>generation-for-nlp-1295</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>generation-for-nlp-962</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>'&lt;/think&gt;\\n\\n4'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  ground_truth prediction  is_found  \\\n",
       "0   generation-for-nlp-1261             4        N/A     False   \n",
       "1   generation-for-nlp-1063             1          4      True   \n",
       "2   generation-for-nlp-1149             4        N/A     False   \n",
       "3   generation-for-nlp-1160             3          3      True   \n",
       "4    generation-for-nlp-566             4          4      True   \n",
       "5    generation-for-nlp-501             3          4      True   \n",
       "6    generation-for-nlp-767             1        N/A     False   \n",
       "7   generation-for-nlp-1048             3          4      True   \n",
       "8    generation-for-nlp-585             2        N/A     False   \n",
       "9   generation-for-nlp-1137             2        N/A     False   \n",
       "10   generation-for-nlp-678             4        N/A     False   \n",
       "11   generation-for-nlp-693             4          4      True   \n",
       "12   generation-for-nlp-710             2        N/A     False   \n",
       "13   generation-for-nlp-506             3          3      True   \n",
       "14   generation-for-nlp-688             1        N/A     False   \n",
       "15  generation-for-nlp-1333             1          2      True   \n",
       "16   generation-for-nlp-715             4          4      True   \n",
       "17   generation-for-nlp-564             1          4      True   \n",
       "18  generation-for-nlp-1281             4          4      True   \n",
       "19  generation-for-nlp-1056             2          1      True   \n",
       "20   generation-for-nlp-915             4        N/A     False   \n",
       "21   generation-for-nlp-668             3        N/A     False   \n",
       "22   generation-for-nlp-883             1          2      True   \n",
       "23   generation-for-nlp-968             3          3      True   \n",
       "24   generation-for-nlp-792             1          2      True   \n",
       "25   generation-for-nlp-460             2          2      True   \n",
       "26   generation-for-nlp-941             3          3      True   \n",
       "27   generation-for-nlp-822             3          3      True   \n",
       "28   generation-for-nlp-880             4          4      True   \n",
       "29  generation-for-nlp-1317             2          2      True   \n",
       "30  generation-for-nlp-1093             2        N/A     False   \n",
       "31   generation-for-nlp-690             4          4      True   \n",
       "32  generation-for-nlp-1046             4          4      True   \n",
       "33  generation-for-nlp-1211             3          3      True   \n",
       "34   generation-for-nlp-472             1          2      True   \n",
       "35   generation-for-nlp-452             1          2      True   \n",
       "36   generation-for-nlp-735             3          3      True   \n",
       "37  generation-for-nlp-1176             4          4      True   \n",
       "38   generation-for-nlp-795             1          2      True   \n",
       "39   generation-for-nlp-965             2        N/A     False   \n",
       "40   generation-for-nlp-899             3          3      True   \n",
       "41  generation-for-nlp-1077             4          4      True   \n",
       "42   generation-for-nlp-897             3          3      True   \n",
       "43  generation-for-nlp-1147             2        N/A     False   \n",
       "44  generation-for-nlp-1212             1          2      True   \n",
       "45  generation-for-nlp-1055             1          4      True   \n",
       "46   generation-for-nlp-672             4        N/A     False   \n",
       "47   generation-for-nlp-999             4          4      True   \n",
       "48  generation-for-nlp-1295             3          3      True   \n",
       "49   generation-for-nlp-962             4          4      True   \n",
       "\n",
       "         raw_output  \n",
       "0                ''  \n",
       "1               '4'  \n",
       "2                ''  \n",
       "3   '</think>\\n\\n3'  \n",
       "4   '</think>\\n\\n4'  \n",
       "5   '</think>\\n\\n4'  \n",
       "6                ''  \n",
       "7   '</think>\\n\\n4'  \n",
       "8                ''  \n",
       "9                ''  \n",
       "10               ''  \n",
       "11  '</think>\\n\\n4'  \n",
       "12               ''  \n",
       "13  '</think>\\n\\n3'  \n",
       "14               ''  \n",
       "15              '2'  \n",
       "16  '</think>\\n\\n4'  \n",
       "17              '4'  \n",
       "18  '</think>\\n\\n4'  \n",
       "19  '</think>\\n\\n1'  \n",
       "20               ''  \n",
       "21               ''  \n",
       "22              '2'  \n",
       "23  '</think>\\n\\n3'  \n",
       "24              '2'  \n",
       "25  '</think>\\n\\n2'  \n",
       "26  '</think>\\n\\n3'  \n",
       "27  '</think>\\n\\n3'  \n",
       "28  '</think>\\n\\n4'  \n",
       "29  '</think>\\n\\n2'  \n",
       "30               ''  \n",
       "31  '</think>\\n\\n4'  \n",
       "32  '</think>\\n\\n4'  \n",
       "33  '</think>\\n\\n3'  \n",
       "34              '2'  \n",
       "35              '2'  \n",
       "36  '</think>\\n\\n3'  \n",
       "37  '</think>\\n\\n4'  \n",
       "38              '2'  \n",
       "39               ''  \n",
       "40  '</think>\\n\\n3'  \n",
       "41  '</think>\\n\\n4'  \n",
       "42  '</think>\\n\\n3'  \n",
       "43               ''  \n",
       "44              '2'  \n",
       "45              '4'  \n",
       "46               ''  \n",
       "47  '</think>\\n\\n4'  \n",
       "48  '</think>\\n\\n3'  \n",
       "49  '</think>\\n\\n4'  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_results_2[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
