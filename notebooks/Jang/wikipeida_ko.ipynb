{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fef547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import unicodedata\n",
    "from tqdm.auto import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197abe4d",
   "metadata": {},
   "source": [
    "### wikipedia-korean\n",
    "- title: 문서 제목\n",
    "- section_titles: 섹션 제목들의 리스트\n",
    "- section_texts: 각 섹션의 본문 텍스트 리스트\n",
    "- text: 문서 전체 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2459624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d2a0e9f6e64254af87ab3623695abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843acdf73c9d423d84a959b4f7debef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00007.parquet:   0%|          | 0.00/468M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a6fcb241f54c3b80e5e29f332614df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00007.parquet:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f030ae8de1074a508b3b22d52e00ae63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00007.parquet:   0%|          | 0.00/232M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecc1a86adc14124aa6b297d5c973563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00007.parquet:   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e45f3507748412ebb0679449dfd43f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00007.parquet:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab20920c36e2449c8bdc0295b385d8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00007.parquet:   0%|          | 0.00/186M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17dab06e0454a2b9fff1cd799719380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00007.parquet:   0%|          | 0.00/198M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f138a99565842b1ab5647fcfd0120d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/515425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"lcw99/wikipedia-korean-20240501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c35ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e233fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515425 entries, 0 to 515424\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   title           515425 non-null  object\n",
      " 1   section_titles  515425 non-null  object\n",
      " 2   section_texts   515425 non-null  object\n",
      " 3   text            515425 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ad114b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>section_titles</th>\n",
       "      <th>section_texts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지미 카터</td>\n",
       "      <td>[Introduction, 약력, 생애, 대통령 재임, 퇴임 이후, 평가, 같이 보...</td>\n",
       "      <td>[\\n\\n'''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는...</td>\n",
       "      <td>'''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>수학</td>\n",
       "      <td>[Introduction, 역사, 세부 분야, 영향, 같이 보기, 참고 문헌, 외부...</td>\n",
       "      <td>[\\n\\n'''수학'''(, , '''math''')은 수, 양, 구조, 공간, 변...</td>\n",
       "      <td>'''수학'''(, , '''math''')은 수, 양, 구조, 공간, 변화 등의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>수학 상수</td>\n",
       "      <td>[Introduction, 수학 상수표, 관련 상수들, 기타 상수들, 같이 보기]</td>\n",
       "      <td>[\\n'''수학'''에서 '''상수'''란 그 값이 변하지 않는 불변량으로, 변수의...</td>\n",
       "      <td>'''수학'''에서 '''상수'''란 그 값이 변하지 않는 불변량으로, 변수의 반대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>문학</td>\n",
       "      <td>[Introduction, 일반적인 문학의 분류, 대중문학의 분류, 문학 사조, 문...</td>\n",
       "      <td>[\\n\\n\\n파일:Fragonard, The Reader.jpg|섬네일|250px|...</td>\n",
       "      <td>파일:Fragonard, The Reader.jpg|섬네일|250px|장오노레 프라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나라 목록</td>\n",
       "      <td>[Introduction, 기준, 남극, EU, 참고, 몰타 기사단, [[마이크로네...</td>\n",
       "      <td>[\\n스위스 제네바에 있는 국제 연합 회원국 및 비회원 GA 옵서버의 국기\\n\\n이...</td>\n",
       "      <td>스위스 제네바에 있는 국제 연합 회원국 및 비회원 GA 옵서버의 국기\\n이 목록에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title                                     section_titles  \\\n",
       "0  지미 카터  [Introduction, 약력, 생애, 대통령 재임, 퇴임 이후, 평가, 같이 보...   \n",
       "1     수학  [Introduction, 역사, 세부 분야, 영향, 같이 보기, 참고 문헌, 외부...   \n",
       "2  수학 상수      [Introduction, 수학 상수표, 관련 상수들, 기타 상수들, 같이 보기]   \n",
       "3     문학  [Introduction, 일반적인 문학의 분류, 대중문학의 분류, 문학 사조, 문...   \n",
       "4  나라 목록  [Introduction, 기준, 남극, EU, 참고, 몰타 기사단, [[마이크로네...   \n",
       "\n",
       "                                       section_texts  \\\n",
       "0  [\\n\\n'''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는...   \n",
       "1  [\\n\\n'''수학'''(, , '''math''')은 수, 양, 구조, 공간, 변...   \n",
       "2  [\\n'''수학'''에서 '''상수'''란 그 값이 변하지 않는 불변량으로, 변수의...   \n",
       "3  [\\n\\n\\n파일:Fragonard, The Reader.jpg|섬네일|250px|...   \n",
       "4  [\\n스위스 제네바에 있는 국제 연합 회원국 및 비회원 GA 옵서버의 국기\\n\\n이...   \n",
       "\n",
       "                                                text  \n",
       "0  '''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...  \n",
       "1  '''수학'''(, , '''math''')은 수, 양, 구조, 공간, 변화 등의 ...  \n",
       "2  '''수학'''에서 '''상수'''란 그 값이 변하지 않는 불변량으로, 변수의 반대...  \n",
       "3  파일:Fragonard, The Reader.jpg|섬네일|250px|장오노레 프라...  \n",
       "4  스위스 제네바에 있는 국제 연합 회원국 및 비회원 GA 옵서버의 국기\\n이 목록에 ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347ce4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    515425.000000\n",
       "mean       1448.979076\n",
       "std        3128.588490\n",
       "min         103.000000\n",
       "25%         370.000000\n",
       "50%         646.000000\n",
       "75%        1362.000000\n",
       "max      566852.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bc1f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"lcw99/wikipedia-korean-20240501\", split=\"train\")\n",
    "\n",
    "wiki_50k = wiki.shuffle(seed=42).select(range(50_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db2b0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = wiki_50k.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48cf1aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           50000 non-null  object\n",
      " 1   section_titles  50000 non-null  object\n",
      " 2   section_texts   50000 non-null  object\n",
      " 3   text            50000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5541be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 216720 entries, 0 to 216719\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   title           216720 non-null  object\n",
      " 1   section_titles  216720 non-null  object\n",
      " 2   section_texts   216720 non-null  object\n",
      " 3   text            216720 non-null  object\n",
      " 4   text_norm       216720 non-null  object\n",
      " 5   recon_norm      216720 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_long = sample_df.explode([\"section_titles\", \"section_texts\"], ignore_index=True)\n",
    "df_long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c00ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long.rename(columns={\n",
    "    \"section_titles\": \"section_title\",\n",
    "    \"section_texts\": \"section_text\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d797ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>section_title</th>\n",
       "      <th>section_text</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>recon_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>로스앤젤레스 아줄 레전즈</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n\\n\\n로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는...</td>\n",
       "      <td>로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...</td>\n",
       "      <td>로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...</td>\n",
       "      <td>로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이케다 쇼헤이</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n\\n\\n'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이케다 쇼헤이</td>\n",
       "      <td>외부 링크</td>\n",
       "      <td>* \\n\\n\\n\\n\\n분류:1981년 출생\\n분류:살아있는 사람\\n분류:일본의 남자...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "      <td>'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>심고</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다...</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>심고</td>\n",
       "      <td>각주</td>\n",
       "      <td>\\n\\n\\n\\n분류:불교 용어</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "      <td>'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title section_title  \\\n",
       "0  로스앤젤레스 아줄 레전즈  Introduction   \n",
       "1        이케다 쇼헤이  Introduction   \n",
       "2        이케다 쇼헤이         외부 링크   \n",
       "3             심고  Introduction   \n",
       "4             심고            각주   \n",
       "\n",
       "                                        section_text  \\\n",
       "0  \\n\\n\\n로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는...   \n",
       "1  \\n\\n\\n'''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축...   \n",
       "2  * \\n\\n\\n\\n\\n분류:1981년 출생\\n분류:살아있는 사람\\n분류:일본의 남자...   \n",
       "3  \\n'''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다...   \n",
       "4                                   \\n\\n\\n\\n분류:불교 용어   \n",
       "\n",
       "                                                text  \\\n",
       "0  로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...   \n",
       "1  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...   \n",
       "2  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...   \n",
       "3  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...   \n",
       "4  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...   \n",
       "\n",
       "                                           text_norm  \\\n",
       "0  로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...   \n",
       "1  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...   \n",
       "2  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...   \n",
       "3  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...   \n",
       "4  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...   \n",
       "\n",
       "                                          recon_norm  \n",
       "0  로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마...  \n",
       "1  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...  \n",
       "2  '''이케다 쇼헤이''' (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다...  \n",
       "3  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...  \n",
       "4  '''심고'''(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bc573bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "슈퍼주니어의 음반 목록         93\n",
       "베이블레이드 버스트의 베이 목록    58\n",
       "2022년 K리그2의 경기 결과    47\n",
       "유니코드 11000~11FFF     47\n",
       "2016년 K리그2의 경기 결과    46\n",
       "불교 용어 목록 (육)         41\n",
       "미국의 가톨릭 교구 목록        40\n",
       "수사법                  37\n",
       "2009년 K리그의 경기 결과     33\n",
       "로드의 수상 및 후보 목록       32\n",
       "불교 용어 목록 (선)         31\n",
       "프랑스 사람 목록            29\n",
       "2007년 12월            28\n",
       "2012년 2월             28\n",
       "2007년 11월            28\n",
       "JAY B의 음반 목록         27\n",
       "시에라 엔터테인먼트           26\n",
       "안녕, 절망선생의 보너스 목록     26\n",
       "2006년 10월            24\n",
       "6.25 전쟁 유엔군 파병 국가    23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['title'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12c32f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    216720.000000\n",
       "mean        351.246189\n",
       "std        1055.865494\n",
       "min           0.000000\n",
       "25%          48.000000\n",
       "50%         124.000000\n",
       "75%         320.000000\n",
       "max      112885.000000\n",
       "Name: section_text, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08eddfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['군마현의 거의 중앙에 위치하고 하루나산의 남동의 산록과 도네강 지역에 정역이 있다. 서반부는 하루나산의 저변의 일부로 높이 200~900m의 경사지이다.한편 동반부는 높이 100~200m의 홍적층 대지이다.\\n\\n=== 인접하는 자치체 ===\\n* 마에바시시\\n* 시부카와시\\n* 기타군마군: 신토촌\\n',\n",
       " '=== 12.12 군사반란 ===\\n1979년 자 10·26 사건으로 인해 박정희 대통령이 사망한 뒤, 같은 해 전두환 등 하나회를 중심으로 한 신군부는 12·12 군사 반란을 일으켜 군부를 장악하였고 실권자로 떠올랐다. 1980년 초부터 보안사령관 전두환은 K-공작 계획을 실행하여 언론을 조종·통제하기 시작했다. 전두환은 같은 해 4월 14일에 중앙정보부장 서리에 임명돼 대한민국 내의 정보 기관을 모두 장악했다.\\n\\n=== 민주화시위 ===\\n1980년 5월부터 정치 관여 의도를 드러내는 신군부의 움직임에 대한 반발로 전두환 퇴진을 요구하는 학생 시위가 발생했다. 같은 달 국회에서는 계엄 해제와 개헌 논의를 비롯한 정치 현안에 대한 논의를 본격적으로 진행하기 시작했다. 하지만 신군부는 정국 운영에 방해가 되는 세력들을 제거하기 위해 집권 시나리오에 따라 5월 17일 24시에 비상계엄을 전국으로 확대하였고, 계엄 포고령 10호를 선포하여 정치활동 금지령·휴교령·언론 보도검열 강화 같은 조치를 내렸다. 신군부는 김대중, 김영삼, 김종필 등을 포함한 정치인과 재야 인사들 수천 명을 감금하고 군 병력으로 국회를 봉쇄했다. 광주 지역 대학생들은 5월 18일에 \\'김대중 석방\\', \\'전두환 퇴진\\', \\'비상계엄 해제\\' 등의 구호를 외치며 시위를 일으켰다. 신군부는 부마민주항쟁 때처럼 광주의 민주화 요구 시위도 강경 진압하면 잠잠해질 것으로 판단하였고, 공수부대 같은 계엄군을 동원해 진압했다. 신군부는 1980년 3월부터 5월 18일 직전까지 공수부대에 충정훈련을 실시했고, 5월 초부터 군을 사전 이동 배치하고 신군부에 반발하는 시위를 진압할 준비를 마친 상태였다.\\n\\n=== 광주학살의 시작 ===\\n5월 18일 16시 이후로 광주 시내에 투입된 공수부대원이 운동권 대학생뿐만 아니라 시위에 참여하지 않은 무고한 시민까지 닥치는 대로 살상·폭행하는 것을 목격한 광주시민들은 두려움을 넘어 분노를 느꼈고, 그 결과 중장년층뿐만 아니라 10대 청소년까지 거리로 나서 시위에 참여하면서 5·18 광주 민주화 운동은 걷잡을 수 없이 번졌다. 광주 시민들의 격렬한 저항에 부딪힌 계엄군은 5월 21일 13시경에 전남대학교와 전남도청 앞에서 집단 발포를 한 다음에 철수했다. 이 날 저녁 광주시 외곽으로 철수한 계엄군은 광주 외곽도로 봉쇄작전을 펼쳤으며, 이 과정에서 차량 통행자나 지역 주민들의 희생이 발생했다. 5월 27일 0시를 기해 계엄군은 상무충정작전을 실시해 무력으로 전남도청을 점령했다.\\n\\n10일에 걸친 광주 민주화 운동 결과 사망자 166명, 행방불명자 54명, 상이 후유증 사망자 376명, 부상자 3,139명 등에 달하는 인명피해가 발생했다. 이후 호남 전역에서 전두환과 신군부에 대한 반감이 극도로 높아졌다. 당시 신군부는 언론 사전검열을 실시하고 관제보도를 의무화하도록 해 언론을 장악하고 조종했는데, 주한미대사관과 주한미군 사령관 등 관련자들의 항의에도 불구하고, 당시 대한민국 내 언론이 미국이 신군부의 쿠데타와 5·18 민주화운동 진압을 승인했다는 보도를 쏟아내자 학생운동권 내 미국에 대한 반감이 높아졌다. 이는 부산 미국문화원 방화사건과 강원대학교 성조기 소각사건을 비롯, 1980년대부터 2000년대까지 발생한 각종 민주화 혹은 반미 집회와 시위의 도화선이 됐다.\\n\\n=== 광주민주화운동으로 재평가됨 ===\\n신군부 인사를 주축으로 한 제5공화국 정부는 5·18 민주화운동을 불순분자 또는 김대중의 사주로 인해 발생한 사건으로 왜곡했다. 1988년 제5공화국 비리 청산 분위기와 맞물려 열린 국회 광주진상특위에서 5·18 민주화운동의 진상 조사가 이루어졌다. 1993년 문민정부 출범 이후로 1993년 5월 13일, 김영삼 당시 대한민국 대통령이 5·13 담화에서 \"문민정부는 5·18 광주 민주화운동의 연장선상에 있는 정부\"라고 선언하면서 재평가가 가시화됐으며, 1996년 자 검찰의 수사에 의해 신군부 인사의 쿠데타를 통한 집권 의도와 5·18 민주화운동 유혈진압 책임이 구체적으로 밝혀졌다. 대법원이 1997년에 5·18, 12·12 진압 관련자를 처벌하면서 공식적으로 광주민주화운동으로 재평가됐다. 대한민국의 대법원은 5·18 광주 민주화 운동에 \"피고인(신군부)의 국헌문란행위에 항의하는 광주시민들은 주권자인 국민이 헌법수호를 위하여 결집을 이룬 것.\"이라고 규정했다. 대법원은 전두환·정호용·이희성·황영시·주영복 등을 5·18 민주화운동의 진압 책임자로 판시했다.\\n',\n",
       " \"\\n*  Junk and Advanced Cruising Rig Association\\n*  Stephen Dobbs , ''Urban Redevelopment and the Forced Eviction of Lighters from the Singapore River''\\n\\n분류:범선 유형\\n분류:말레이시아의 배\\n분류:싱가포르의 배\",\n",
       " '* 인텔 8008\\n* 인텔 8080 (8008 소스 호환)\\n* 인텔 8085 (8080 이진 호환)\\n* 인텔 8051 (하버드 구조)\\n\\n* 자일로그 Z80 (8080 이진 호환)\\n* 자일로그 Z180 (Z80 이진 호환)\\n* 자일로그 Z8\\n* 자일로그 eZ80 (Z80 이진 호환)\\n\\n* 모토로라 6800\\n* 모토로라 6809 (부분적으로 6800 호환)\\n* MOS 테크놀로지 6502\\n\\n* 마이크로칩 PIC10\\n* 마이크로칩 PIC12\\n* 마이크로칩 PIC16\\n* 마이크로칩 PIC18\\n* Atmel AVR 계열 마이크로컨트롤러\\n',\n",
       " '이 도시는 인도의 서사시 마하바라타의 신화적 인물 카르나와 관련이 있다. 이 도시의 탱크 또한 이름이 카르나 탈(Karna Tal)이며 도시의 문은 카르날 게이트로 불린다.\\n',\n",
       " '* 2005년 제34회 로테르담 국제영화제 국제비평가상\\n',\n",
       " '*할아버지 구재서(具再書)\\n*할머니 진양 하씨(晉陽 河氏)\\n**백부 구인회 (1907년 ~ 1969년)\\n**백부 구태회 (1923년 ~ 2016년)\\n**백모 최무 (1922년 ~ 2012년)\\n**백부 구평회 (1926년 ~ 2012년)\\n**백모 문남 \\n** 아버지 구두회 (1928년 ~ 2011년)\\n** 어머니 유한선\\n***누님 구은정 (1961년 ~ )\\n***누님 구지희 (1963년 ~ )\\n***여동생 구재희 (1967년 ~ )\\n\\n\\n분류:1964년 출생\\n분류:살아있는 사람\\n분류:대한민국의 기업인\\n분류:능성 구씨',\n",
       " '* \\n\\n분류:1998년 출생\\n분류:살아있는 사람\\n분류:대한민국의 아나운서\\n분류:전라남도 출신\\n분류:동덕여자대학교 동문\\n분류:대한민국의 무종교',\n",
       " '다리는 1895년에서 1897년 사이에 스위스 북동부 철도에 의해 980,000 스위스 프랑의 비용으로 건설되었다.\\n\\n다리는 총 길이가 439m이고, 높이가 강 높이에서 50m이다. 주요 경간은 길이 90m와 높이 9m의 철제 트러스교로 구성된다. 최대 50m 높이의 교각이 있는 여러 자연석 석조 접근로가 강철 트러스의 북쪽과 남쪽에서 다리로 이어진다. 초레스(Zores) 강철 프로파일의 밸러스트 트랙으로 구성된 원래의 차로는 1982년 ~ 1983년에 밸러스트가 있는 강철 홈통으로 교체되었다. 트러스 거더의 여러 조인트도 포스트텐션 볼트를 사용하여 강화되었으며, 강철재는 부식 방지를 위해 전체적으로 다시 도색되었다.\\n\\n2013년의 엔지니어링 연구에 따르면 교량의 추가 서비스 기간은 최소 50년이다.\\n',\n",
       " '=== AFC 아약스 (1991-1997) ===\\n1991년부터 1997년까지, 판 할은 AFC 아약스의 감독으로 큰 성공을 거두었다. 판 할 감독의 지휘 하에, 아약스는 3차례 에레디비시 우승을 거두었고 (1993-94, 1994-95, 1995-96), 1994-95 시즌에는 리그와 UEFA 챔피언스리그에서 모두 무패 우승을 거두었다. 그는 아약스를 이끌고 1992-93 시즌에 KNVB 컵 우승을, 1993년, 1994년, 1995년에는 요한 크라위프 샬을 획득하였다. 유럽대항전에서, 아약스는 1991-92 시즌에 UEFA 컵 우승을 거두었고, UEFA 챔피언스리그 1994-95 결승에서 AC 밀란을 이기고 우승하였다. 이후, 1995년 여름에는 레알 사라고사와의 UEFA 슈퍼컵에서 합계 5-1의 승리를 거두며 트로피를 획득하였다. 1995년 말, 아약스는 브라질의 그레미우를 승부차기 끝에 제압하며 UEFA 인터토토컵 우승을 거두었다. 아약스는 UEFA 챔피언스리그 1995-96 결승전에도 올랐으나 유벤투스 FC와의 결승전에서 승부차기 끝에 패하여 준우승에 머물렀다.\\n\\n아약스는 1990년대에 판 할의 리더쉽 하에 성공을 거두었으며, 네덜란드 축구 국가대표팀은 파트릭 클라위버르트, 마르크 오버르마르스, 데니스 베르흐캄프, 프랑크 더 부르, 로날트 더 부르, 엣하르 다비츠, 클라렌서 세이도르프, 빈스톤 보하르더, 미하얼 레이지허르, 에드빈 판데르사르의 아약스 선수들이 주축을 이루었다.\\n\\n1997년, 계약 만료 후, 판 할은 오라녜-나소 훈장을 수여받았다.\\n\\n=== FC 바르셀로나 (1997-2000) ===\\n1997년, 그는 FC 바르셀로나로 이적하였고, 보비 롭슨의 사령탑 자리를 물려받아 두 차례 라 리가 우승 (1997-98, 1998-99) 과 한 차례 코파 델 레이 (1997-98) 우승을 하였다. 그는 성공에도 불구하고, 언론과 자주 충돌하였고 비난의 대상이 되었다. 판 할은 문화적 차이로 인해 FC 바르셀로나에서 자신의 축구 철학을 펼치기 어려우며, 일부 선수들이 그를 따르지 않아 애로사항이 많다고 불만을 표출하였다. 그의 반대파로 히바우두가 있었다. 판 할은 히바우두를 왼쪽 윙어로 기용하려 하였지만, 히바우두는 중앙 미드필더로 기용되기를 원하였고, 결국 판 할은 히바우두를 스쿼드에서 제외시켰다.\\n\\n판 할은 2000년 5월 20일, 바르셀로나의 감독직에서 사임하였고, 사표에는 다음의 내용을 보였다: \"언론 친구들이여, 나는 떠난다. 축하한다.\" (\"Amigos de la prensa. Yo me voy. Felicidades.\") 라고 하였다. 그는 네덜란드로 돌아가 2002년 FIFA 월드컵 예선을 앞둔 네덜란드 축구 국가대표팀 감독이 되었다.\\n\\n=== 네덜란드 축구 국가대표팀, FC 바르셀로나 복귀 (2002-2003) ===\\n네덜란드 축구 국가대표팀은 2002년 FIFA 월드컵 본선 진출에 실패하였고, 결국 2002년 1월 31일에 감독직에서 사임하였고, 딕 아드보카트가 그의 자리를 대신하였다. 그 후, 언론들은 알렉스 퍼거슨의 뒤를 이어 맨체스터 유나이티드 FC의 차기 감독이 될 수 있다고 점쳤다. 판 할이 언급한 바에 의하면 퍼거슨은 은퇴를 하지 않을 것이어서 맨체스터 유나이티드의 감독직을 맡지 않을 것이라고 하였다. 그는 FC 바르셀로나의 사령탑에 복직하였지만, 시즌 중반에 하차하였고, 그의 자리는 라도미르 안티치가 대신하였다.\\n\\n=== 아약스 복귀 (2004) ===\\n2004년, 그는 AFC 아약스의 기술 고문으로 복귀하였지만, 내부 분열로 연말에 사임하였다.\\n\\n=== AZ 알크마르 (2005-2009) ===\\n파일:Louis-van-gaal4-CN.jpg|섬네일|AZ 알크마르의 판 할\\n2005년 1월, AZ 알크마르는 판 할이 코 아드리안서를 대신하여 2005년 7월 1일에 새 감독으로 취임할 것이라고 발표하였다. AZ 알크마르는 2006-07 시즌을 우승팀 PSV 에인트호번과 준우승팀 AFC 아약스와 3점차의 3위로 마감하였다. 판 할은 또한 2006-07 KNVB 컵에서도 준우승을 거두었는데, AFC 아약스에 합계 2-4로 패하여 UEFA 챔피언스리그 진출이 좌절되었다.\\n\\n루이 판 할은 형편없는 성적으로 2007-08 시즌 종료 후 알크마르를 떠날것이라고 처음에 선언하였다. 그러나, 알크마르의 일부 선수들은 판 할의 잔류를 원하였고, 결국 판 할 감독은 2008-09 시즌에도 알크마르에 남을 것이라고 마음을 바꾸었다.\\n\\n2008-09 시즌은 에레디비시 첫 두 경기를 NAC 브레다에 1-2로, ADO 덴하흐에 0-3으로 패하며 거칠게 시작하였다. 그러나 AZ는 이후 4월 18일까지 단 한경기도 패하지 않았고, FC 트벤터와 AFC 아약스를 앞지르며 리그 선두를 달렸다. AZ는 이 시즌 최소 실점과, 2위의 득점력 (1위는 AFC 아약스) 을 기록하였고, 이 기록은 2008-09 시즌의 득점왕 무니르 엘 함다우이와 브라질인 아리 투톱의 공이 컸다. 2009년 4월 19일, AZ는 피테서와의 뜻밖의 패배로 28경기 무패행진이 끝났지만, 그 다음날 리그 우승을 확정지었다. 같은 날, AFC 아약스는 PSV 에인트호번을 꺾고 우승에 대한 희망의 불씨를 살릴 수 있었으나 2-6으로 패하며 허무하게 끝났다.\\n\\n=== FC 바이에른 뮌헨 (2009-2011) ===\\n2009년 7월 1일, 판 할은 FC 바이에른 뮌헨의 신임 감독이 되었다. 판 할은 그의 통역가에게 \"꿈의 클럽\"에 입단하였다고 하였다. 그는 뮌헨에서의 첫 4경기 중 1경기만 승리하며 혹독한 신고식을 치루었고, 11월로 넘어가자, 클럽은 FC 지롱댕 드 보르도와의 경기에서 홈과 원정 모두 패하며 UEFA 챔피언스리그 조기 탈락의 위기에 처했었다. 바이어 04 레버쿠젠이 분데스리가의 선두를 점거하였고, 언론들은 전임이었던 위르겐 클린스만이 뮌헨의 감독으로 지낸 기간보다 더 짧은 기간안에 판 할이 경질될 것이라고 점쳤다. 그러나, 팀이 다시 페이스를 찾으면서 불길한 예상은 빗나갔고, 판 할이 다시 한번 슬로 스타터였음을 입증하였다. 2009년 8월 28일, 판 할은 아르연 로번을 레알 마드리드 CF로부터 보강을 위해 영입하였고, 로번은 네덜란드 U-20팀 이래 처음으로 판 할과 재회하였다. 2007-08 시즌, 분데스리가와 DFB-포칼의 우승 주역이었으나, 마리오 고메즈의 영입과 부상으로 인한 기량 저하로 인해 이탈리아인 스트라이커, 루카 토니는 AS 로마로 이적하였다. 바이에른 뮌헨은 UEFA 챔피언스리그의 유벤투스 FC의 원정경기에서 4-1 승리를 거두며, FC 지롱댕 드 보르도의 뒤를 이어 2위가 되어 극적으로 16강에 진출하였고, 전세가 역전되었다. 바이에른 뮌헨은 DFB-포칼 4강 진출에 성공하는 와중에 2010년 3월, 바이어 04 레버쿠젠이 반시즌이 넘도록 지키고 있던 분데스리가 선두를 가져갔다. 2010년 3월 24일, 뮌헨은 FC 샬케 04를 상대로 포칼 준결승을 치루었다. 뮌헨은 연장전 끝에 극적인 승리를 거두고, 포칼 결승에 진출하였다.\\n\\n2010년 5월 8일, 바이에른은 헤르타 BSC 베를린과의 분데스리가 최종전을 3-1로 마무리하고, 2009-10 시즌 우승을 거두었다. 그는 네덜란드인 감독으로는 분데스리가를 우승한 첫 감독이 되었다. 바이에른은 2010년 5월 15일, DFB-포칼 결승에서도 승리하고 더블을 달성하며, 네덜란드, 스페인, 독일 3개국에서 컵 우승을 경험한 감독이 되었다.\\n\\nUEFA 챔피언스리그 2009-10에서는 맨체스터 유나이티드 FC와 8강전에서 합계 4-4로 원정 다득점 원칙에 따라 4강에 진출하였고, 올랭피크 리옹과의 4강전에서 합계 4-0의 스코어로 결승에 진출하였다. 바이에른 뮌헨은 결승전에서 조제 모리뉴가 이끄는 FC 인테르나치오날레 밀라노를 만났는데, 판 할은 FC 바르셀로나 시절 모리뉴를 그의 제자로 두고 있었다. 바이에른 뮌헨은 결승전에서 0-2로 패하여 트레블이 좌절되었고, 반대로 FC 인테르나치오날레가 이탈리아 최초의 트레블을 달성한 클럽이 되었다. 2010년 5월 25일, 카를하인츠 루메니게는 판 할 감독의 성과에 대해 만족하였고, 아직 계약기간이 1년 더 남았음에도 불구하고, 판 할에게 연장계약을 요구하였다.\\n\\n판 할의 바이에른은 2010년 FIFA 월드컵 종료 직후, 14년 만에 다시 공식화된 DFL-슈퍼컵에서 승리하여 트로피를 획득하였다. 시즌 말 투표에서 판 할은 독일 프로 선수 조합 (VDV) 과 키커지에서 실시한 연간 독일 축구 선수 투표에서 올해의 감독으로 선정되었다.\\n\\n더 나아가서, 판 할은 많은 리저브팀 출신 선수들을 선발 라인업에 집어넣었는데, 이들 중에는 토마스 뮐러, 홀거 바드스투버가 포함되었고, 바스티안 슈바인슈타이거를 윙어외에도 수비형 미드필더로 효율적으로 활용하였다.\\n\\n2011년 3월 7일, 바이에른 뮌헨은 2010-11 시즌 종료 이후 판 할은 계약이 해지될 것이라고 발표하였지만, 2011년 4월 10일, 뮌헨이 3위 자리를 하노버 96에게 내주었고, 판 할은 본래 계획된 시기보다 일찍 해임되었다.\\n\\n=== 네덜란드 축구 국가대표팀 복귀 (2012-2014) ===\\n2012년, UEFA 유로 2012에서 전패로 대회를 마감한 베르트 판 마르베이크 감독에 이어, 판 할은 네덜란드 대표팀에 부임하였다. 그의 지휘아래 네덜란드 대표팀은 2014 브라질 월드컵 유럽 지역예선에서 D조 1위로 무난히 본선에 진출하였다. 2014 브라질 월드컵 직전에 부상선수들이 대거 나오고, 세대교체 시기가 맞물리고, 죽음의 조에 배치되는 등 네덜란드의 조기 탈락을 예상한 사람들이 많았지만, 판 할은 변칙 3-5-2 플랫폼과, 적재적소의 기용 등으로 예상을 훨씬 뛰어넘는 3위의 성적을 이룩했다. 그 과정에서 디펜딩 챔피언인 스페인을 5-1로 꺾고, 개최국 브라질을 3-0으로 꺾는 모습을 보였다.\\n\\n=== 맨체스터 유나이티드 FC (2014-2016) ===\\n판 할은 2014년 5월 19일에 맨체스터 유나이티드와 3년 계약을 맺고, 모예스의 후임으로 맨유의 새로운 사령탑으로 선임되었다.\\n\\n맨유 감독으로 부임한 판 할은 2014 브라질 월드컵에서 네덜란드 대표팀을 이끌고 보여줬던 3-4-1-2 또는 3-5-2, 3-4-3 등 플랫3에 기반을 둔 전술을 맨유에 접목시키기 위해 주력하고 있는데다가 이번 미국 투어 때, 탁월한 전술과 경기력을 유지시켰으며, 맨유가 2014 기네스 인터내셔널 챔피언스컵 우승을 차지하는 전과를 올렸다.\\n\\n시즌 초반에는 스리백과 본인만의 전술 등을 고집하여 팀의 성적은 좋지 않았지만, 시즌 중반에 들어서자 본인의 전술 고집을 버리고, 다시 포백 포메이션인 4-4-2로 전환하여 팀의 성적을 상위권으로 끌어올려 리그 순위인 4위를 기록, UEFA 챔피언스리그 PO 진출권을 확보하여 맨유가 2년 만에 챔피언스리그 복귀하는 큰 전과를 거두고 맨유에서의 첫 시즌을 마감하였다.\\n\\n2015년 8월 8일에 있었던 토트넘과의 리그 개막전 경기에서 본인의 64번째 생일을 승리로 장식하였다. (맨유 1-0 토트넘, 맨유 승리)\\n\\n8월 15일(한국시각 새벽)에 있었던 아스톤 빌라와의 리그 원정 경기에서 1-0 승리를 챙겨 2연승을 달렸지만, 맨유의 공격력에 의문을 남겼다. 그러자, 판 할 감독을 기자 회견에서 \"그 점은 신경 쓰이지 않는다. 상대보다 더 많은 골을 넣었기 때문이다. 결과적으로 상대에게 많은 찬스를 허용하지 않았고, 모든 경기에서 상대보다 더 많은 찬스를 잡았다”고 말했다.\\n\\n한 때는 완고한 성격으로 선수들과 종종 마찰을 빚은 바 있다. 최근에는 하파엘 다 실바의 이적 과정에서 맨유 선수들이 불만을 품은 것으로 알려지면서 불화설이 제기되기도 했다. 당시 판 할 감독은 하파엘을 존중하지 않는 듯한 행동을 했고, 이에 웨인 루니와 마이클 캐릭이 대화 요청을 했지만 받아들여지지 않은 것으로 알려졌다. 그러자, 판 할 감독은 11일 영국 ‘스카이스포츠’와의 인터뷰에서 “모든 선수들과 대화를 나누고 있다. 그들은 거리낌 없이 사무실로 찾아오며, 나에 대한 믿음을 가지고 있다”면서 “나는 독재자가 아니다. 선수들과의 소통에 항상 열려있는 사람”이라고 해명하였다. 하지만 이 과정에서 앙헬 디 마리아와 척을 지게 되었고 결국 견디다 못한 디 마리아는 파리 생제르맹으로 교도소를 탈옥하듯 이적했다.\\n\\n15-16시즌에는 수비력을 올리고 안정적인 경기를 하였지만 공격전개가 단조로워서 스콜스 등 맨유 레전드로부터 맨유의 전통적인 공격적인 축구를 구사하지 못한다며 비판을 받기도 하였다. 팬들로부터는 \"재미없는 축구\"를 한다며 비판을 받았고, 결국 12월 9일(한국시각)에 있었던 15-16 챔피언스리그 16강 진출에 실패하는 등 맨유 팬들에게 경질 압박을 받고 있고, 노리치 전에 패배에 이어 12월 26일(한국시각)에 있었던 스토크 시티와의 원정 경기에서 0-2로 패했으며, 유로파리그에서 리버풀에게 패하여 8강 진출 실패한데다가 웨스트 햄과의 리그 원정 경기에서 2-3 역전패를 당하여 다음 시즌 챔피언스리그 진출이 좌절되어, 리그 5위를 기록한 맨유는 다음 시즌 유로파리그로 진출하게 되었다. 그 후, 판 할은 맨유 팬들로부터 감독직에서 물러나라는 야유를 받기도 하였다.\\n\\n2016년 5월 22일(한국 시각) 크리스털 팰리스와의 FA컵 결승전에서 2-1 역전승을 이루고 맨체스터 유나이티드 부임 후 첫 트로피를 들어올렸다.\\n\\n그러나 FA컵 우승 직후, 영국 BBC와 스카이스포츠 측에서 \"판 할이 경질되고 무리뉴가 차기 맨유 감독으로 내정되었다.\"라는 기사를 내었고, 판 할 또한 해임을 인정하였다.\\n\\n그 후, 이틀 뒤인 5월 24일 맨체스터 유나이티드는 공식 홈페이지를 통해 판 할 감독과의 이별을 알렸다.\\n\\n=== 네덜란드 축구 국가대표팀 두 번째 복귀 (2021-2022) ===\\n2021년 8월 4일 감독직 은퇴 번복을 하고 네덜란드 축구 국가대표팀을 2022년 FIFA 월드컵까지 맡게 됐다.\\n\\n2020년에 전립선암 판정을 받았다는 사실을 뒤늦게 밝혔다.\\n\\n조별리그에서 카타르와 세네갈을 이기고 에콰도르와 비겨 조 1위로 16강에 진출했다. 16강에서는 그다지 어렵지 않은 미국을 만나 쉽게 이기고 8강에 진출했으나 8강에서 우승후보 중 하나이자 실제로도 이 월드컵에서 우승한 아르헨티나를 만나 엄청난 고전과 더불어 주심으로 배정된 안토니오 마테우 라오스 심판이 경기를 난장판으로 만든 탓에 승부차기까지 갔다. 하지만 아르헨티나에는 명 골키퍼인 에밀리아노 마르티네스가 있었고 네덜란드는 첫 2명의 키커부터 에밀리아노 마르티네스에게 막히면서 승부차기를 패배해 8년 전과 똑같이 아르헨티나에 밀려 다음 라운드 진출이 좌절되었다.\\n\\n8강 아르헨티나전을 끝으로 71세의 고령과 전립선암이라는 지병까지 겹치며 결국 축구 감독에서 은퇴했다.\\n',\n",
       " \"\\n섬네일\\n'''렉스 리스'''(Rex Lease, 1903년 2월 11일 ~ 1966년 1월 3일)는 미국의 배우이다.\\n\\n웨스트버지니아주에서 태어났으며 밴나이즈에서 사망하였다.\\n\",\n",
       " \"\\n\\n\\n\\n'''학산고등학교'''(鶴山高等學校)는 충청북도 영동군 학산면 서산리에 있는 공립 고등학교이다.\\n\",\n",
       " \"\\n\\n\\n\\n\\n연도\\n\\n소속\\n\\n나이\\n\\n출장\\n\\n타석\\n\\n타수\\n\\n득점\\n\\n안타\\n\\n2루타\\n\\n3루타\\n\\n홈런\\n\\n타점\\n\\n도루\\n\\n도실\\n\\n볼넷\\n\\n삼진\\n\\n타율\\n\\n출루율\\n\\n장타율\\n\\nOPS\\n\\n루타\\n\\n병살타\\n\\n몸맞\\n\\n희타\\n\\n희플\\n\\n고4\\n\\n\\n\\n1987\\n\\n빙그레\\n\\n24\\n\\n100\\n\\n441\\n\\n370\\n\\n56\\n\\n'''124'''\\n\\n13\\n\\n7\\n\\n4\\n\\n34\\n\\n20\\n\\n13\\n\\n35\\n\\n25\\n\\n.335\\n\\n.405\\n\\n.441\\n\\n.846\\n\\n163\\n\\n1\\n\\n'''10'''\\n\\n'''24'''\\n\\n2\\n\\n2\\n\\n\\n\\n1988\\n\\n25\\n\\n96\\n\\n414\\n\\n366\\n\\n69\\n\\n113\\n\\n20\\n\\n3\\n\\n4\\n\\n34\\n\\n18\\n\\n9\\n\\n37\\n\\n26\\n\\n.309\\n\\n.377\\n\\n.413\\n\\n.789\\n\\n151\\n\\n4\\n\\n4\\n\\n5\\n\\n2\\n\\n1\\n\\n\\n\\n1989\\n\\n26\\n\\n51\\n\\n219\\n\\n198\\n\\n30\\n\\n64\\n\\n20\\n\\n2\\n\\n1\\n\\n23\\n\\n14\\n\\n1\\n\\n14\\n\\n12\\n\\n.323\\n\\n.372\\n\\n.460\\n\\n.831\\n\\n91\\n\\n0\\n\\n3\\n\\n1\\n\\n3\\n\\n0\\n\\n\\n\\n1990\\n\\n27\\n\\n114\\n\\n459\\n\\n402\\n\\n71\\n\\n117\\n\\n20\\n\\n3\\n\\n7\\n\\n52\\n\\n22\\n\\n6\\n\\n45\\n\\n26\\n\\n.291\\n\\n.360\\n\\n.408\\n\\n.768\\n\\n164\\n\\n5\\n\\n2\\n\\n4\\n\\n6\\n\\n2\\n\\n\\n\\n1991\\n\\n28\\n\\n111\\n\\n433\\n\\n379\\n\\n81\\n\\n132\\n\\n21\\n\\n'''12'''\\n\\n17\\n\\n55\\n\\n18\\n\\n6\\n\\n43\\n\\n33\\n\\n'''.348'''\\n\\n.417\\n\\n.602\\n\\n1.019\\n\\n228\\n\\n6\\n\\n4\\n\\n4\\n\\n3\\n\\n2\\n\\n\\n\\n1992\\n\\n29\\n\\n111\\n\\n437\\n\\n369\\n\\n89\\n\\n133\\n\\n17\\n\\n2\\n\\n25\\n\\n68\\n\\n21\\n\\n3\\n\\n46\\n\\n27\\n\\n'''.360'''\\n\\n.436\\n\\n.621\\n\\n1.056\\n\\n229\\n\\n6\\n\\n10\\n\\n3\\n\\n9\\n\\n1\\n\\n\\n\\n1993\\n\\n30\\n\\n41\\n\\n129\\n\\n111\\n\\n10\\n\\n25\\n\\n5\\n\\n1\\n\\n2\\n\\n12\\n\\n3\\n\\n1\\n\\n15\\n\\n15\\n\\n.225\\n\\n.313\\n\\n.342\\n\\n.655\\n\\n38\\n\\n2\\n\\n0\\n\\n1\\n\\n2\\n\\n2\\n\\n\\n\\n1994\\n\\n한화\\n\\n31\\n\\n67\\n\\n272\\n\\n239\\n\\n35\\n\\n59\\n\\n7\\n\\n2\\n\\n2\\n\\n22\\n\\n13\\n\\n6\\n\\n25\\n\\n23\\n\\n.247\\n\\n.321\\n\\n.318\\n\\n.639\\n\\n76\\n\\n2\\n\\n2\\n\\n4\\n\\n2\\n\\n2\\n\\n\\n\\n1995\\n\\n삼성\\n\\n32\\n\\n74\\n\\n257\\n\\n236\\n\\n33\\n\\n59\\n\\n9\\n\\n0\\n\\n1\\n\\n10\\n\\n12\\n\\n6\\n\\n16\\n\\n20\\n\\n.250\\n\\n.302\\n\\n.301\\n\\n.603\\n\\n71\\n\\n2\\n\\n2\\n\\n2\\n\\n1\\n\\n1\\n\\n\\n\\n1996\\n\\n33\\n\\n61\\n\\n156\\n\\n136\\n\\n16\\n\\n29\\n\\n4\\n\\n2\\n\\n1\\n\\n10\\n\\n3\\n\\n1\\n\\n16\\n\\n13\\n\\n.213\\n\\n.314\\n\\n.294\\n\\n.608\\n\\n40\\n\\n1\\n\\n4\\n\\n0\\n\\n0\\n\\n3\\n\\n\\n\\n1997\\n\\nOB\\n\\n34\\n\\n92\\n\\n295\\n\\n266\\n\\n25\\n\\n63\\n\\n16\\n\\n3\\n\\n2\\n\\n33\\n\\n7\\n\\n3\\n\\n20\\n\\n26\\n\\n.237\\n\\n.296\\n\\n.342\\n\\n.638\\n\\n91\\n\\n5\\n\\n3\\n\\n4\\n\\n2\\n\\n3\\n\\n\\n\\nKBO 통산 : 11년\\n\\n'''918'''\\n\\n'''3512'''\\n\\n'''3072'''\\n\\n'''515'''\\n\\n'''918'''\\n\\n'''152'''\\n\\n'''37'''\\n\\n'''66'''\\n\\n'''353'''\\n\\n'''151'''\\n\\n'''55'''\\n\\n'''312'''\\n\\n'''246'''\\n\\n'''.299'''\\n\\n'''.368'''\\n\\n'''.437'''\\n\\n'''.805'''\\n\\n'''1342'''\\n\\n'''34'''\\n\\n'''44'''\\n\\n'''52'''\\n\\n'''32'''\\n\\n'''19'''\\n\\n\\n\\n* 시즌 기록 중 '''굵은 글씨'''는 해당 시즌 최고 기록\\n\",\n",
       " \"\\n*  ''Molasse'', 정의와 사례, 서알프스의 간략 지질 지도. 리즈 대학교\\n\\n분류:퇴적학\",\n",
       " \"\\n\\n'''엘리 알레비'''()는 1870년 9월 6일 에트르타에서 태어나서 1937년 8월 21일 쉬시앙브리에서 사망한 프랑스의 철학자이자 역사가로, 영국 실용주의자에 대한 연구와 19세기 영국사를 저술하였다.\\n\",\n",
       " \"\\n\\n\\n\\n'''부천북여자중학교'''(富川北女子中學校)는 경기도 부천시 도당동에 있었던 공립 중학교이다.\\n\",\n",
       " '\\n',\n",
       " '오와리국 가스가이군(春日井郡) 기타노(北野, 현재의 아이치현 기타나고야시)에서 야스이 시게쓰구의 아들로 태어났다. 오다 노부나가의 유미슈(弓衆)였던 외삼촌 아사노 나가카쓰가 아들이 없었기 때문에, 나가카쓰의 딸 야야의 데릴사위로서 아사노 가문에 받아들여졌고 아사노 가문을 상속하게 된다. 나가카쓰의 양녀인 네네(이후의 기타노만도코로, 고다이인)가 기노시타 도키치로(이후의 도요토미 히데요시)에게 시집 감에 따라 나가마사는 히데요시의 가장 가까운 인척이 되었고, 노부나가의 명으로 히데요시의 요리키(与力)가 된다. 1573년(덴쇼원년)의 아자이 나가마사 토벌을 시작으로 1583년(덴쇼11년)의 시즈가타케 전투, 노부나가를 계승한 히데요시의 통일사업, 조선출병 등에서 무공을 올렸다. 또한 그 탁월한 행정수완으로 태합검지(太閤検地)나 교토쇼시다이 임무를 신속하게 수행해는 등 다양한 공을 세워 1593년(분로쿠2년)에는 가이국 22만 석을 받았다.\\n\\n오대로인 도쿠가와 이에야스와 친한 관계로 히데요시가 죽은 후에는 같은 고부교인 이시다 미쓰나리와 견원지간이었다고 한다(여기에는 최근 들어 의문도 제시되고 있다. 후술 참조). 1599년(게이초 4년)엔 마에다 도시나가 등과 함께 이에야스에게서 암살의 의심을 받아 그 혐의로 가이국에서 근신을 명받는다.\\n\\n1600년(게이초 5년) 가을 세키가하라 전투에서는 이에야스의 아들 도쿠가와 히데타다에게 속했으며 전후에는 적장자·요시나가에게 가독을 물려주고 은거한다. 이에야스는 에도에 무가정권을 세웠고, 1606년(게이초 11년)에 나가마사는 요시나가의 영토와는 별개로 자신의 은거료로서 히타치 마카베(真壁)의 5만 석을 지급받았다. 1611년(게이초 16년) 마카베 진야에서 사망한다. 향년 65세. 그 후 마카베 5만 석은 삼남 나가시게가 이었으며 그의 아들 나가나오는 영지를 아코번으로 옮겼고, 이 가계에서 아코 사건으로 유명한 아사노 다쿠미노카미 나가노리가 나왔다.\\n\\n법명 ： 傳正院殿前霜台功山道忠大居士.\\n\\n묘소 ： 이바라키현 사쿠라가와시 사쿠라이의 덴모쿠 산(天目山) 덴쇼지 절(伝正寺). 또, 와카야마현 고야정의 고야산 싯치인(悉地院).\\n',\n",
       " '\\n',\n",
       " \"\\n\\n\\n'''김대거 종법사 생가'''는 전라북도 진안군 성수면 좌포리 815 (원좌길 33-10) 에 있는 건축물이다. 2016년 12월 28일 진안군의 향토문화유산 제19호로 지정되었다. 원불교 성보 제15호 대산 종사 탄생가 이기도 하다.\\n\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.sample(n=20, random_state=42)['section_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff622a5",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f78457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # HTML/XML 태그 제거 (특히 <ref> 태그 안의 내용은 노이즈이므로 내용까지 삭제)\n",
    "    text = re.sub(r'<ref.*?>.*?</ref>', '', text, flags=re.DOTALL) # <ref>내용</ref> 전체 삭제\n",
    "    text = re.sub(r'<[^>]+>', '', text) # 남은 <html> 태그들 삭제\n",
    "\n",
    "    # 링크 괄호 제거\n",
    "    text = text.replace(\"[[\", \"\").replace(\"]]\", \"\")\n",
    "\n",
    "    # 위키 문법 제거 (=== 소제목 ===, ''', [[링크]])\n",
    "    text = re.sub(r'={2,}', '', text)  # === 제거\n",
    "    text = text.replace(\"'''\", \"\")     # ''' 제거\n",
    "    \n",
    "    # 불필요한 공백 및 개행 정리\n",
    "    # 연속된 공백/탭을 스페이스 하나로\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # 연속된 줄바꿈을 줄바꿈 하나로 (\\n\\n\\n -> \\n)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11ec827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['유럽과 아시아의 러시아\\n유럽 러시아()는 유럽에 속해 있는 러시아의 부분을 이야기한다. 전통적으로 유럽의 경계선은 우랄산맥이지만, 이 정의는 현재 논쟁 중이다.\\n러시아의 거의 대부분의 영토는 아시아에 속해 있지만 인구의 거의 대부분은 유럽에 집중되어 있다. 또한 러시아의 대도시들은 유럽에 편중되어 있는데 그 예로 모스크바와 상트페테르부르크가 있다. 이 두 도시는 모두 과거에 러시아의 수도였거나 현재까지 러시아의 수도인 대도시이다.\\n러시아 제국 시대에는 러시아의 통제를 받는 동슬라브족 영토를 이야기하는 말로 쓰였다. 이는 벨라루스(백러시아)와 우크라이나의 거의 대부분 지역(드니프르 우크라이나)을 포함한다.', '* 세례시 성호를 긋는 행위를 금할 것.\\n* 견진성사를 금할 것.\\n* 평신도에 의한 세례를 금할 것.\\n* 결혼식에 반지 사용을 금할 것.\\n* 예수의 이름으로 고개 숙이는 행위를 금할 것.\\n* 중백의와 모자 사용을 금할 것.\\n* 다중직을 제공하는 것과 급료 제공을 금할 것.', '순번\\n 재임 기간\\n 이름\\n 1대\\n 1997.2.1-1999.4.30\\n 최명룡\\n 2대\\n 1999.5.1-2001.1.3\\n 최종규\\n 3대\\n 2001.1.4-2001.12.26\\n 김동욱\\n 감독대행\\n 2001.12.27-2002.4.30\\n 전창진\\n 4대\\n 2002.5.1-2009.4.22\\n 전창진\\n 5대\\n 2009.4.23-2013.3.11\\n 강동희\\n 감독대행\\n 2013.3.12-2013.4.29\\n 김영만\\n 6대\\n 2013.4.30-2014.2.1\\n 이충희\\n 7대\\n 2014.2.2-2017.4.14\\n 김영만\\n 8대\\n 2017.4.21-2023.1.5\\n 이상범\\n 대행\\n 2023.1.6 - 2023.4.11\\n 김주성\\n 9대\\n 2023.4.12 -\\n 김주성', '남북 전쟁 후 미국은 북부를 중심으로 하는 하나의 큰 국민경제가 정리되었다. 1869년, 오마하와 새크라멘토를 잇는 최초의 대륙횡단철도가 개통되면서 유럽에서 다수한 이민자들이 유입되었다. 이러한 자본주의의 급속한 성장에 따라 철강왕 앤드루 카네기(스코틀랜드 출신), 석유왕 존 록펠러, 솔로몬 R. 구겐하임의 아버지 광산왕 마이어 구겐하임(스위스 출신의 유태계 독일인) 등 이름난 부호가 등장했다. 그러나 정치는 부패했고, 국가의 비호를 받은 자본가는 더욱 부를 축적하였고 역설적으로 하층민들은 가난에 허덕였다. 가히 도금시대.', '폴란드 실레시안 베스키즈의 유럽전나무\\n유럽전나무(Abies alba, European silver fir, silver fir)는 피레네산맥에서 북쪽으로 노르망디, 동쪽으로 알프스산맥, 카르파티아산맥, 슬로베니아, 크로아티아, 보스니아 헤르체고비나, 몬테네그로, 세르비아, 남쪽으로 이탈리아, 불가리아, 코소보, 알바니아, 그리스 북부에 이르기까지 유럽의 산맥에 자생하는 전나무이다. 북아메리카 북동부의 성탄절 트리 농장에서 흔히 자란다.']\n"
     ]
    }
   ],
   "source": [
    "# 적용\n",
    "df_long['section_text_clean'] = df_long['section_text'].apply(clean_wiki_text)\n",
    "\n",
    "print(df_long.sample(5)['section_text_clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0d698de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    216720.000000\n",
       "mean        330.814752\n",
       "std         987.262379\n",
       "min           0.000000\n",
       "25%          42.000000\n",
       "50%         116.000000\n",
       "75%         306.000000\n",
       "max      109620.000000\n",
       "Name: section_text_clean, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text_clean'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ce7ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    216720.000000\n",
       "mean        351.246189\n",
       "std        1055.865494\n",
       "min           0.000000\n",
       "25%          48.000000\n",
       "50%         124.000000\n",
       "75%         320.000000\n",
       "max      112885.000000\n",
       "Name: section_text, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dcc2d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마추어 축구팀이다. 현재 USL 프리미어 디벨로프먼트 리그에 참가하고 있으며 성적은 중위권이다. 창단 당시에는 로스앤젤레스 스톰, 2008년부터 2009년까지는 로스앤젤레스 레전즈였다가 2010년 크루스 아술과 제휴 계약을 맺으면서 이름도 현재의 이름으로 바꾸었다.\\n분류:USL 리그 2 구단\\n분류:2010년 설립된 축구단\\n분류:로스앤젤레스 아줄 레전즈',\n",
       " '이케다 쇼헤이 (1981년 4월 27일 ~ )는 일본의 전 축구 선수이다. 과거 시미즈 에스펄스, 산프레체 히로시마, 베갈타 센다이, 제프 유나이티드 지바, 에히메 FC, FC 기후에서 활동하였다.',\n",
       " '* \\n분류:1981년 출생\\n분류:살아있는 사람\\n분류:일본의 남자 축구 선수\\n분류:일본 남자 청소년 축구 국가대표팀 선수\\n분류:J1리그의 축구 선수\\n분류:J2리그의 축구 선수\\n분류:시미즈 에스펄스의 축구 선수\\n분류:산프레체 히로시마의 축구 선수\\n분류:베갈타 센다이의 축구 선수\\n분류:제프 유나이티드 이치하라 지바의 축구 선수\\n분류:에히메 FC의 축구 선수\\n분류:FC 기후의 축구 선수\\n분류:아시안 게임 축구 메달리스트\\n분류:2002년 아시안 게임 축구 참가 선수\\n분류:일본의 아시안 게임 은메달리스트\\n분류:남자 축구 수비수\\n분류:시즈오카시 출신\\n분류:시즈오카현 출신 축구 선수',\n",
       " '심고(心告)는 진리(眞理) 앞에 자신의 참뜻을 고백하는 불교 용어이다. 그 방법은 \"천지(天地)·부모(父母)여 하감(下鑑)하소서, 동포(同胞)·법률(法律)이여 응감(應鑑)하소서. 은혜를 입은 저는 진심으로 진리(眞理) 부처님 앞에 고백하나이다\"라고 한 다음 즐거운 일을 당하면 감사를 올리고 괴로운 일을 당하면 사죄를 올리며 결정하기 어려운 일을 당할 때는 결정될 심고를 올려 진리에 의지하고 상의하며 힘을 얻으려는 것을 가리킨다.',\n",
       " '분류:불교 용어',\n",
       " '섬네일\\n빌라 에스쿠데로 플렌테이션(Villa Escudero)은 필리핀의 코코야자 농장이다.\\n여기서 에스쿠데로라는 말은 필리핀의 아주 유명한 가문의 이름이다. 빌라 에스쿠데로는 필리핀의 토속적인 정취를 느낄 수 있는 3대째 계속 내려오는 북부 최대의 개인 농장이다. 비록 규모는 작지만 중국 도자기, 전통 은마차, 조각품 등을 전시하고 있다. 낭만적인 필리핀 전통식사를 즐길 수 있고, 카라바우(물소가 끄는 마차)와 대나무 뗏목도 즐기는 남국의 운치가 있는 곳이다.',\n",
       " '* Villa Escudero Official Web Site\\n* Villa Escudero videos at WN\\n분류:필리핀의 리조트',\n",
       " '웍으로 요리를 볶는 모습\\n웍(, , )은 중국에서 기원한 요리도구로, 커다란 냄비와 솥의 중간에 있는 물건이다. 중국을 비롯한 동아시아 및 동남아시아에서, 매우 대중적인 요리도구이며, 오늘날에는 전세계적으로 사용되고 있다.\\n웍은 볶기, 찌기, 지짐, 튀김, 졸이기, 끓이기, 찌개, 그슬기, 훈제 등 온갖 방법의 조리법에 사용될 수 있다.',\n",
       " '분류:조리용기',\n",
       " '정종여(鄭鍾汝, 1914년~1984년)는 한국의 화가이다. 아호는 청계(靑谿)이다.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text_clean'][:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a165862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    216720.000000\n",
       "mean        330.814752\n",
       "std         987.262379\n",
       "min           0.000000\n",
       "25%          42.000000\n",
       "50%         116.000000\n",
       "75%         306.000000\n",
       "max      109620.000000\n",
       "Name: section_text_clean_len, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text_clean_len'] = df_long['section_text_clean'].apply(lambda x: len(x))\n",
    "df_long['section_text_clean_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "304b4ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 116.0,\n",
       " 0.9: 712.0,\n",
       " 0.95: 1200.0499999999884,\n",
       " 0.99: 3490.0,\n",
       " 0.995: 5244.404999999999,\n",
       " 0.999: 11964.711000000534}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['section_text_clean_len'].quantile([0.5, 0.9, 0.95, 0.99, 0.995, 0.999]).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78d1a32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcc34dfd6524821a56dd5d4e9f9367d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e25fb02509b470fb7707743d46cd85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c01dda2ab9346d99f51933268376dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f245ef00848bba4180089db2f528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "623d06c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814f7c2a137d4f44b7ad313f95c607b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "token_len (num_proc=8):   0%|          | 0/216720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10781 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def add_tok_len(batch):\n",
    "    enc = tokenizer(\n",
    "        batch['section_text_clean'],\n",
    "        add_special_tokens=True,   # 일관되게 (분포 볼 때 보통 True)\n",
    "        truncation=False,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    return {\"tok_len\": [len(ids) for ids in enc[\"input_ids\"]]}\n",
    "\n",
    "\n",
    "ds_clean = Dataset.from_pandas(df_long[['section_text_clean']].dropna())\n",
    "ds_len = ds_clean.map(\n",
    "    add_tok_len,\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    num_proc=8,\n",
    "    desc=\"token_len\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bacd6fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'section_text_clean': '로스앤젤레스 아줄 레전즈는 미국 캘리포니아주 로스앤젤레스를 연고지로 하는 순수 아마추어 축구팀이다. 현재 USL 프리미어 디벨로프먼트 리그에 참가하고 있으며 성적은 중위권이다. 창단 당시에는 로스앤젤레스 스톰, 2008년부터 2009년까지는 로스앤젤레스 레전즈였다가 2010년 크루스 아술과 제휴 계약을 맺으면서 이름도 현재의 이름으로 바꾸었다.\\n분류:USL 리그 2 구단\\n분류:2010년 설립된 축구단\\n분류:로스앤젤레스 아줄 레전즈',\n",
       " 'tok_len': 143}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "773733be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    216720.000000\n",
      "mean        184.693785\n",
      "std         551.015682\n",
      "min           2.000000\n",
      "25%          26.000000\n",
      "50%          67.000000\n",
      "75%         171.000000\n",
      "max       67768.000000\n",
      "dtype: float64\n",
      "quantiles: {0.5: 67, 0.9: 394, 0.95: 659, 0.99: 1914, 0.995: 2851, 0.999: 6727}\n"
     ]
    }
   ],
   "source": [
    "tok = np.array(ds_len['tok_len'])\n",
    "print(pd.Series(tok).describe())\n",
    "print(\"quantiles:\", {q: int(np.quantile(tok, q)) for q in [0.5, 0.9, 0.95, 0.99, 0.995, 0.999]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f431c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 0.16557308970099668\n",
      "384 0.10288390550018457\n",
      "512 0.07075489110372832\n",
      "768 0.04027316352897748\n",
      "1024 0.026448874123292727\n",
      "2048 0.008891657438169066\n"
     ]
    }
   ],
   "source": [
    "for t in [256, 384, 512, 768, 1024, 2048]:\n",
    "    print(t, (tok > t).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "af6369ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b667cfc611f74242976972086ac16c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c963947dc3174c4290112649e2574ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "colbert_linear.pt:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b746b95507e45c1ac65f5eac7e8e43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa9fdaa58264bb7be7f03f11d5e1641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4a0dc897354ebea86cae1a1b2d8c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6e452016a342c0be9629f91b89c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bm25.jpg:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f163075700d4ec78665d94dbf55263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "long.jpg:   0%|          | 0.00/485k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a5d58b0d524937bfd33e9aa4f51033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "miracl.jpg:   0%|          | 0.00/576k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822a23fc53fc4cdd8be6977f98cf1541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nqa.jpg:   0%|          | 0.00/158k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d302b4b467ce4d4e873355895da6ca3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mkqa.jpg:   0%|          | 0.00/608k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe95955b6064d26aca2c1547c63fbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "others.webp:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e42dca857924097910acb93c2c2e7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e823032bd37b4255a9d3ab3869b07d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "long.jpg:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ef5ea95b04ca3b308d6278346ba48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/725k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5878be475c02466985eb2995b20abe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constant_7_attr__value:   0%|          | 0.00/65.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a341f6e5b5834262b20f21b69059e85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9e463510f846deb23ba4fdbc96f5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx_data:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935c33a35f74dfdaa72e13dbaacfad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d578e1265bde464c893869df3fc96a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa95efc7cee4c99bf8c3ecdb0a76ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac370aea22a84108bcd45f3a15e422ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sparse_linear.pt:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c52a8826696434dbb26d60b3f20588c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a295ee94e3c4028a4d2a2afeeeafb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9998207eee74647b0a1b746fdfdcb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06835febcc74b3aa060868ae2b8221a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc7383",
   "metadata": {},
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fcfc803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id  title                                               text\n",
      "0       0  지미 카터  '''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...\n",
      "1       1     수학  '''수학'''(, , '''math''')은 수, 양, 구조, 공간, 변화 등의 ...\n",
      "2       2  수학 상수  '''수학'''에서 '''상수'''란 그 값이 변하지 않는 불변량으로, 변수의 반대...\n",
      "3       3     문학  파일:Fragonard, The Reader.jpg|섬네일|250px|장오노레 프라...\n",
      "4       4  나라 목록  스위스 제네바에 있는 국제 연합 회원국 및 비회원 GA 옵서버의 국기\\n이 목록에 ...\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"lcw99/wikipedia-korean-20240501\", split=\"train\")\n",
    "df = ds.to_pandas()\n",
    "\n",
    "df['doc_id'] = df.index\n",
    "\n",
    "df_clean = df[['doc_id', 'title', 'text']].copy()\n",
    "\n",
    "df_clean = df_clean[df_clean['text'].str.strip().astype(bool)]\n",
    "\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce2df2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515425 entries, 0 to 515424\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   doc_id  515425 non-null  int64 \n",
      " 1   title   515425 non-null  object\n",
      " 2   text    515425 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "96e36c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_text_tokenwise(\n",
    "    text: str,\n",
    "    title: str,\n",
    "    doc_id: int,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 512,\n",
    "    overlap: int = 50,\n",
    "    safety_margin: int = 8,\n",
    "    min_tokens: int = 80,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    - chunk 대상: text\n",
    "    - 최종 입력은 title + chunk_text 를 가정하므로,\n",
    "      text chunk는 effective_max 토큰으로 제한\n",
    "    - overlap은 토큰 단위로 보장\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    title = \"\" if title is None else str(title).strip()\n",
    "    title_tok = tokenizer.encode(title, add_special_tokens=False) if title else []\n",
    "    effective_max = max(32, max_tokens - len(title_tok) - safety_margin)  # 최소 32 보장\n",
    "\n",
    "    # 문장/문단 단위 split\n",
    "    # - \\n+ 로 문단을 나누고, 마침표/물음표/느낌표 뒤 공백에서도 나눔\n",
    "    units = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n",
    "    units = [u.strip() for u in units if u and u.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    cur_tokens = []\n",
    "    chunk_idx = 0\n",
    "\n",
    "    def flush_chunk(tokens):\n",
    "        nonlocal chunk_idx\n",
    "        if not tokens:\n",
    "            return\n",
    "        chunk_text = tokenizer.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n",
    "        if not chunk_text:\n",
    "            return\n",
    "        chunks.append({\n",
    "            \"chunk_id\": f\"{doc_id}_{chunk_idx}\",\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"text\": chunk_text,\n",
    "            \"offset\": chunk_idx,\n",
    "            \"token_len\": len(tokens),\n",
    "        })\n",
    "        chunk_idx += 1\n",
    "\n",
    "    for u in units:\n",
    "        u_tokens = tokenizer.encode(u, add_special_tokens=False)\n",
    "        if not u_tokens:\n",
    "            continue\n",
    "\n",
    "        # 유닛 자체가 effective_max보다 길면: 토큰 슬라이딩으로 쪼갬\n",
    "        if len(u_tokens) > effective_max:\n",
    "            # 현재 누적분 먼저 flush\n",
    "            flush_chunk(cur_tokens)\n",
    "            cur_tokens = []\n",
    "\n",
    "            step = max(1, effective_max - overlap)\n",
    "            for i in range(0, len(u_tokens), step):\n",
    "                sub = u_tokens[i:i + effective_max]\n",
    "                flush_chunk(sub)\n",
    "            continue\n",
    "\n",
    "        # 누적 시 초과하면 flush + overlap 토큰 유지\n",
    "        if len(cur_tokens) + len(u_tokens) > effective_max and cur_tokens:\n",
    "            flush_chunk(cur_tokens)\n",
    "\n",
    "            # 토큰 단위 overlap 유지\n",
    "            keep = min(overlap, len(cur_tokens))\n",
    "            cur_tokens = cur_tokens[-keep:]  # 다음 chunk 시작에 붙일 overlap\n",
    "\n",
    "        cur_tokens.extend(u_tokens)\n",
    "\n",
    "    # 마지막 flush\n",
    "    flush_chunk(cur_tokens)\n",
    "\n",
    "    # 너무 짧은 chunk 후처리(옵션): min_tokens 미만이면 앞 chunk에 합치기\n",
    "    if min_tokens and len(chunks) >= 2:\n",
    "        merged = []\n",
    "        for c in chunks:\n",
    "            if merged and c[\"token_len\"] < min_tokens:\n",
    "                # 이전 chunk에 합치기\n",
    "                prev = merged[-1]\n",
    "                prev_text = prev[\"text\"] + \" \" + c[\"text\"]\n",
    "                prev_tokens = tokenizer.encode(prev_text, add_special_tokens=False)\n",
    "\n",
    "                # 다시 effective_max 맞추기(넘치면 그냥 유지하거나, 뒤를 자르는 정책 선택)\n",
    "                if len(prev_tokens) <= effective_max:\n",
    "                    prev[\"text\"] = prev_text\n",
    "                    prev[\"token_len\"] = len(prev_tokens)\n",
    "                else:\n",
    "                    # 넘치면: 합치지 않고 그냥 독립 chunk로 둠(보수적)\n",
    "                    merged.append(c)\n",
    "            else:\n",
    "                merged.append(c)\n",
    "        chunks = merged\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청킹 시작\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a2ca7c2cc64fdd99bd8d1246381d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m청킹 시작\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, title, doc_id \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m]), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_clean)):\n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 작성하신 함수 호출\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_text_tokenwise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 미리 로드해둔 토크나이저 전달\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 결과 리스트에 추가 (chunks는 리스트 형태이므로 extend 사용)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     all_chunk_records\u001b[38;5;241m.\u001b[39mextend(chunks)\n",
      "Cell \u001b[0;32mIn[117], line 55\u001b[0m, in \u001b[0;36mchunk_text_tokenwise\u001b[0;34m(text, title, doc_id, tokenizer, max_tokens, overlap, safety_margin, min_tokens)\u001b[0m\n\u001b[1;32m     52\u001b[0m     chunk_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m units:\n\u001b[0;32m---> 55\u001b[0m     u_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m u_tokens:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2830\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2851\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2854\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2867\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2870\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3258\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3249\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3250\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3251\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3256\u001b[0m )\n\u001b[0;32m-> 3258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3277\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    626\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 627\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all_chunk_records = []\n",
    "\n",
    "# print(\"청킹 시작\")\n",
    "\n",
    "# for text, title, doc_id in tqdm(zip(df_clean['text'], df_clean['title'], df_clean['doc_id']), total=len(df_clean)):\n",
    "    \n",
    "#     # 작성하신 함수 호출\n",
    "#     chunks = chunk_text_tokenwise(\n",
    "#         text=text,\n",
    "#         title=title,\n",
    "#         doc_id=doc_id,\n",
    "#         tokenizer=tokenizer,  # 미리 로드해둔 토크나이저 전달\n",
    "#         max_tokens=512,\n",
    "#         overlap=50\n",
    "#     )\n",
    "    \n",
    "#     # 결과 리스트에 추가 (chunks는 리스트 형태이므로 extend 사용)\n",
    "#     all_chunk_records.extend(chunks)\n",
    "\n",
    "# df_chunks = pd.DataFrame(all_chunk_records)\n",
    "\n",
    "# print(f\"청킹 완료! 총 청크 개수: {len(df_chunks)}\")\n",
    "# print(df_chunks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4732ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2325.000000\n",
       "mean      380.468817\n",
       "std       135.481123\n",
       "min        53.000000\n",
       "25%       269.000000\n",
       "50%       460.000000\n",
       "75%       488.000000\n",
       "max       527.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks['token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a92d2e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 5개 사용 / 총 516개 배치로 묶어서 처리 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c67a98fcd174ab5a23e7a398697c9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청킹 완료! 총 청크 개수: 1165197\n",
      "  chunk_id  doc_id  title                                               text  \\\n",
      "0      0_0       0  지미 카터  '''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...   \n",
      "1      0_1       0  지미 카터  주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다....   \n",
      "2      0_2       0  지미 카터  6,000명을 감축하는 데 그쳤다. 또한 박정희 정권의 인권 문제 등과의 논란으로 ...   \n",
      "3      0_3       0  지미 카터  과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 ...   \n",
      "4      0_4       0  지미 카터  자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신...   \n",
      "\n",
      "   offset  token_len  \n",
      "0       0        457  \n",
      "1       1        485  \n",
      "2       2        433  \n",
      "3       3        483  \n",
      "4       4        476  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    batch_results = []\n",
    "    # 전역 tokenizer 사용\n",
    "    for text, title, doc_id in zip(batch_df['text'], batch_df['title'], batch_df['doc_id']):\n",
    "        # 기존 로직 그대로 수행\n",
    "        chunks = chunk_text_tokenwise(\n",
    "            text=text, \n",
    "            title=title, \n",
    "            doc_id=doc_id, \n",
    "            tokenizer=tokenizer, \n",
    "            max_tokens=512, \n",
    "            overlap=50\n",
    "        )\n",
    "        batch_results.extend(chunks)\n",
    "    return batch_results\n",
    "\n",
    "BATCH_SIZE = 1000  # 한 번에 1000개씩 처리 (적절한 크기)\n",
    "num_cores = cpu_count()\n",
    "\n",
    "\n",
    "num_batches = len(df_clean) // BATCH_SIZE + 1\n",
    "batches = np.array_split(df_clean, num_batches)\n",
    "\n",
    "print(f\"CPU {num_cores}개 사용 / 총 {len(batches)}개 배치로 묶어서 처리 시작...\")\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(process_batch)(batch)\n",
    "    for batch in tqdm(batches)\n",
    ")\n",
    "\n",
    "all_chunk_records = [chunk for sublist in results for chunk in sublist]\n",
    "df_chunks = pd.DataFrame(all_chunk_records)\n",
    "\n",
    "print(f\"청킹 완료! 총 청크 개수: {len(df_chunks)}\")\n",
    "print(df_chunks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f7542da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1165197 entries, 0 to 1165196\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   chunk_id   1165197 non-null  object\n",
      " 1   doc_id     1165197 non-null  int64 \n",
      " 2   title      1165197 non-null  object\n",
      " 3   text       1165197 non-null  object\n",
      " 4   offset     1165197 non-null  int64 \n",
      " 5   token_len  1165197 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 53.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_chunks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2db43846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>offset</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>지미 카터</td>\n",
       "      <td>'''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>지미 카터</td>\n",
       "      <td>주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다....</td>\n",
       "      <td>1</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_2</td>\n",
       "      <td>0</td>\n",
       "      <td>지미 카터</td>\n",
       "      <td>6,000명을 감축하는 데 그쳤다. 또한 박정희 정권의 인권 문제 등과의 논란으로 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_3</td>\n",
       "      <td>0</td>\n",
       "      <td>지미 카터</td>\n",
       "      <td>과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>지미 카터</td>\n",
       "      <td>자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신...</td>\n",
       "      <td>4</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id  doc_id  title                                               text  \\\n",
       "0      0_0       0  지미 카터  '''제임스 얼 “지미” 카터 주니어'''(, 1924년 10월 1일~)는 민주당 ...   \n",
       "1      0_1       0  지미 카터  주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다....   \n",
       "2      0_2       0  지미 카터  6,000명을 감축하는 데 그쳤다. 또한 박정희 정권의 인권 문제 등과의 논란으로 ...   \n",
       "3      0_3       0  지미 카터  과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 ...   \n",
       "4      0_4       0  지미 카터  자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신...   \n",
       "\n",
       "   offset  token_len  \n",
       "0       0        457  \n",
       "1       1        485  \n",
       "2       2        433  \n",
       "3       3        483  \n",
       "4       4        476  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c47edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_single_row(args):\n",
    "#     text, title, doc_id = args\n",
    "#     # 기존 함수 호출 (tokenizer는 전역 변수로 있는 것을 사용)\n",
    "#     return chunk_text_tokenwise(\n",
    "#         text=text, \n",
    "#         title=title, \n",
    "#         doc_id=doc_id, \n",
    "#         tokenizer=tokenizer, \n",
    "#         max_tokens=512, \n",
    "#         overlap=50\n",
    "#     )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#     data_inputs = list(zip(df_clean['text'], df_clean['title'], df_clean['doc_id']))\n",
    "    \n",
    "#     num_cores = cpu_count()\n",
    "#     print(f\" 총 {num_cores}개의 CPU 코어\")\n",
    "\n",
    "#     all_chunk_records = []\n",
    "    \n",
    "#     with Pool(processes=num_cores) as pool:\n",
    "#         # imap: 순서대로 결과를 뱉어줌 (tqdm과 연동하기 좋음)\n",
    "#         for chunks in tqdm(pool.imap(process_single_row, data_inputs, chunksize=100), total=len(data_inputs)):\n",
    "#             all_chunk_records.extend(chunks)\n",
    "\n",
    "#     df_chunks = pd.DataFrame(all_chunk_records)\n",
    "#     print(f\"청킹 완료! 총 청크 개수: {len(df_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7e69c2f4574f129e80d5ed9481e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1165197개 청크 임베딩\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a11435cc6643ec820c28859bd38c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 22.94it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 23.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 22.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.10it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 22.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 23.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 45.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 48.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 44.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 48.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 18.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 66.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 59.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 38.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 42.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 38.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 38.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 25.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 53.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 24.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 36.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 35.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 35.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 39.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.36it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 61.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 26.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 34.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 36.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 32.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 29.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 30.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 31.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 27.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "batch_size = 32\n",
    "text_list = df_chunks['text'].tolist()\n",
    "\n",
    "all_dense_embeddings = []\n",
    "all_sparse_embeddings = []\n",
    "\n",
    "print(f\"총 {len(text_list)}개 청크 임베딩\")\n",
    "for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "    batch_texts = text_list[i : i + batch_size]\n",
    "    \n",
    "    output = model.encode(\n",
    "        batch_texts, \n",
    "        batch_size=batch_size, \n",
    "        max_length=512,\n",
    "        return_dense=True, \n",
    "        return_sparse=True,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    \n",
    "    all_dense_embeddings.append(output['dense_vecs'])\n",
    "    all_sparse_embeddings.extend(output['lexical_weights'])\n",
    "\n",
    "print(\"FAISS 인덱싱 중...\")\n",
    "dense_vectors = np.vstack(all_dense_embeddings).astype('float32')\n",
    "dimension = dense_vectors.shape[1]\n",
    "\n",
    "# L2 normalize for cosine similarity (IP와 동일 효과)\n",
    "faiss.normalize_L2(dense_vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(dense_vectors)\n",
    "\n",
    "print(f\"FAISS 인덱스 생성 완료 (총 {index.ntotal}개)\")\n",
    "\n",
    "# 4. 저장\n",
    "\n",
    "faiss.write_index(index, \"wikipedia_bge_m3.index\")\n",
    "\n",
    "with open(\"wikipedia_sparse_vecs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_sparse_embeddings, f)\n",
    "\n",
    "df_chunks[['text', 'doc_id', 'title', 'chunk_idx']].to_parquet(\n",
    "    \"wikipedia_chunks_meta.parquet\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 모든 작업 완료!\")\n",
    "print(\"📁 생성된 파일:\")\n",
    "print(\"  1. wikipedia_bge_m3.index (Dense vectors)\")\n",
    "print(\"  2. wikipedia_sparse_vecs.pkl (Sparse vectors)\")\n",
    "print(\"  3. wikipedia_chunks_meta.parquet (메타데이터)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b693406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
