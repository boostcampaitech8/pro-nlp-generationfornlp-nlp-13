{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b593591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720e46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454a4b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             2031 non-null   object \n",
      " 1   paragraph      2031 non-null   object \n",
      " 2   problems       2031 non-null   object \n",
      " 3   question_plus  0 non-null      float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 63.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '/data/ephemeral/pro-nlp-generationfornlp-nlp-13'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "dataset.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df[\"choices_len\"] = df[\"choices\"].apply(len)\n",
    "df['choices_len'].value_counts(dropna=False)\n",
    "\n",
    "df4 = df[df['choices_len'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81153c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 673\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    df4,\n",
    "    test_size=0.15,        \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "valid_ds = Dataset.from_pandas(valid_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e562d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\"\n",
    "    )\n",
    "\n",
    "### 정책별 System Prompt 함수\n",
    "def get_system_message(row, system_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 하나의 데이터 행\n",
    "    system_prompts: {choices_len: {version: prompt_text}}\n",
    "    prompt_policy: {choices_len: version}\n",
    "    \"\"\"\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    return system_prompts[choices_len][version]\n",
    "\n",
    "SYSTEM_PROMPT_4_V1 = (\n",
    "    \"당신은 **지식 추론(Knowledge Inference) 전문가**입니다. \"\n",
    "    \"이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. \"\n",
    "    \"지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 \"\n",
    "    \"가장 타당한 선택지 하나를 고르십시오.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_5_V1 = (\n",
    "    \"당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. \"\n",
    "    \"이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. \"\n",
    "    \"당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\\n\\n\"\n",
    ")\n",
    "\n",
    "# PROMPT_POLICY:\n",
    "# - 이번 실험(run)에서 \"어떤 system prompt 버전을 사용할지\"를 결정하는 설정값\n",
    "# - choices_len(4 or 5) → 사용할 prompt 버전(v1, v2, ...)\n",
    "# - 실험을 바꿀 때는 이 딕셔너리만 수정\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": SYSTEM_PROMPT_4_V1,\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": SYSTEM_PROMPT_5_V1,\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2b09747",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 User Prompt 함수\n",
    "def get_user_message(row, user_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 데이터 행\n",
    "    user_prompts: 템플릿 저장소\n",
    "    prompt_policy: 버전 정책\n",
    "    \"\"\"\n",
    "    # 메타 데이터 확인\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    \n",
    "    # 해당 버전의 템플릿 세트 가져오기 (plus, no_plus가 들어있음)\n",
    "    template_set = user_prompts[choices_len][version]\n",
    "    \n",
    "    # 데이터 준비\n",
    "    paragraph = row['paragraph']\n",
    "    question = row['question']\n",
    "    choices_str = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(row['choices'])])\n",
    "    q_plus = row.get('question_plus', None)\n",
    "    \n",
    "    # 분기 처리 및 포맷팅 (여기가 핵심!)\n",
    "    # q_plus가 존재하고, nan이 아닐 때 -> Plus 템플릿 사용\n",
    "    if q_plus and str(q_plus) != 'nan':\n",
    "        return template_set[\"plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question_plus=q_plus, # 여기 들어감\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    # q_plus가 없을 때 -> No Plus 템플릿 사용\n",
    "    else:\n",
    "        return template_set[\"no_plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    \n",
    "# =========================\n",
    "# User Prompt Templates (V1)\n",
    "# =========================\n",
    "\n",
    "# 4지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 4지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 4지선다 + <보기> 있음 + RAG 힌트 추가\n",
    "USER_PROMPT_PLUS_4_RAG_V1 = \"\"\"### 참고 정보 (검색된 지식)\n",
    "{retrieved_context}\n",
    "\n",
    "### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 상단의 '참고 정보'와 '지문'을 바탕으로 문제 풀이에 필요한 핵심 단서를 정리하세요.\n",
    "2. '참고 정보'의 내용이 지문과 상호 보완적일 경우 이를 적극적으로 활용하여 추론하세요.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "# 4지선다 + <보기> 없음 + RAG 힌트 추가\n",
    "USER_PROMPT_NO_PLUS_4_RAG_V1 = \"\"\"### 참고 정보 (검색된 지식)\n",
    "{retrieved_context}\n",
    "\n",
    "### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 상단의 '참고 정보'와 '지문'을 바탕으로 문제 풀이에 필요한 핵심 단서를 정리하세요.\n",
    "2. 지문 내용만으로 부족할 경우 '참고 정보'에 기술된 개념이나 사실을 근거로 판단하세요.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "# 4지선다 + <보기> 없음 + RAG 힌트 추가 (V2: 지문 우선순위 강화)\n",
    "USER_PROMPT_NO_PLUS_4_RAG_V2 = \"\"\"### 참고 정보 (검색된 지식)\n",
    "{retrieved_context}\n",
    "\n",
    "### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 상단의 '참고 정보'와 '지문'을 대조하여 문제 풀이에 필요한 핵심 팩트를 정리하세요.\n",
    "2. 지문 내용이 부족할 경우 '참고 정보'를 활용하되, **반드시 '지문'에 명시된 조건과 모순되지 않는지 확인하세요.** (지문 조건이 최우선입니다.)\n",
    "3. 선택지 중 지문의 상황을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "USER_PROMPT_PLUS_4_RAG_V2 = \"\"\"### 참고 정보 (검색된 지식)\n",
    "{retrieved_context}\n",
    "\n",
    "### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. '지문'의 조건과 '참고 정보'의 핵심 단서를 각각 정리하세요.\n",
    "2. '참고 정보'를 활용하되, 반드시 '지문'에 명시된 조건과 모순되지 않는지 먼저 확인하세요. (지문 조건이 최우선입니다.)\n",
    "3. 선택지 중 지문의 상황을 가장 잘 만족하고, 참고 정보와 일치하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "# 5지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "USER_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_5_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_5_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    }\n",
    "}\n",
    "\n",
    "USER_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08b8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Assistant Prompt 함수\n",
    "def get_assistant_message(row):\n",
    "    \"\"\"\n",
    "    Assistant 메시지 생성 함수.\n",
    "    Qwen3 모델의 토크나이저 템플릿이 자동으로 <think> 태그를 처리하므로,\n",
    "    여기서는 순수한 정답(Label) 텍스트만 반환\n",
    "    \"\"\"\n",
    "    return str(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2c3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    \"\"\"\n",
    "    원본 example(row)로부터 학습용 chat messages를 구성한다.\n",
    "    - choices_len(4/5) 및 question_plus 유무에 따라 system/user 프롬프트를 선택\n",
    "    - assistant는 정답 숫자만\n",
    "    - 이후 평가/추적용으로 id, label도 함께 유지\n",
    "    \"\"\"\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "    asst_msg = get_assistant_message(example)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ],\n",
    "        \"label\": int(example[\"answer\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_test_messages(example):\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def to_text(example):\n",
    "    \"\"\"\n",
    "    messages(list[dict])를 tokenizer의 chat_template 규칙에 따라\n",
    "    단일 텍스트로 직렬화한다.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  \n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "def to_test_text(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, \n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "def tokenize_fn(example, truncation=True, max_length=2048, padding=False):\n",
    "    \"\"\"\n",
    "    batched=True면 example[\"text\"]는 List[str]\n",
    "    batched=False면 example[\"text\"]는 str\n",
    "    \"\"\"\n",
    "    tok_kwargs = dict(truncation=truncation, padding=padding)\n",
    "    if truncation is True:\n",
    "        tok_kwargs[\"max_length\"] = max_length\n",
    "\n",
    "    out = tokenizer(example[\"text\"], **tok_kwargs)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": out[\"input_ids\"],\n",
    "        \"attention_mask\": out[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"truncation\": True, \"max_length\": 2048, \"padding\": False},\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ead907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa5448565d7409791e8238946aa83c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360479efb8cc475c857d19cdd285821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec992f70f2f4f6da256fd2f1deed2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733abe84c32b462cb839f092e74d0790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effd7252b66048879a91d684334bfcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3c5e39dc0643879ae6e323a210354d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"truncation\": True, \"max_length\": 2048, \"padding\": False},\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9968d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 변환 완료 ===\n",
      "Train 개수: 673\n",
      "첫 번째 샘플 Keys: dict_keys(['id', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 변환 완료 ===\")\n",
    "print(\"Train 개수:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"첫 번째 샘플 Keys:\", tokenized_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fb965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 673\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da9ac8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed06c97227648febb131006a4318ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4f7dcf839e49288643a845bf27c685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 검증용\n",
    "test_ds_msg = valid_ds.map(\n",
    "    build_test_messages,\n",
    "    batched=False,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "test_ds_text = test_ds_msg.map(\n",
    "    to_test_text,\n",
    "    batched=False,\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "225fdf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a53f2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels, pos_from_tail=4):\n",
    "    \"\"\"\n",
    "    반환: (batch, 5)  -> '1'~'5'에 해당하는 logits만 뽑아서 metrics 단계로 전달\n",
    "    \"\"\"\n",
    "    # Trainer가 (logits, ...) 튜플을 줄 때가 있어서 정리\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # (B, L, V)\n",
    "\n",
    "    # labels: (B, L), pad/무시 영역은 -100일 가능성이 큼\n",
    "    # real_len = 마지막으로 labels != -100 인 위치 + 1 로 복원\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "    not_ignored = (labels_t != -100)\n",
    "\n",
    "    # 샘플별로 마지막 not_ignored 위치 찾기\n",
    "    # (뒤에서부터 True 찾기)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)          # (B,)\n",
    "    has_any = not_ignored.any(dim=1)                             # (B,)\n",
    "    # real_len = seq_len - last_true_from_end\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "\n",
    "    # 만약 labels가 전부 -100인 샘플이 있으면(비정상) 그냥 seq_len로 처리\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos = (real_len - pos_from_tail).clamp(min=0, max=seq_len-1) # (B,)\n",
    "\n",
    "    # (B, V)로 해당 위치의 logits만 gather\n",
    "    logits_t = torch.as_tensor(logits)                           # (B, L, V)\n",
    "    batch_idx = torch.arange(logits_t.size(0), device=logits_t.device)\n",
    "    picked = logits_t[batch_idx, pos, :]                         # (B, V)\n",
    "\n",
    "    # digit ids만 슬라이스 -> (B, 5)\n",
    "    picked_digits = picked[:, DIGIT_IDS]\n",
    "    return picked_digits\n",
    "\n",
    "def compute_metrics(eval_pred, label_pos_from_tail=3):\n",
    "    \"\"\"\n",
    "    eval_pred:\n",
    "      - (predictions, label_ids) 튜플 형태가 가장 흔함\n",
    "      - predictions: preprocess_logits_for_metrics가 반환한 (B, 5)\n",
    "      - label_ids: (B, L) with -100 ignored\n",
    "    반환: {\"accuracy\": ..., \"macro_f1\": ...}\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "    preds_t = torch.as_tensor(preds)\n",
    "    pred_cls = torch.argmax(preds_t, dim=-1).cpu().numpy().astype(np.int64)  # (B,)\n",
    "\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "\n",
    "    not_ignored = (labels_t != -100)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)\n",
    "    has_any = not_ignored.any(dim=1)\n",
    "\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos_label = (real_len - label_pos_from_tail).clamp(min=0, max=seq_len - 1)\n",
    "    batch_idx = torch.arange(labels_t.size(0), device=labels_t.device)\n",
    "    gold_tok = labels_t[batch_idx, pos_label].cpu().numpy().astype(np.int64) \n",
    "\n",
    "    gold_cls = gold_tok - DIGIT_IDS[0]  \n",
    "\n",
    "    valid = (gold_cls >= 0) & (gold_cls < 5)\n",
    "    pred_cls = pred_cls[valid]\n",
    "    gold_cls = gold_cls[valid]\n",
    "\n",
    "    acc = (pred_cls == gold_cls).mean() if len(gold_cls) > 0 else 0.0\n",
    "\n",
    "    f1s = []\n",
    "    for c in range(5):\n",
    "        tp = np.sum((pred_cls == c) & (gold_cls == c))\n",
    "        fp = np.sum((pred_cls == c) & (gold_cls != c))\n",
    "        fn = np.sum((pred_cls != c) & (gold_cls == c))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_f1 = float(np.mean(f1s)) if len(f1s) > 0 else 0.0\n",
    "\n",
    "    return {\"accuracy\": float(acc), \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9fd149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_dataset = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27647e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10f323bbfcc40a3ac0aefcdefada32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Pad Token: <|endoftext|> (151643)\n",
      "Use Cache: False, Grad Ckpt: True\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 이름 및 양자화 설정\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 2. 토크나이저 먼저 로드 및 PAD 설정 (중요!)\n",
    "# 모델 로드 전에 PAD 토큰을 확정지어야 나중에 모델 설정에 바로 반영됩니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. 모델 로드 (양자화 및 장치 할당)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 4. 모델 설정 업데이트\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 토크나이저의 PAD 설정을 모델에 동기화\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 확인 출력\n",
    "print(f\"Final Pad Token: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
    "print(f\"Use Cache: {model.config.use_cache}, Grad Ckpt: {model.is_gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4fdd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attention proj만\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Attention + MLP까지 (성능 더 노리되 trainable 조금 증가)\n",
    "# \"gate_proj\", \"up_proj\", \"down_proj\" -> FFN\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# target_modules = [\"q_proj\", \"k_proj\"]\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "080c9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../../qwen-sft-results\",\n",
    "    \n",
    "    num_train_epochs=2,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 전략 설정\n",
    "    eval_strategy=\"steps\",       # 주석 해제 (활성화)\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",          # 저장 안 함 (비교용)\n",
    "    load_best_model_at_end=False, # 저장 안 할 때는 False가 안전\n",
    "    \n",
    "    # 지표 및 로깅\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b58b0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df3c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 23:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.182100</td>\n",
       "      <td>0.132695</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.507284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.110179</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.571552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.107921</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.574373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.112901</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.573093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.109935</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.561669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.108128</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.567527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.104781</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.554779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.104107</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.554779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=0.374595701343873, metrics={'train_runtime': 1405.3218, 'train_samples_per_second': 0.958, 'train_steps_per_second': 0.121, 'total_flos': 4.16753022432768e+16, 'train_loss': 0.374595701343873, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78eb8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 119/119 [02:02<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료! 총 119개 중 119개 추출 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer_results = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for ex in tqdm(test_ds_text):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # temperature=0.0일 때는 do_sample=False여야 합니다.\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,      \n",
    "            do_sample=False,\n",
    "            # temperature=0.0, # do_sample=False일 땐 생략 가능\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        pred = 'N/A' # 찾지 못했을 때의 기본값\n",
    "        found = False\n",
    "        \n",
    "        # 텍스트 전체에서 뒤집어서(reversed) 검사\n",
    "        for char in reversed(gen_text):\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                pred = char\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        # 분석을 위해 더 많은 정보를 저장합니다.\n",
    "        infer_results.append({\n",
    "            \"id\": _id,\n",
    "            \"prediction\": pred,\n",
    "            \"is_found\": found,\n",
    "            \"raw_output\": gen_text \n",
    "        })\n",
    "\n",
    "print(f\"완료! 총 {len(infer_results)}개 중 {sum(1 for x in infer_results if x['is_found'])}개 추출 성공\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f385dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ 최종 검증 정확도: 0.7059\n",
      "추출 성공률: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_pred = pd.DataFrame(infer_results) # 여기에는 이미 'prediction' 컬럼이 있습니다.\n",
    "\n",
    "df_analysis = pd.merge(\n",
    "    df_pred, \n",
    "    valid_df[['id', 'paragraph', 'question', 'answer']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_analysis.rename(columns={'answer': 'ground_truth'}, inplace=True)\n",
    "\n",
    "accuracy = (df_analysis['prediction'].astype(str) == df_analysis['ground_truth'].astype(str)).mean()\n",
    "\n",
    "print(f\"✨ 최종 검증 정확도: {accuracy:.4f}\")\n",
    "print(f\"추출 성공률: {df_analysis['is_found'].mean():.2%}\")\n",
    "\n",
    "df_analysis.to_csv(\"val_results_no_RAG.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53d724d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# 기존 객체 삭제\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'model' in locals(): del model\n",
    "\n",
    "# 가비지 컬렉션 및 CUDA 캐시 비우기\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75321b2",
   "metadata": {},
   "source": [
    "### With RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce664a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2253c01403f64dfd9c9f049a0d06d3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "### 검색\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)\n",
    "index = faiss.read_index(\"wikipedia_bge_m3.index\")\n",
    "df = pd.read_parquet(\"wikipedia_chunks_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2efb98a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1165197 entries, 0 to 1165196\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   doc_id    1165197 non-null  int64 \n",
      " 1   title     1165197 non-null  object\n",
      " 2   text      1165197 non-null  object\n",
      " 3   chunk_id  1165197 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 35.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b4f05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_context(row, top_k=1):\n",
    "    query = f\"{row['paragraph']} \\n\\n {row['question']}\"\n",
    "    \n",
    "    q_vec = model.encode([query], return_dense=True)['dense_vecs']\n",
    "    \n",
    "    q_vec = q_vec.astype(\"float32\")\n",
    "    \n",
    "    # 검색\n",
    "    distances, indices = index.search(q_vec, top_k)\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for i in indices[0]:\n",
    "        if i != -1:\n",
    "            retrieved_docs.append(df.iloc[i]['text'])\n",
    "            \n",
    "    return \"\\n\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "435bdbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 119/119 [03:49<00:00,  1.93s/it]\n",
      "100%|██████████| 673/673 [21:15<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "valid_df['retrieved_context'] = valid_df.progress_apply(get_retrieved_context, axis=1)\n",
    "train_df['retrieved_context'] = train_df.progress_apply(get_retrieved_context, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f57069",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e5f70ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len', 'retrieved_context', '__index_level_0__'],\n",
       "    num_rows: 673\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "072f80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_V1,\n",
    "        },\n",
    "        # 추가: RAG 전용 버전\n",
    "        \"rag_v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_RAG_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_RAG_V1,\n",
    "        },\n",
    "        \"rag_v2\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_RAG_V2,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_RAG_V2,\n",
    "        },\n",
    "    \n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_5_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_5_V1,\n",
    "        },\n",
    "        # 필요시 5지선다 RAG 템플릿도 추가\n",
    "    }\n",
    "}\n",
    "\n",
    "# 정책을 rag_v1으로 변경\n",
    "USER_PROMPT_POLICY = {\n",
    "    4: \"rag_v2\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "137acd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_message(row, user_prompts, prompt_policy):\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    template_set = user_prompts[choices_len][version]\n",
    "    \n",
    "    paragraph = row['paragraph']\n",
    "    question = row['question']\n",
    "    choices_str = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(row['choices'])])\n",
    "    q_plus = row.get('question_plus', None)\n",
    "    \n",
    "    # RAG 데이터 가져오기 (없을 경우 빈 문자열)\n",
    "    r_context = row.get('retrieved_context', \"\")\n",
    "    \n",
    "    # 템플릿에 들어갈 인자 구성 (모든 템플릿 변수를 포함해도 무방)\n",
    "    format_kwargs = {\n",
    "        \"paragraph\": paragraph,\n",
    "        \"question\": question,\n",
    "        \"choices\": choices_str,\n",
    "        \"retrieved_context\": r_context, # RAG 템플릿일 때 사용됨\n",
    "        \"question_plus\": q_plus if q_plus and str(q_plus) != 'nan' else \"\"\n",
    "    }\n",
    "    \n",
    "    if q_plus and str(q_plus) != 'nan':\n",
    "        return template_set[\"plus\"].format(**format_kwargs)\n",
    "    else:\n",
    "        return template_set[\"no_plus\"].format(**format_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7449fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    \"\"\"\n",
    "    원본 example(row)로부터 학습용 chat messages를 구성한다.\n",
    "    - choices_len(4/5) 및 question_plus 유무에 따라 system/user 프롬프트를 선택\n",
    "    - assistant는 정답 숫자만\n",
    "    - 이후 평가/추적용으로 id, label도 함께 유지\n",
    "    \"\"\"\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "    asst_msg = get_assistant_message(example)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ],\n",
    "        \"label\": int(example[\"answer\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_test_messages(example):\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89999e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9afd56ee69468a83be2e50ca541dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff03bc3589a7480f993f24ee37cd79de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854eaec15fb340fc927c15e474cc1f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99495dbe34444abaa20ae2a5c17c425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f23dd9992e450bbb139913f0968e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63173edd1fb4db780491589ad769f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12101312aaf4271b1c12821ec0739fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75df3bf49e104a7cbd2927bb2ba036d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"truncation\": True, \"max_length\": 2048, \"padding\": False},\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "### 검증용\n",
    "test_ds_msg = valid_ds.map(\n",
    "    build_test_messages,\n",
    "    batched=False,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "test_ds_text = test_ds_msg.map(\n",
    "    to_test_text,\n",
    "    batched=False,\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa72987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Dataset Stats]\n",
      "- 평균 토큰 길이: 1164.1\n",
      "- 최대 토큰 길이: 1805\n",
      "- 2048 초과 개수: 0 / 전체 673\n",
      "- 95퍼센타일 길이: 1549.8\n",
      "------------------------------\n",
      "[Validation Dataset Stats]\n",
      "- 평균 토큰 길이: 1130.7\n",
      "- 최대 토큰 길이: 1839\n",
      "- 2048 초과 개수: 0 / 전체 119\n",
      "- 95퍼센타일 길이: 1575.2\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_lens = [len(ids) for ids in tokenized_dataset[\"train\"][\"input_ids\"]]\n",
    "valid_lens = [len(ids) for ids in tokenized_dataset[\"validation\"][\"input_ids\"]]\n",
    "\n",
    "def print_stats(name, lens):\n",
    "    print(f\"[{name} Dataset Stats]\")\n",
    "    print(f\"- 평균 토큰 길이: {np.mean(lens):.1f}\")\n",
    "    print(f\"- 최대 토큰 길이: {max(lens)}\")\n",
    "    print(f\"- 2048 초과 개수: {sum(1 for l in lens if l >= 2048)} / 전체 {len(lens)}\")\n",
    "    print(f\"- 95퍼센타일 길이: {np.percentile(lens, 95):.1f}\") # 상위 5% 수준의 길이\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print_stats(\"Train\", train_lens)\n",
    "print_stats(\"Validation\", valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b023b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-1261',\n",
       " 'paragraph': '오, 수치스럽도다, 불쌍한 겨울의 왕이여! 그대는 도대체 무슨 짓을 벌인 것인가? 카이저의 왕좌를 찬탈하는 것은 무척이나 나쁜 일이 아니던가? 이제 그대는 라인강과 프라하 모두로부터 멀어져야 할지니, 무엇보다도 수치와 경멸에 의해 밤낮으로 괴로움에 떨 것이다. 그대와 온 세상이 잘 알고 있었고, 그들 역시도 잘 알고 있었다, 바로 페르디난트만이 보헤미아의 정당한 왕이라는 것을. 그러니 프리츠여, 일어서서 그대의 왕 페르디난트에게 가라, 그대의 왕에게 부디 그 죄를 사하게 해달라 은혜롭게 간청하라. ”불쌍한 겨울의 왕,” 17세기의 노래',\n",
       " 'question': '다음 중 위 노래에 영감을 준 사건은?',\n",
       " 'choices': ['아우스부르크의 평화', '스페인 왕위 계승 전쟁', '낭트칙령', '30년 전쟁'],\n",
       " 'answer': 4,\n",
       " 'question_plus': None,\n",
       " 'choices_len': 4,\n",
       " 'retrieved_context': \"보헤미아 국왕시절의 베드르지흐(프리드리히 5세) '''프리드리히 5세'''(Friedrich V, 1596년 8월 26일 ~ 1632년 11월 29일)는 라인의 팔츠 선제후 겸 보헤미아 왕국의 국왕(재위: 1610년 ~ 1620년)이다. 보헤미아 국왕으로는 '''베드르지흐'''()로 불렸다. 그가 보헤미아 왕으로 재위한 후 정적인 제국 측은 그의 치세가 그해 겨울 안에 끝날 것이라는 의미의 '''겨울왕'''()이라는 별칭을 붙여 모욕했고, 실제 그의 치세가 짧은 기간에 그치면서 이 별명이 굳어지게 되었다. 프리드리히는 프랑스식 교육을 받았으며 1610년 부친 프리드리히 4세가 사망하자 승계했다. 프로테스탄트 국가인 보헤미아에서 귀족들이 로마 가톨릭교도인 오스트리아 대공 페르디난트 2세에 대항해 보헤미아 반란을 일으켰고 1619년 11월 4일 프라하에서 즉위식을 가져 보헤미아 왕위에 올랐다. 보헤미아 국왕으로 등극한 직후 베드르지흐는 진드리히 마타야스 트런 백작과 더불어 보헤미아-팔츠-지벤뷔르겐의 개신교 연합군을 이끌고 빈을 공격했으나 아무런 성과도 거두지 못했다. 이러한 선제공격으로 유럽의 가톨릭 국가들은 페르디난트 2세에 대한 지원을 결의하였고, 그러한 결의로 가톨릭 연합군은 개신교 연합군보다 우위를 차지하게 되었다. 그는 보헤미아-팔츠의 총사령관이었던 안할트(C. v. Anhalt)로 하여금 최후 결전을 준비하게 했다.\",\n",
       " '__index_level_0__': 694,\n",
       " 'messages': [{'content': \"당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.\",\n",
       "   'role': 'system'},\n",
       "  {'content': \"### 참고 정보 (검색된 지식)\\n보헤미아 국왕시절의 베드르지흐(프리드리히 5세) '''프리드리히 5세'''(Friedrich V, 1596년 8월 26일 ~ 1632년 11월 29일)는 라인의 팔츠 선제후 겸 보헤미아 왕국의 국왕(재위: 1610년 ~ 1620년)이다. 보헤미아 국왕으로는 '''베드르지흐'''()로 불렸다. 그가 보헤미아 왕으로 재위한 후 정적인 제국 측은 그의 치세가 그해 겨울 안에 끝날 것이라는 의미의 '''겨울왕'''()이라는 별칭을 붙여 모욕했고, 실제 그의 치세가 짧은 기간에 그치면서 이 별명이 굳어지게 되었다. 프리드리히는 프랑스식 교육을 받았으며 1610년 부친 프리드리히 4세가 사망하자 승계했다. 프로테스탄트 국가인 보헤미아에서 귀족들이 로마 가톨릭교도인 오스트리아 대공 페르디난트 2세에 대항해 보헤미아 반란을 일으켰고 1619년 11월 4일 프라하에서 즉위식을 가져 보헤미아 왕위에 올랐다. 보헤미아 국왕으로 등극한 직후 베드르지흐는 진드리히 마타야스 트런 백작과 더불어 보헤미아-팔츠-지벤뷔르겐의 개신교 연합군을 이끌고 빈을 공격했으나 아무런 성과도 거두지 못했다. 이러한 선제공격으로 유럽의 가톨릭 국가들은 페르디난트 2세에 대한 지원을 결의하였고, 그러한 결의로 가톨릭 연합군은 개신교 연합군보다 우위를 차지하게 되었다. 그는 보헤미아-팔츠의 총사령관이었던 안할트(C. v. Anhalt)로 하여금 최후 결전을 준비하게 했다.\\n\\n### 지문\\n오, 수치스럽도다, 불쌍한 겨울의 왕이여! 그대는 도대체 무슨 짓을 벌인 것인가? 카이저의 왕좌를 찬탈하는 것은 무척이나 나쁜 일이 아니던가? 이제 그대는 라인강과 프라하 모두로부터 멀어져야 할지니, 무엇보다도 수치와 경멸에 의해 밤낮으로 괴로움에 떨 것이다. 그대와 온 세상이 잘 알고 있었고, 그들 역시도 잘 알고 있었다, 바로 페르디난트만이 보헤미아의 정당한 왕이라는 것을. 그러니 프리츠여, 일어서서 그대의 왕 페르디난트에게 가라, 그대의 왕에게 부디 그 죄를 사하게 해달라 은혜롭게 간청하라. ”불쌍한 겨울의 왕,” 17세기의 노래\\n\\n### 질문\\n다음 중 위 노래에 영감을 준 사건은?\\n\\n### 선택지\\n1. 아우스부르크의 평화\\n2. 스페인 왕위 계승 전쟁\\n3. 낭트칙령\\n4. 30년 전쟁\\n\\n### 문제 해결 가이드라인\\n1. 상단의 '참고 정보'와 '지문'을 대조하여 문제 풀이에 필요한 핵심 팩트를 정리하세요.\\n2. 지문 내용이 부족할 경우 '참고 정보'를 활용하되, **반드시 '지문'에 명시된 조건과 모순되지 않는지 확인하세요.** (지문 조건이 최우선입니다.)\\n3. 선택지 중 지문의 상황을 가장 잘 만족하는 것 하나만 고르세요.\\n\\n정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\\n정답:\",\n",
       "   'role': 'user'}],\n",
       " 'text': \"<|im_start|>system\\n당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.<|im_end|>\\n<|im_start|>user\\n### 참고 정보 (검색된 지식)\\n보헤미아 국왕시절의 베드르지흐(프리드리히 5세) '''프리드리히 5세'''(Friedrich V, 1596년 8월 26일 ~ 1632년 11월 29일)는 라인의 팔츠 선제후 겸 보헤미아 왕국의 국왕(재위: 1610년 ~ 1620년)이다. 보헤미아 국왕으로는 '''베드르지흐'''()로 불렸다. 그가 보헤미아 왕으로 재위한 후 정적인 제국 측은 그의 치세가 그해 겨울 안에 끝날 것이라는 의미의 '''겨울왕'''()이라는 별칭을 붙여 모욕했고, 실제 그의 치세가 짧은 기간에 그치면서 이 별명이 굳어지게 되었다. 프리드리히는 프랑스식 교육을 받았으며 1610년 부친 프리드리히 4세가 사망하자 승계했다. 프로테스탄트 국가인 보헤미아에서 귀족들이 로마 가톨릭교도인 오스트리아 대공 페르디난트 2세에 대항해 보헤미아 반란을 일으켰고 1619년 11월 4일 프라하에서 즉위식을 가져 보헤미아 왕위에 올랐다. 보헤미아 국왕으로 등극한 직후 베드르지흐는 진드리히 마타야스 트런 백작과 더불어 보헤미아-팔츠-지벤뷔르겐의 개신교 연합군을 이끌고 빈을 공격했으나 아무런 성과도 거두지 못했다. 이러한 선제공격으로 유럽의 가톨릭 국가들은 페르디난트 2세에 대한 지원을 결의하였고, 그러한 결의로 가톨릭 연합군은 개신교 연합군보다 우위를 차지하게 되었다. 그는 보헤미아-팔츠의 총사령관이었던 안할트(C. v. Anhalt)로 하여금 최후 결전을 준비하게 했다.\\n\\n### 지문\\n오, 수치스럽도다, 불쌍한 겨울의 왕이여! 그대는 도대체 무슨 짓을 벌인 것인가? 카이저의 왕좌를 찬탈하는 것은 무척이나 나쁜 일이 아니던가? 이제 그대는 라인강과 프라하 모두로부터 멀어져야 할지니, 무엇보다도 수치와 경멸에 의해 밤낮으로 괴로움에 떨 것이다. 그대와 온 세상이 잘 알고 있었고, 그들 역시도 잘 알고 있었다, 바로 페르디난트만이 보헤미아의 정당한 왕이라는 것을. 그러니 프리츠여, 일어서서 그대의 왕 페르디난트에게 가라, 그대의 왕에게 부디 그 죄를 사하게 해달라 은혜롭게 간청하라. ”불쌍한 겨울의 왕,” 17세기의 노래\\n\\n### 질문\\n다음 중 위 노래에 영감을 준 사건은?\\n\\n### 선택지\\n1. 아우스부르크의 평화\\n2. 스페인 왕위 계승 전쟁\\n3. 낭트칙령\\n4. 30년 전쟁\\n\\n### 문제 해결 가이드라인\\n1. 상단의 '참고 정보'와 '지문'을 대조하여 문제 풀이에 필요한 핵심 팩트를 정리하세요.\\n2. 지문 내용이 부족할 경우 '참고 정보'를 활용하되, **반드시 '지문'에 명시된 조건과 모순되지 않는지 확인하세요.** (지문 조건이 최우선입니다.)\\n3. 선택지 중 지문의 상황을 가장 잘 만족하는 것 하나만 고르세요.\\n\\n정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\\n정답:<|im_end|>\\n<|im_start|>assistant\\n\"}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0decec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbeb9beee68d4d2c962f140b99ef5472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Pad Token: <|endoftext|> (151643)\n",
      "Use Cache: False, Grad Ckpt: True\n",
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n"
     ]
    }
   ],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataset = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_dataset = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])\n",
    "\n",
    "# 모델 이름 및 양자화 설정\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델 로드 (양자화 및 장치 할당)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 모델 설정 업데이트\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 토크나이저의 PAD 설정을 모델에 동기화\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 확인 출력\n",
    "print(f\"Final Pad Token: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
    "print(f\"Use Cache: {model.config.use_cache}, Grad Ckpt: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attention proj만\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Attention + MLP까지 (성능 더 노리되 trainable 조금 증가)\n",
    "# \"gate_proj\", \"up_proj\", \"down_proj\" -> FFN\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# target_modules = [\"q_proj\", \"k_proj\"]\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2acef99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../../qwen-sft-results\",\n",
    "    \n",
    "    num_train_epochs=2,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 전략 설정\n",
    "    eval_strategy=\"steps\",       # 주석 해제 (활성화)\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",          # 저장 안 함 (비교용)\n",
    "    load_best_model_at_end=False, # 저장 안 할 때는 False가 안전\n",
    "    \n",
    "    # 지표 및 로깅\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74540bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 43:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.213500</td>\n",
       "      <td>0.155997</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>0.415040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.124936</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.527464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.125845</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.525704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.131436</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.563249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.121077</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.554445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.120533</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.539082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.118769</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.538153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.116921</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.538153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=0.3773255818030413, metrics={'train_runtime': 2652.2344, 'train_samples_per_second': 0.507, 'train_steps_per_second': 0.064, 'total_flos': 8.001250633660416e+16, 'train_loss': 0.3773255818030413, 'epoch': 2.0})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12fec672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:37<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료! 총 119개 중 119개 추출 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer_results3 = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for ex in tqdm(test_ds_text):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        retrieved_chunk = ex.get(\"retrieved_context\") or ex.get(\"retrieved_chunk\") or ex.get(\"context\")\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # temperature=0.0일 때는 do_sample=False여야 합니다.\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,      \n",
    "            do_sample=False,\n",
    "            # temperature=0.0, # do_sample=False일 땐 생략 가능\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        pred = 'N/A' # 찾지 못했을 때의 기본값\n",
    "        found = False\n",
    "        \n",
    "        # 텍스트 전체에서 뒤집어서(reversed) 검사\n",
    "        for char in reversed(gen_text):\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                pred = char\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        # 분석을 위해 더 많은 정보를 저장합니다.\n",
    "        infer_results3.append({\n",
    "            \"id\": _id,\n",
    "            \"prediction\": pred,\n",
    "            \"is_found\": found,\n",
    "            \"raw_output\": gen_text,\n",
    "            \"retrieved_chunk\": retrieved_chunk, \n",
    "        })\n",
    "\n",
    "print(f\"완료! 총 {len(infer_results3)}개 중 {sum(1 for x in infer_results3 if x['is_found'])}개 추출 성공\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6850f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ 최종 검증 정확도: 0.6807\n",
      "추출 성공률: 100.00%\n"
     ]
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(infer_results3) # 여기에는 이미 'prediction' 컬럼이 있습니다.\n",
    "\n",
    "df_analysis = pd.merge(\n",
    "    df_pred, \n",
    "    valid_df[['id', 'paragraph', 'question', 'answer']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_analysis.rename(columns={'answer': 'ground_truth'}, inplace=True)\n",
    "\n",
    "accuracy = (df_analysis['prediction'].astype(str) == df_analysis['ground_truth'].astype(str)).mean()\n",
    "\n",
    "print(f\"✨ 최종 검증 정확도: {accuracy:.4f}\")\n",
    "print(f\"추출 성공률: {df_analysis['is_found'].mean():.2%}\")\n",
    "\n",
    "df_analysis.to_csv(\"val_results_RAG2.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d56cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
