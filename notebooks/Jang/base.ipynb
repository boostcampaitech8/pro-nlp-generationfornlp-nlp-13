{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70614a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60353771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910ba9b",
   "metadata": {},
   "source": [
    "### 학습 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8030d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             2031 non-null   object \n",
      " 1   paragraph      2031 non-null   object \n",
      " 2   problems       2031 non-null   object \n",
      " 3   question_plus  0 non-null      float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 63.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '/data/ephemeral/pro-nlp-generationfornlp-nlp-13'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb528144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     869 non-null    int64 \n",
      " 1   id             869 non-null    object\n",
      " 2   paragraph      869 non-null    object\n",
      " 3   problems       869 non-null    object\n",
      " 4   question_plus  44 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 34.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "test_df.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)\n",
    "test_df[\"choices_len\"] = test_df[\"choices\"].apply(len)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Model load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f13096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4c2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2031 non-null   object\n",
      " 1   paragraph      2031 non-null   object\n",
      " 2   question       2031 non-null   object\n",
      " 3   choices        2031 non-null   object\n",
      " 4   answer         2031 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 95.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a5203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-425</td>\n",
       "      <td>상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...</td>\n",
       "      <td>상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?</td>\n",
       "      <td>[ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-426</td>\n",
       "      <td>(가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...</td>\n",
       "      <td>(가)에 대한 설명으로 옳지 않은 것은?</td>\n",
       "      <td>[고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-427</td>\n",
       "      <td>나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...</td>\n",
       "      <td>(가) 지역에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-428</td>\n",
       "      <td>이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....</td>\n",
       "      <td>밑줄 친 ‘그’에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-429</td>\n",
       "      <td>선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...</td>\n",
       "      <td>(가) 인물이 추진한 정책으로 옳지 않은 것은?</td>\n",
       "      <td>[사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                          paragraph  \\\n",
       "0  generation-for-nlp-425  상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...   \n",
       "1  generation-for-nlp-426  (가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...   \n",
       "2  generation-for-nlp-427  나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...   \n",
       "3  generation-for-nlp-428  이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....   \n",
       "4  generation-for-nlp-429  선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...   \n",
       "\n",
       "                                question  \\\n",
       "0  상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?   \n",
       "1                 (가)에 대한 설명으로 옳지 않은 것은?   \n",
       "2                 (가) 지역에 대한 설명으로 옳은 것은?   \n",
       "3               밑줄 친 ‘그’에 대한 설명으로 옳은 것은?   \n",
       "4             (가) 인물이 추진한 정책으로 옳지 않은 것은?   \n",
       "\n",
       "                                             choices  answer question_plus  \n",
       "0                           [ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]       2          None  \n",
       "1  [고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...       1          None  \n",
       "2  [이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...       4          None  \n",
       "3  [살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...       2          None  \n",
       "4  [사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....       3          None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76243f92",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9b3170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choices_len\n",
       "5    1239\n",
       "4     792\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"choices_len\"] = df[\"choices\"].apply(len)\n",
    "df['choices_len'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b78292",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,        \n",
    "    random_state=42,\n",
    "    stratify=df[\"choices_len\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b964f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choices_len\n",
       "5    0.609836\n",
       "4    0.390164\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"choices_len\"].value_counts(normalize=True)\n",
    "valid_df[\"choices_len\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fcbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "valid_ds = Dataset.from_pandas(valid_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d8707a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18646a5a",
   "metadata": {},
   "source": [
    "### Tokenizer Load\n",
    "- 모델은 굳이 필요 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3c3bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df656a76b33a458ab51bfe88ae8a8f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\"\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3137a",
   "metadata": {},
   "source": [
    "### Message 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5229c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 생성\n",
    "# message 구조로 감싸기\n",
    "# chat_template로 실제 모델 입력 문자열 생성\n",
    "# tokenizer로 input_ids/ attention_mask 생성\n",
    "# Trainer에 넘길 dataset 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d90f51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 System Prompt 함수\n",
    "def get_system_message(row, system_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 하나의 데이터 행\n",
    "    system_prompts: {choices_len: {version: prompt_text}}\n",
    "    prompt_policy: {choices_len: version}\n",
    "    \"\"\"\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    return system_prompts[choices_len][version]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d547137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt 관리 구조 설명\n",
    "\n",
    "# SYSTEM_PROMPTS:\n",
    "# - 실제 system prompt 문구들을 모두 정의해 두는 \"저장소\"\n",
    "# - 문제 유형(choices_len = 4 / 5)별로 나누고, 각 유형 안에서 v1, v2 처럼 여러 버전의 prompt를 보관\n",
    "# - 즉, 여기에는 \"쓸 수 있는 모든 프롬프트 후보\"가 들어 있다.\n",
    "#\n",
    "# 예:\n",
    "# SYSTEM_PROMPTS = {\n",
    "#     4: { \"v1\": \"...\", \"v2\": \"...\" },\n",
    "#     5: { \"v1\": \"...\", \"v2\": \"...\" }\n",
    "# }\n",
    "\n",
    "SYSTEM_PROMPT_4_V1 = (\n",
    "    \"당신은 **지식 추론(Knowledge Inference) 전문가**입니다. \"\n",
    "    \"이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. \"\n",
    "    \"지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 \"\n",
    "    \"가장 타당한 선택지 하나를 고르십시오.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_5_V1 = (\n",
    "    \"당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. \"\n",
    "    \"이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. \"\n",
    "    \"당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\\n\\n\"\n",
    ")\n",
    "\n",
    "# PROMPT_POLICY:\n",
    "# - 이번 실험(run)에서 \"어떤 system prompt 버전을 사용할지\"를 결정하는 설정값\n",
    "# - choices_len(4 or 5) → 사용할 prompt 버전(v1, v2, ...)\n",
    "# - 실험을 바꿀 때는 이 딕셔너리만 수정\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": SYSTEM_PROMPT_4_V1,\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": SYSTEM_PROMPT_5_V1,\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26574201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.\n"
     ]
    }
   ],
   "source": [
    "print(SYSTEM_PROMPTS[4]['v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca98347",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정책별 User Prompt 함수\n",
    "def get_user_message(row, user_prompts, prompt_policy):\n",
    "    \"\"\"\n",
    "    row: 데이터 행\n",
    "    user_prompts: 템플릿 저장소\n",
    "    prompt_policy: 버전 정책\n",
    "    \"\"\"\n",
    "    # 메타 데이터 확인\n",
    "    choices_len = row[\"choices_len\"]\n",
    "    version = prompt_policy[choices_len]\n",
    "    \n",
    "    # 해당 버전의 템플릿 세트 가져오기 (plus, no_plus가 들어있음)\n",
    "    template_set = user_prompts[choices_len][version]\n",
    "    \n",
    "    # 데이터 준비\n",
    "    paragraph = row['paragraph']\n",
    "    question = row['question']\n",
    "    choices_str = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(row['choices'])])\n",
    "    q_plus = row.get('question_plus', None)\n",
    "    \n",
    "    # 분기 처리 및 포맷팅 (여기가 핵심!)\n",
    "    # q_plus가 존재하고, nan이 아닐 때 -> Plus 템플릿 사용\n",
    "    if q_plus and str(q_plus) != 'nan':\n",
    "        return template_set[\"plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question_plus=q_plus, # 여기 들어감\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )\n",
    "    # q_plus가 없을 때 -> No Plus 템플릿 사용\n",
    "    else:\n",
    "        return template_set[\"no_plus\"].format(\n",
    "            paragraph=paragraph,\n",
    "            question=question,\n",
    "            choices=choices_str\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0426fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# User Prompt Templates (V1)\n",
    "# =========================\n",
    "\n",
    "# 4지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 4지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_4_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
    "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
    "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
    "\n",
    "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 있음\n",
    "USER_PROMPT_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 보기\n",
    "{question_plus}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "\n",
    "# 5지선다 + <보기> 없음\n",
    "USER_PROMPT_NO_PLUS_5_V1 = \"\"\"### 지문\n",
    "{paragraph}\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 선택지\n",
    "{choices}\n",
    "\n",
    "### 문제 해결 가이드라인\n",
    "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
    "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
    "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
    "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
    "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
    "\n",
    "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "USER_PROMPTS = {\n",
    "    4: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_4_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_4_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    },\n",
    "    5: {\n",
    "        \"v1\": {\n",
    "            \"plus\": USER_PROMPT_PLUS_5_V1,\n",
    "            \"no_plus\": USER_PROMPT_NO_PLUS_5_V1,\n",
    "        },\n",
    "        # \"v2\": {...}\n",
    "    }\n",
    "}\n",
    "\n",
    "USER_PROMPT_POLICY = {\n",
    "    4: \"v1\",\n",
    "    5: \"v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db822dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1309\n",
      "id: generation-for-nlp-1957\n",
      "choices_len: 5\n",
      "question_plus: None\n",
      "\n",
      "===== SYSTEM =====\n",
      "당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. 이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. 당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\n",
      "\n",
      "\n",
      "\n",
      "===== USER =====\n",
      "### 지문\n",
      "건강분야 연구개발(R&D)에 대한 정부의 투자가 급격히 늘어나고 있다. 하지만 실제 사업화로 연결된 과제는 100개 중 6개도 채 되지 않는 것으로 조사됐다. 수요 조사를 정확하게 하지 않은 상태에서 마구잡이로 연구과제를 선정하는 경우가 많기 때문이다. 일단 받은 예산으로 개발부터 하고 보자는 식의 R&D 문화가 확산돼 있는 것도 원인이다.○리포트로만 남는 국민 세금11일 보건복지부와 과학기술정책연구원, 한국보건산업진흥원 등에 따르면 일상적 건강영역에 대한 정부 R&D 투자액은 2009년 679억원에서 지난해 1178억원으로 늘었다. 과제 수도 444개에서 812개로 증가했다. 만성질환자가 증가하고 건강관리에 대한 관심이 높아진 상황이 반영된 것이다.일상적 건강영역 R&D는 전체 건강 R&D 중 중증질환 치료와 직결된 것을 제외한 것으로 재활, 건강식품, 의료보조기기, 만성질환 연구 등이 모두 포함된다.그러나 이 분야의 정부 R&D 과제 중 실제 사업화로 이어진 과제는 2009년 33개에서 2012년 46개로 늘어나는 데 그쳐 사업화율은 오히려 7.43%에서 5.85%로 떨어졌다. 전체 사업화 건수(하나의 연구에 중복 사업화 포함)도 82건에서 62건으로 줄었다. 고용창출 인원 수도 같은 기간 283명에서 172명으로 감소했다.한 대학 연구소 관계자는 “국가가 지원한 건강관련 R&D 사업의 6% 정도만 사업화로 이어지는 것은 심각하게 봐야 한다”며 “많은 세금을 쏟아부어 개발한 기술과 제품, 서비스 등이 실제 사업으로 이어지지 않고 아무도 거들떠보지 않는 연구 리포트로만 남게 된다는 얘기”라고 지적했다.이는 정부의 전체 R&D 지원 프로젝트의 평균 사업화율 20%보다도 훨씬 낮은 수치다. 건강분야 R&D가 규모를 키워가고 있지만 내실은 없다는 뜻이다. 이는 상품화가 되기까지 넘어야 하는 ‘규제장벽’이 높고 건강증진 효과를 개발자가 입증해야 하는 어려움 등이 걸림돌로 작용하고 있기 때문이라는 분석이다. 환자의 수요보다는 공급자(정부·연구소) 위주로 R&D 과제가 선정되는 점도 실효성을 떨어뜨린다는 비판이다.○평가·검증 R&D ‘빈약’더욱이 정부가 지원한 R&D 예산 대부분이 개발 분야에만 몰린 것으로 나타났다. 2009~2012년 정부 R&D 투자액의 72.7%가 제품·서비스 개발에 쓰였다. 건강관리 기구나 보조기구, 건강식품 개발 등이다.반면 이렇게 개발된 제품과 서비스, 정책 연구결과를 사업으로 연결시킬 수 있는 검증·평가 R&D 비율은 0.6%에 불과했다. 정책·제도 관련 R&D는 18.5%, 건강원인 규명 R&D도 8.1%에 그쳤다. 서지영 과학기술정책연구원 연구위원은 “제대로 된 R&D를 진행하기 위해선 원인 규명과 개발-검증-평가-사업화-신기술 개발의 선순환 과정이 이뤄져야 하는데 현재는 R&D 프로세스 중 개발과 정책 쪽에만 집중되고 있다”고 지적했다.개발분야 중에서도 재활영역 쏠림이 특히 심각했다. 재활은 전체 건강 R&D 지원액의 19.4%를 차지해 매년 증가하고 있지만 최근 환자가 늘고 있는 만성질환 등 질병의 일상적 관리 시스템 연구에 쓰인 돈은 8.3%에 불과했다. 미세먼지 등 공기 중 물질로 발생하는 질병이 증가하고 있지만 이에 따른 인과관계나 관리방안 연구 또한 취약한 상태였다.\n",
      "\n",
      "### 질문\n",
      "정부의 건강분야 R&D 투자액이 2009년과 2012년 사이에 증가한 이유로 가장 적절한 것은 무엇인가?\n",
      "\n",
      "### 선택지\n",
      "1. 만성질환자의 증가와 건강관리에 대한 관심 증가\n",
      "2. R&D 예산의 감소\n",
      "3. 정책 연구의 증가\n",
      "4. 기술 개발의 감소\n",
      "5. 연구소의 수 증가\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
      "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
      "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
      "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
      "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
      "\n",
      "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:\n",
      "\n",
      "... (len: 2012 )\n",
      "\n",
      "===== CHAT TEMPLATE =====\n",
      "<|im_start|>system\n",
      "당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. 이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. 당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "### 지문\n",
      "건강분야 연구개발(R&D)에 대한 정부의 투자가 급격히 늘어나고 있다. 하지만 실제 사업화로 연결된 과제는 100개 중 6개도 채 되지 않는 것으로 조사됐다. 수요 조사를 정확하게 하지 않은 상태에서 마구잡이로 연구과제를 선정하는 경우가 많기 때문이다. 일단 받은 예산으로 개발부터 하고 보자는 식의 R&D 문화가 확산돼 있는 것도 원인이다.○리포트로만 남는 국민 세금11일 보건복지부와 과학기술정책연구원, 한국보건산업진흥원 등에 따르면 일상적 건강영역에 대한 정부 R&D 투자액은 2009년 679억원에서 지난해 1178억원으로 늘었다. 과제 수도 444개에서 812개로 증가했다. 만성질환자가 증가하고 건강관리에 대한 관심이 높아진 상황이 반영된 것이다.일상적 건강영역 R&D는 전체 건강 R&D 중 중증질환 치료와 직결된 것을 제외한 것으로 재활, 건강식품, 의료보조기기, 만성질환 연구 등이 모두 포함된다.그러나 이 분야의 정부 R&D 과제 중 실제 사업화로 이어진 과제는 2009년 33개에서 2012년 46개로 늘어나는 데 그쳐 사업화율은 오히려 7.43%에서 5.85%로 떨어졌다. 전체 사업화 건수(하나의 연구에 중복 사업화 포함)도 82건에서 62건으로 줄었다. 고용창출 인원 수도 같은 기간 283명에서 172명으로 감소했다.한 대학 연구소 관계자는 “국가가 지원한 건강관련 R&D 사업의 6% 정도만 사업화로 이어지는 것은 심각하게 봐야 한다”며 “많은 세금을 쏟아부어 개발한 기술과 제품, 서비스 등이 실제 사업으로 이어지지 않고 아무도 거들떠보지 않는 연구 리포트로만 남게 된다는 얘기”라고 지적했다.이는 정부의 전체 R&D 지원 프로젝트의 평균 사업화율 20%보다도 훨씬 낮은 수치다. 건강분야 R&D가 규모를 키워가고 있지만 내실은 없다는 뜻이다. 이는 상품화가 되기까지 넘어야 하는 ‘규제장벽’이 높고 건강증진 효과를 개발자가 입증해야 하는 어려움 등이 걸림돌로 작용하고 있기 때문이라는 분석이다. 환자의 수요보다는 공급자(정부·연구소) 위주로 R&D 과제가 선정되는 점도 실효성을 떨어뜨린다는 비판이다.○평가·검증 R&D ‘빈약’더욱이 정부가 지원한 R&D 예산 대부분이 개발 분야에만 몰린 것으로 나타났다. 2009~2012년 정부 R&D 투자액의 72.7%가 제품·서비스 개발에 쓰였다. 건강관리 기구나 보조기구, 건강식품 개발 등이다.반면 이렇게 개발된 제품과 서비스, 정책 연구결과를 사업으로 연결시킬 수 있는 검증·평가 R&D 비율은 0.6%에 불과했다. 정책·제도 관련 R&D는 18.5%, 건강원인 규명 R&D도 8.1%에 그쳤다. 서지영 과학기술정책연구원 연구위원은 “제대로 된 R&D를 진행하기 위해선 원인 규명과 개발-검증-평가-사업화-신기술 개발의 선순환 과정이 이뤄져야 하는데 현재는 R&D 프로세스 중 개발과 정책 쪽에만 집중되고 있다”고 지적했다.개발분야 중에서도 재활영역 쏠림이 특히 심각했다. 재활은 전체 건강 R&D 지원액의 19.4%를 차지해 매년 증가하고 있지만 최근 환자가 늘고 있는 만성질환 등 질병의 일상적 관리 시스템 연구에 쓰인 돈은 8.3%에 불과했다. 미세먼지 등 공기 중 물질로 발생하는 질병이 증가하고 있지만 이에 따른 인과관계나 관리방안 연구 또한 취약한 상태였다.\n",
      "\n",
      "### 질문\n",
      "정부의 건강분야 R&D 투자액이 2009년과 2012년 사이에 증가한 이유로 가장 적절한 것은 무엇인가?\n",
      "\n",
      "### 선택지\n",
      "1. 만성질환자의 증가와 건강관리에 대한 관심 증가\n",
      "2. R&D 예산의 감소\n",
      "3. 정책 연구의 증가\n",
      "4. 기술 개발의 감소\n",
      "5. 연구소의 수 증가\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
      "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
      "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
      "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
      "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
      "\n",
      "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "1<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 형식 확인\n",
    "import random\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "\n",
    "# 1) 랜덤 샘플 1개 뽑기\n",
    "idx = random.randrange(len(train_ds))\n",
    "row = train_ds[idx]\n",
    "\n",
    "print(\"idx:\", idx)\n",
    "print(\"id:\", row[\"id\"])\n",
    "print(\"choices_len:\", row[\"choices_len\"])\n",
    "print(\"question_plus:\", row[\"question_plus\"])\n",
    "\n",
    "# 2) system/user 메시지 만들기\n",
    "sys_msg = get_system_message(row, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "user_msg = get_user_message(row, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "\n",
    "print(\"\\n===== SYSTEM =====\")\n",
    "print(sys_msg)\n",
    "\n",
    "print(\"\\n===== USER =====\")\n",
    "print(user_msg)\n",
    "print(\"\\n... (len:\", len(user_msg), \")\")\n",
    "\n",
    "# 3) messages 구성 (정답은 숫자만)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg},\n",
    "    {\"role\": \"assistant\", \"content\": str(row[\"answer\"])},\n",
    "]\n",
    "\n",
    "# 4) chat template 적용\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "print(\"\\n===== CHAT TEMPLATE =====\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0058336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Assistant Prompt 함수\n",
    "def get_assistant_message(row):\n",
    "    \"\"\"\n",
    "    Assistant 메시지 생성 함수.\n",
    "    Qwen3 모델의 토크나이저 템플릿이 자동으로 <think> 태그를 처리하므로,\n",
    "    여기서는 순수한 정답(Label) 텍스트만 반환\n",
    "    \"\"\"\n",
    "    return str(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0561473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    \"\"\"\n",
    "    원본 example(row)로부터 학습용 chat messages를 구성한다.\n",
    "    - choices_len(4/5) 및 question_plus 유무에 따라 system/user 프롬프트를 선택\n",
    "    - assistant는 정답 숫자만\n",
    "    - 이후 평가/추적용으로 id, label도 함께 유지\n",
    "    \"\"\"\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "    asst_msg = get_assistant_message(example)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ],\n",
    "        \"label\": int(example[\"answer\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e947d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-1948',\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': '당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다. 이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다. 당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.\\n\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '### 지문\\n국제축구연맹(FIFA) 뇌물 스캔들 파문이 일파만파 번지고 있다. 미국 법무부의 조사 착수 배경을 놓고 미국과 러시아가 갈등을 빚고 있고, FIFA와 거래해온 대형 금융회사들로까지 조사가 확대될 조짐을 보이고 있다.외신들에 따르면 블라디미르 푸틴 러시아 대통령은 28일(현지시간) 한 언론과의 인터뷰에서 미 법무부의 FIFA 수사와 관련, “누군가가 무언가를 위반했을 것으로 추정할 수 있지만 미국이 이와 무슨 상관이란 말인가”라며 “이번 수사는 국제기구의 운영 원칙에 대한 심각한 침해”라고 목소리를 높였다. 그는 “미국이 제프 블래터 FIFA 회장(사진)의 재선을 막으려 한다는 것은 의심의 여지가 없다”며 “이번 조사가 러시아의 2018년 월드컵 개최에 아무런 영향을 미치지 않아야 한다”고 강조했다. 블래터 회장은 2018년 월드컵 개최지 선정 때 공개적으로 러시아를 지지했다.뉴욕타임스(NYT)는 우크라이나 사태 등으로 러시아와 불편한 관계에 있는 미국이 이번 수사를 통해 러시아를 압박하려는 게 아니냐는 의혹이 일면서 FIFA 뇌물 스캔들이 양국 외교 갈등으로 비화할 조짐을 보이고 있다고 보도했다. 푸틴이 이번 사건을 미국의 러시아 월드컵 개최 저지를 위한 사전 공작쯤으로 해석하고 있다는 것이다.미국은 정치적 의도를 부인했다. 제프리 래스키 미 국무부 대변인은 “이번 수사엔 정치적 목적이 전혀 없으며 부패 행위는 용납할 수 없다는 미국의 명확한 메시지를 보여주는 것일 뿐”이라고 말했다. 래스키 대변인은 “FIFA 간부들에 대한 체포와 기소 역시 이에 초점이 맞춰져 있다”며 “미 법무부가 FIFA의 내부 의사결정 과정에 영향력을 행사하려 한다는 의혹은 근거 없는 주장”이라고 강조했다.마켓워치는 씨티그룹, 뱅크오브아메리카(BoA), JP모간, HSBC, UBS 등 월가 대형은행들이 이번 스캔들과 관련해 미 법무부 조사 대상에 오를 것으로 보인다고 보도했다. 기소된 FIFA 인사들의 돈세탁을 도왔다는 혐의다. 세계 최대 스포츠용품업체 나이키도 브라질 월드컵 대표팀과 스폰서 계약을 체결하면서 3000만달러의 뇌물을 준 혐의로 조사받고 있다.\\n\\n### 질문\\n블라디미르 푸틴 러시아 대통령이 FIFA 수사와 관련하여 주장한 내용 중 어떤 것이 아닌가?\\n\\n### 선택지\\n1. 미국이 FIFA 회장의 재선을 막으려 한다\\n2. 이번 수사는 국제기구의 운영 원칙에 대한 침해이다\\n3. 이번 조사가 러시아의 월드컵 개최에 영향을 미쳐야 한다\\n4. 미국의 수사는 정치적 목적이 없다\\n5. FIFA 간부들에 대한 체포와 기소가 초점이다\\n\\n### 문제 해결 가이드라인\\n1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\\n2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\\n3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\\n4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\\n5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\\n\\n정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\\n정답:'},\n",
       "  {'role': 'assistant', 'content': '3'}],\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_messages(dataset['train'][24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2c3af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(example):\n",
    "    \"\"\"\n",
    "    messages(list[dict])를 tokenizer의 chat_template 규칙에 따라\n",
    "    단일 텍스트로 직렬화한다.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  \n",
    "    )\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f512921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example, truncation=True, max_length=2048, padding=False):\n",
    "    \"\"\"\n",
    "    batched=True면 example[\"text\"]는 List[str]\n",
    "    batched=False면 example[\"text\"]는 str\n",
    "    \"\"\"\n",
    "    tok_kwargs = dict(truncation=truncation, padding=padding)\n",
    "    if truncation is True:\n",
    "        tok_kwargs[\"max_length\"] = max_length\n",
    "\n",
    "    out = tokenizer(example[\"text\"], **tok_kwargs)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": out[\"input_ids\"],\n",
    "        \"attention_mask\": out[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f7344ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len']\n"
     ]
    }
   ],
   "source": [
    "orig_cols = dataset[\"train\"].column_names\n",
    "print(orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a39b27d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b227e736f3824d9fb7f467b45a22cd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab43fde5a51247f0a996cd32734f8a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cd44eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages', 'label'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'messages', 'label'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76bcd8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed2e240236742bb88a88c533ea87f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8345601f81647faa1bad5dc0e0fd355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f3cd70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9255e81894319bfe02d99b0efc6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0620fb42cfbf4eb281d870d2278c87c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ad096e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa15e4a17124d989591c10845efb41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3df7d364394c0fade82d74bce18617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b190b4243d2f4db9813c5917c60dbce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc38908307e54cbfaec135f08bc69717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a7295c4dca4244a79f31a5074714a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16529119a90f4f33880d2e5a72aaffee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 전체 pipeline 다시 확인\n",
    "orig_cols = dataset[\"train\"].column_names\n",
    "dataset_msg = dataset.map(\n",
    "    build_messages,\n",
    "    batched=False,\n",
    "    remove_columns=orig_cols,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "dataset_text = dataset_msg.map(\n",
    "    to_text,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    desc=\"Serialize to text\",\n",
    ")\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"truncation\": True, \"max_length\": 2048, \"padding\": False},\n",
    "    num_proc=4, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d27363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 변환 완료 ===\n",
      "Train 개수: 1726\n",
      "첫 번째 샘플 Keys: dict_keys(['id', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 변환 완료 ===\")\n",
    "print(\"Train 개수:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"첫 번째 샘플 Keys:\", tokenized_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- messages example ---\n",
      "id: generation-for-nlp-1209\n",
      "label: 2\n",
      "[0] role=system\n",
      "당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오. ...\n",
      "\n",
      "[1] role=user\n",
      "### 지문\n",
      "저는 순응하는 마음으로 그리스도교 개혁에 관한 몇 가지 지점들을 취합해 독일 국가의 그리스도인 귀족 앞에 그대로 제시하고자 합니다. 미천한 개인에 불과한 제가 귀족 분들께 이렇게 나서서 말씀드리는 것은 단순한 오만이나 고집 때문이 아닙니다. 모든 그리스도교, 특히 독일의 그리스도교를 억압하는 고통과 비참함으로 인해 저를 비롯한 모든 사람은 울부짖으며 도움을 요청하게 되었습니다… “이 로마 가톨릭 교도들은 자기들 주위로 세 개의 벽을 쌓아 스스로를 보호함으로써 그 누구도 그들을 개혁하지 못했습니다. 이로 인해 모든 기독교권이 크게 추락했습니다… 세속의 권력은 영성에 어떠한 권한도 없다… 성경을 해석할 수 있는 자는 교황이 유일하다… 공의회를 소집할 수 있는 자는 교황이 유일하다… 이제 공의회에서 다루어야 할 문제, 교황과 추기경과 주교와 모든 학식 있는 자가 밤낮으로 골몰해야 할 문제들을 생각해봅시다… 그리스도의 대리자이자 성 베드로의 후계자라고 뽐내는 그리스도교의 수장이 어떠한 왕이나 황제도 따라올 수 없는 세속적 허세 속에 사는 모습을 지켜보는 일은 비통하고 끔찍한 일이 아닐 수 없습니다. 그리스도교에서 ‘추기경’이라고 불리는 사람들은 왜 있는 것입니까? 제가 말씀드리지요. 이탈리아와 독일에는 부유한 수녀회, 기금, 영지, 성직급이 있었는데, 이것들을 로마의 손에 가장 손쉽게 쥐어주기 위해 추기경이란 자리를 만들고 이 추기경에게 주교 관구, 수녀회, 고위 성직자들을 넘겼습니다. 그리고 그 결과 하느님을 섬기는 일이 망가지고 말았습니다.\n",
      "\n",
      "### 질문\n",
      "위 글의 마틴 루터가 밝힌 견해와 유사한 견해를 표명한 개혁가는 누구입니까?\n",
      "\n",
      "### 선택지\n",
      "1. 울리히 츠빙글리\n",
      "2. 토마스 모어 경\n",
      "3. 에라스무스\n",
      "4. 존 위클리프\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
      "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
      "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
      "\n",
      "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답: ...\n",
      "\n",
      "[2] role=assistant\n",
      "2 ...\n",
      "\n",
      "\n",
      "--- text example ---\n",
      "<|im_start|>system\n",
      "당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.<|im_end|>\n",
      "<|im_start|>user\n",
      "### 지문\n",
      "저는 순응하는 마음으로 그리스도교 개혁에 관한 몇 가지 지점들을 취합해 독일 국가의 그리스도인 귀족 앞에 그대로 제시하고자 합니다. 미천한 개인에 불과한 제가 귀족 분들께 이렇게 나서서 말씀드리는 것은 단순한 오만이나 고집 때문이 아닙니다. 모든 그리스도교, 특히 독일의 그리스도교를 억압하는 고통과 비참함으로 인해 저를 비롯한 모든 사람은 울부짖으며 도움을 요청하게 되었습니다… “이 로마 가톨릭 교도들은 자기들 주위로 세 개의 벽을 쌓아 스스로를 보호함으로써 그 누구도 그들을 개혁하지 못했습니다. 이로 인해 모든 기독교권이 크게 추락했습니다… 세속의 권력은 영성에 어떠한 권한도 없다… 성경을 해석할 수 있는 자는 교황이 유일하다… 공의회를 소집할 수 있는 자는 교황이 유일하다… 이제 공의회에서 다루어야 할 문제, 교황과 추기경과 주교와 모든 학식 있는 자가 밤낮으로 골몰해야 할 문제들을 생각해봅시다… 그리스도의 대리자이자 성 베드로의 후계자라고 뽐내는 그리스도교의 수장이 어떠한 왕이나 황제도 따라올 수 없는 세속적 허세 속에 사는 모습을 지켜보는 일은 비통하고 끔찍한 일이 아닐 수 없습니다. 그리스도교에서 ‘추기경’이라고 불리는 사람들은 왜 있는 것입니까? 제가 말씀드리지요. 이탈리아와 독일에는 부유한 수녀회, 기금, 영지, 성직급이 있었는데, 이것들을 로마의 손에 가장 손쉽게 쥐어주기 위해 추기경이란 자리를 만들고 이 추기경에게 주교 관구, 수녀회, 고위 성직자들을 넘겼습니다. 그리고 그 결과 하느님을 섬기는 일이 망가지고 말았습니다.\n",
      "\n",
      "### 질문\n",
      "위 글의 마틴 루터가 밝힌 견해와 유사한 견해를 표명한 개혁가는 누구입니까?\n",
      "\n",
      "### 선택지\n",
      "1. 울리히 츠빙글리\n",
      "2. 토마스 모어 경\n",
      "3. 에라스무스\n",
      "4. 존 위클리프\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
      "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
      "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
      "\n",
      "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "2<|im_end|>\n",
      " ...\n",
      "\n",
      "\n",
      "--- tokenized example ---\n",
      "len: 814\n",
      "decoded preview:\n",
      "<|im_start|>system\n",
      "당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.<|im_end|>\n",
      "<|im_start|>user\n",
      "### 지문\n",
      "저는 순응하는 마음으로 그리스도교 개혁에 관한 몇 가지 지점들을 취합해 독일 국가의 그리스도인 귀족 앞에 그대로 제시하고자 합니다. 미천한 개인에 불과한 제가 귀족 분들께 이렇게 나서서 말씀드리는 것은 단순한 오만이나 고집 때문이 아닙니다. 모든 그리스도교, 특히 독일의 그리스도교를 억압하는 고통과 비참함으로 인해 저를 비롯한 모든 사람은 울부짖으며 도움을 요청하게 되었습니다… “이 로마 가톨릭 교도들은 자기들 주위로 세 개의 벽을 쌓아 스스로를 보호함으로써 그 누구도 그들을 개혁하지 못했습니다. 이로 인해 모든 기독교권이 크게 추락했습니다… 세속의 권력은 영성에 어떠한 권한도 없다… 성경을 해석할 수 있는 자는 교황이 유일하다… 공의회를 소집할 수 있는 자는 교황이 유일하다… 이제 공의회에서 다루어야 할 문제, 교황과 추기경과 주교와 모든 학식 있는 자가 밤낮으로 골몰해야 할 문제들을 생각해봅시다… 그리스도의 대리자이자 성 베드로의 후계자라고 뽐내는 그리스도교의 수장이 어떠한 왕이나 황제도 따라올 수 없는 세속적 허세 속에 사는 모습을 지켜보는 일은 비통하고 끔찍한 일이 아닐 수 없습니다. 그리스도교에서 ‘추기경’이라고 불리는 사람들은 왜 있는 것입니까? 제가 말씀드리지요. 이탈리아와 독일에는 부유한 수녀회, 기금, 영지, 성직급이 있었는데, 이것들을 로마의 손에 가장 손쉽게 쥐어주기 위해 추기경이란 자리를 만들고 이 추기경에게 주교 관구, 수녀회, 고위 성직자들을 넘겼습니다. 그리고 그 결과 하느님을 섬기는 일이 망가지고 말았습니다.\n",
      "\n",
      "### 질문\n",
      "위 글의 마틴 루터가 밝힌 견해와 유사한 견해를 표명한 개혁가는 누구입니까?\n",
      "\n",
      "### 선택지\n",
      "1. 울리히 츠빙글리\n",
      "2. 토마스 모어 경\n",
      "3. 에라스무스\n",
      "4. 존 위클리프\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
      "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
      "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
      "\n",
      "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "2<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"  # 또는 \"validation\"\n",
    "\n",
    "msg_ds = dataset_msg[split]\n",
    "text_ds = dataset_text[split]\n",
    "tok_ds  = tokenized_dataset[split]\n",
    "\n",
    "idx = random.randrange(len(msg_ds))\n",
    "\n",
    "print(\"\\n--- messages example ---\")\n",
    "ex_msg = msg_ds[idx]\n",
    "print(\"id:\", ex_msg[\"id\"])\n",
    "print(\"label:\", ex_msg[\"label\"])\n",
    "\n",
    "for j, m in enumerate(ex_msg[\"messages\"]): \n",
    "    print(f\"[{j}] role={m['role']}\")\n",
    "    print(m[\"content\"], \"...\\n\") \n",
    "\n",
    "print(\"\\n--- text example ---\")\n",
    "ex_text = text_ds[idx][\"text\"]\n",
    "print(ex_text, \"...\\n\")\n",
    "\n",
    "print(\"\\n--- tokenized example ---\")\n",
    "ex_tok = tok_ds[idx]\n",
    "print(\"len:\", len(ex_tok[\"input_ids\"]))\n",
    "print(\"decoded preview:\")\n",
    "print(tokenizer.decode(ex_tok[\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae317d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "당신은 **지식 추론(Knowledge Inference) 전문가**입니다. 이 유형은 정답이 지문에 그대로 쓰여 있지 않을 수 있으며, 지문은 '조건/단서'를 제공합니다. 지문에서 주어진 조건을 정확히 반영하고, 그 조건과 모순되지 않는 범위에서 일반적으로 알려진 지식을 적용해 가장 타당한 선택지 하나를 고르십시오.\n",
      "user\n",
      "### 지문\n",
      "저는 순응하는 마음으로 그리스도교 개혁에 관한 몇 가지 지점들을 취합해 독일 국가의 그리스도인 귀족 앞에 그대로 제시하고자 합니다. 미천한 개인에 불과한 제가 귀족 분들께 이렇게 나서서 말씀드리는 것은 단순한 오만이나 고집 때문이 아닙니다. 모든 그리스도교, 특히 독일의 그리스도교를 억압하는 고통과 비참함으로 인해 저를 비롯한 모든 사람은 울부짖으며 도움을 요청하게 되었습니다… “이 로마 가톨릭 교도들은 자기들 주위로 세 개의 벽을 쌓아 스스로를 보호함으로써 그 누구도 그들을 개혁하지 못했습니다. 이로 인해 모든 기독교권이 크게 추락했습니다… 세속의 권력은 영성에 어떠한 권한도 없다… 성경을 해석할 수 있는 자는 교황이 유일하다… 공의회를 소집할 수 있는 자는 교황이 유일하다… 이제 공의회에서 다루어야 할 문제, 교황과 추기경과 주교와 모든 학식 있는 자가 밤낮으로 골몰해야 할 문제들을 생각해봅시다… 그리스도의 대리자이자 성 베드로의 후계자라고 뽐내는 그리스도교의 수장이 어떠한 왕이나 황제도 따라올 수 없는 세속적 허세 속에 사는 모습을 지켜보는 일은 비통하고 끔찍한 일이 아닐 수 없습니다. 그리스도교에서 ‘추기경’이라고 불리는 사람들은 왜 있는 것입니까? 제가 말씀드리지요. 이탈리아와 독일에는 부유한 수녀회, 기금, 영지, 성직급이 있었는데, 이것들을 로마의 손에 가장 손쉽게 쥐어주기 위해 추기경이란 자리를 만들고 이 추기경에게 주교 관구, 수녀회, 고위 성직자들을 넘겼습니다. 그리고 그 결과 하느님을 섬기는 일이 망가지고 말았습니다.\n",
      "\n",
      "### 질문\n",
      "위 글의 마틴 루터가 밝힌 견해와 유사한 견해를 표명한 개혁가는 누구입니까?\n",
      "\n",
      "### 선택지\n",
      "1. 울리히 츠빙글리\n",
      "2. 토마스 모어 경\n",
      "3. 에라스무스\n",
      "4. 존 위클리프\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문이 주는 조건/단서를 먼저 정리하세요. (무엇을 가정/설명하고 있는지)\n",
      "2. 필요하면 일반적으로 알려진 지식(개념/원리/사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
      "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
      "\n",
      "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "2\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(tokenizer.decode(ex_tok[\"input_ids\"], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cdd5b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "155e8142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1726\n",
      "min: 309\n",
      "max: 1865\n",
      "mean: 887.3823870220162\n",
      "p50: 868.0\n",
      "p75: 1097.75\n",
      "p90: 1384.5\n",
      "p95: 1527.0\n",
      "p99: 1662.0\n"
     ]
    }
   ],
   "source": [
    "### 토큰화된 길이 확인\n",
    "ds = tokenized_dataset[\"train\"]\n",
    "\n",
    "lengths = [len(x) for x in ds[\"input_ids\"]]\n",
    "\n",
    "print(\"count:\", len(lengths))\n",
    "print(\"min:\", min(lengths))\n",
    "print(\"max:\", max(lengths))\n",
    "print(\"mean:\", np.mean(lengths))\n",
    "print(\"p50:\", np.percentile(lengths, 50))\n",
    "print(\"p75:\", np.percentile(lengths, 75))\n",
    "print(\"p90:\", np.percentile(lengths, 90))\n",
    "print(\"p95:\", np.percentile(lengths, 95))\n",
    "print(\"p99:\", np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61100a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in content %}\\n                {%- set reasoning_content = content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set content = content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a986697",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb59b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbd1008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len1: 855\n",
      "len2: 323\n",
      "input_ids shape: torch.Size([2, 855])\n",
      "attention_mask shape: torch.Size([2, 855])\n",
      "labels shape: torch.Size([2, 855])\n",
      "pad_token_id: 151643\n",
      "[batch 0] real_len(attn=1): 855 | num_pad: 0\n",
      "[batch 1] real_len(attn=1): 323 | num_pad: 532\n",
      "[batch 0] ignored(-100): 848\n",
      "[batch 1] ignored(-100): 848\n",
      "\n",
      "--- decoded tail (input_ids) ---\n",
      "사례)을 적용하되, 지문 조건과 모순되면 안 됩니다.\n",
      "3. 선택지 중 조건을 가장 잘 만족하는 것 하나만 고르세요.\n",
      "\n",
      "정답은 1~4 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "4<|im_end|>\n",
      "\n",
      "\n",
      "--- tail labels (show -100 positions) ---\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 19, 151645, 198]\n"
     ]
    }
   ],
   "source": [
    "# train_ds는 id/label 제거한 상태 OK\n",
    "train_ds = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "\n",
    "# 길이가 다른 샘플 2개 뽑기 (패딩을 보려면 반드시 길이 차이가 나야 함)\n",
    "i1, i2 = 0, 1\n",
    "print(\"len1:\", len(train_ds[i1][\"input_ids\"]))\n",
    "print(\"len2:\", len(train_ds[i2][\"input_ids\"]))\n",
    "\n",
    "sample_batch = [train_ds[i1], train_ds[i2]]\n",
    "\n",
    "# collate\n",
    "collated = data_collator(sample_batch)\n",
    "\n",
    "# 텐서 shape 확인 (= 배치 단위로 같은 길이로 패딩 됐는지)\n",
    "print(\"input_ids shape:\", collated[\"input_ids\"].shape)        # (B, L)\n",
    "print(\"attention_mask shape:\", collated[\"attention_mask\"].shape)\n",
    "print(\"labels shape:\", collated[\"labels\"].shape)\n",
    "\n",
    "# 패딩 토큰 개수 확인 (pad_token_id 기준)\n",
    "pad_id = tokenizer.pad_token_id\n",
    "print(\"pad_token_id:\", pad_id)\n",
    "\n",
    "for b in range(collated[\"input_ids\"].shape[0]):\n",
    "    num_pad = (collated[\"input_ids\"][b] == pad_id).sum().item() if pad_id is not None else 0\n",
    "    real_len = collated[\"attention_mask\"][b].sum().item()\n",
    "    print(f\"[batch {b}] real_len(attn=1):\", real_len, \"| num_pad:\", num_pad)\n",
    "\n",
    "# labels 마스킹 확인: -100이 '학습 제외' 영역\n",
    "for b in range(collated[\"labels\"].shape[0]):\n",
    "    num_ignored = (collated[\"labels\"][b] == -100).sum().item()\n",
    "    print(f\"[batch {b}] ignored(-100):\", num_ignored)\n",
    "\n",
    "# 눈으로 보고 싶으면 끝부분 몇 토큰 decode\n",
    "b = 0\n",
    "tail_ids = collated[\"input_ids\"][b][-80:].tolist()\n",
    "tail_lbl = collated[\"labels\"][b][-80:].tolist()\n",
    "\n",
    "print(\"\\n--- decoded tail (input_ids) ---\")\n",
    "print(tokenizer.decode(tail_ids, skip_special_tokens=False))\n",
    "\n",
    "print(\"\\n--- tail labels (show -100 positions) ---\")\n",
    "print(tail_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d961d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 학습 토큰 위치: 848\n",
      "해당 토큰: <think>\n",
      "패딩 영역도 올바르게 -100 처리됨\n"
     ]
    }
   ],
   "source": [
    "# assistant 턴 시작 지점이 labels에서 -100이 아닌 첫 위치인가?\n",
    "first_real = (collated[\"labels\"][0] != -100).nonzero()[0].item()\n",
    "print(\"첫 학습 토큰 위치:\", first_real)\n",
    "print(\"해당 토큰:\", tokenizer.decode([collated[\"input_ids\"][0][first_real]]))\n",
    "# <think> 또는 <|im_start|>assistant\n",
    "\n",
    "# -100 개수가 입력(user) 길이와 일치하는가?\n",
    "# 848개가 -100 → user prompt + system이 대략 이 정도 길이\n",
    "\n",
    "# 패딩 토큰도 -100인가?\n",
    "if pad_id is not None:\n",
    "    pad_positions = (collated[\"input_ids\"][1] == pad_id)\n",
    "    labels_at_pad = collated[\"labels\"][1][pad_positions]\n",
    "    assert (labels_at_pad == -100).all(), \"패딩 위치의 labels가 -100이어야 함\"\n",
    "    print(\"패딩 영역도 올바르게 -100 처리됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b4d42",
   "metadata": {},
   "source": [
    "### Metric 함수 구현\n",
    "\n",
    "본 학습에서는 모델의 성능을 검증하기 위해 **Accuracy(정확도)**와 **Macro-F1 Score**를 평가지표로 사용\n",
    "모델의 출력은 항상 고정된 7개의 토큰 시퀀스를 가지며, 실제 정답(1~5)은 **뒤에서 3번째(Index -3)**에 위치\n",
    "\n",
    "#### 1. 모델 출력 구조 분석 (7 Tokens)\n",
    "학습 데이터는 다음과 같은 구조로 구성되어 있으며, 평가 시 **Index -3**의 로짓(Logits)만을 사용하여 정답을 판별\n",
    "\n",
    "| 순서 (Index) | 역순 (Neg Index) | Token ID | 내용 (Decoded) | 역할 | 비고 |\n",
    "|:---:|:---:|:---:|:---:|:---|:---|\n",
    "| 0 | -7 | 151667 | `<think>` | 생각 시작 | |\n",
    "| 1 | -6 | 271 | `\\n` | 줄바꿈 | Qwen 전용 줄바꿈 |\n",
    "| 2 | -5 | 151668 | `</think>` | 생각 끝 | 내용 없음 |\n",
    "| 3 | -4 | 271 | `\\n` | 줄바꿈 | |\n",
    "| **4** | **-3** | **19 (가변)** | **정답 숫자** | **실제 정답** | **Target (채점 대상)** |\n",
    "| 5 | -2 | 151645 | `<|im_end|>` | 턴 종료 | |\n",
    "| 6 | -1 | 198 | `\\n` | 문장 끝 | 패딩 직전 구분자 |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. 평가 지표 (Metrics)\n",
    "\n",
    "* **Accuracy (정확도)**\n",
    "    * 전체 검증 샘플 중에서 모델이 정답을 정확히 맞춘 비율을 의미\n",
    "    * 가장 직관적인 지표로, 전체적인 성능을 파악하는 데 사용\n",
    "\n",
    "* **Macro-F1 Score**\n",
    "    * 정답 선지(1~5번)를 각각 독립된 클래스로 간주하여 클래스별 F1 Score를 계산한 뒤, 단순 평균(Arithmetic Mean)을 낸 값\n",
    "    * 특정 번호에 정답이 쏠려 있을 경우 발생할 수 있는 편향을 방지하고, 모든 선지를 고르게 잘 맞추는지 평가하기 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da0935b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' -> ids=[16], toks=['1']\n",
      "'2' -> ids=[17], toks=['2']\n",
      "'3' -> ids=[18], toks=['3']\n",
      "'4' -> ids=[19], toks=['4']\n",
      "'5' -> ids=[20], toks=['5']\n",
      "' 1' -> ids=[220, 16], toks=['Ġ', '1']\n",
      "' 2' -> ids=[220, 17], toks=['Ġ', '2']\n",
      "' 3' -> ids=[220, 18], toks=['Ġ', '3']\n",
      "' 4' -> ids=[220, 19], toks=['Ġ', '4']\n",
      "' 5' -> ids=[220, 20], toks=['Ġ', '5']\n",
      "1 -> 16\n",
      "2 -> 17\n",
      "3 -> 18\n",
      "4 -> 19\n",
      "5 -> 20\n"
     ]
    }
   ],
   "source": [
    "### Qwen3 1,2,3,4,5 토큰 id 확인\n",
    "# 가장 확실: encode 결과 보기\n",
    "for s in [\"1\",\"2\",\"3\",\"4\",\"5\",\" 1\",\" 2\",\" 3\",\" 4\",\" 5\"]:\n",
    "    ids = tokenizer.encode(s, add_special_tokens=False)\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"{s!r} -> ids={ids}, toks={toks}\")\n",
    "\n",
    "# 단일 토큰 매핑을 기대한다면 (토크나이저에 따라 None/-1일 수 있음)\n",
    "for tok in [\"1\",\"2\",\"3\",\"4\",\"5\"]:\n",
    "    print(tok, \"->\", tokenizer.convert_tokens_to_ids(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b74d9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 검증용 / 미리 사전 체크\n",
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def assert_answer_pos_is_digit(\n",
    "    ds,                    # train_ds or eval_ds (collator 전에 remove_columns 한 ds)\n",
    "    data_collator,         # DataCollatorForCompletionOnlyLM\n",
    "    n_samples=1000,\n",
    "    seed=42,\n",
    "    pos_from_tail=3,       # -3 규칙\n",
    "    batch_size=16,\n",
    "    verbose_fail=5,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "    n_samples = min(n_samples, len(ds))\n",
    "    idxs = rng.sample(range(len(ds)), n_samples)\n",
    "\n",
    "    fail = 0\n",
    "    shown = 0\n",
    "\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        batch_idxs = idxs[start:start+batch_size]\n",
    "        sample_batch = [ds[i] for i in batch_idxs]\n",
    "        collated = data_collator(sample_batch)\n",
    "\n",
    "        input_ids = collated[\"input_ids\"]          # (B, L)\n",
    "        attn = collated[\"attention_mask\"]          # (B, L)\n",
    "        labels = collated[\"labels\"]                # (B, L)\n",
    "\n",
    "        real_len = attn.sum(dim=1)                 # (B,)\n",
    "        pos = real_len - pos_from_tail             # (B,)\n",
    "\n",
    "        bsz = input_ids.size(0)\n",
    "        for b in range(bsz):\n",
    "            p = int(pos[b].item())\n",
    "            tok = int(input_ids[b, p].item())\n",
    "            lab = int(labels[b, p].item())\n",
    "\n",
    "            ok = (tok in DIGIT_IDS) and (lab in DIGIT_IDS)\n",
    "            if not ok:\n",
    "                fail += 1\n",
    "                if shown < verbose_fail:\n",
    "                    shown += 1\n",
    "                    print(f\"[FAIL] idx={batch_idxs[b]} real_len={int(real_len[b])} pos={p} tok={tok} lab={lab}\")\n",
    "                    # 주변 토큰도 같이 확인\n",
    "                    left = max(0, p-6)\n",
    "                    right = min(input_ids.size(1), p+6)\n",
    "                    print(\"  decoded window:\", tokenizer.decode(input_ids[b, left:right], skip_special_tokens=False))\n",
    "                    # labels에서 digit 토큰이 아예 있는지 확인\n",
    "                    digit_mask = torch.isin(labels[b], torch.tensor(DIGIT_IDS, device=labels.device))\n",
    "                    if digit_mask.any():\n",
    "                        digit_positions = torch.where(digit_mask)[0].tolist()\n",
    "                        print(\"  digit_positions_in_labels:\", digit_positions)\n",
    "                    else:\n",
    "                        print(\"  digit_positions_in_labels: NONE\")\n",
    "\n",
    "    assert fail == 0, f\"pos=real_len-{pos_from_tail} 규칙이 깨진 샘플이 {fail}개 있습니다.\"\n",
    "    print(f\"OK: {n_samples} samples passed. (pos=real_len-{pos_from_tail} is digit token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc38beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 1500 samples passed. (pos=real_len-3 is digit token)\n"
     ]
    }
   ],
   "source": [
    "# collator 돌리기 전에 id/label 제거한 ds 기준\n",
    "train_ds = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_ds  = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])\n",
    "\n",
    "assert_answer_pos_is_digit(train_ds, data_collator, n_samples=1500, pos_from_tail=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb92f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels, pos_from_tail=4):\n",
    "    \"\"\"\n",
    "    반환: (batch, 5)  -> '1'~'5'에 해당하는 logits만 뽑아서 metrics 단계로 전달\n",
    "    \"\"\"\n",
    "    # Trainer가 (logits, ...) 튜플을 줄 때가 있어서 정리\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # (B, L, V)\n",
    "\n",
    "    # labels: (B, L), pad/무시 영역은 -100일 가능성이 큼\n",
    "    # real_len = 마지막으로 labels != -100 인 위치 + 1 로 복원\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "    not_ignored = (labels_t != -100)\n",
    "\n",
    "    # 샘플별로 마지막 not_ignored 위치 찾기\n",
    "    # (뒤에서부터 True 찾기)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)          # (B,)\n",
    "    has_any = not_ignored.any(dim=1)                             # (B,)\n",
    "    # real_len = seq_len - last_true_from_end\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "\n",
    "    # 만약 labels가 전부 -100인 샘플이 있으면(비정상) 그냥 seq_len로 처리\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos = (real_len - pos_from_tail).clamp(min=0, max=seq_len-1) # (B,)\n",
    "\n",
    "    # (B, V)로 해당 위치의 logits만 gather\n",
    "    logits_t = torch.as_tensor(logits)                           # (B, L, V)\n",
    "    batch_idx = torch.arange(logits_t.size(0), device=logits_t.device)\n",
    "    picked = logits_t[batch_idx, pos, :]                         # (B, V)\n",
    "\n",
    "    # digit ids만 슬라이스 -> (B, 5)\n",
    "    picked_digits = picked[:, DIGIT_IDS]\n",
    "    return picked_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e874bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 실행 하지 않아도 됨\n",
    "# # 검증용 GPT 생성 함수\n",
    "# DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def check_minus4_vs_minus3(\n",
    "#     model,\n",
    "#     ds,                 # train_ds or eval_ds (id/label 제거된 것)\n",
    "#     data_collator,      # DataCollatorForCompletionOnlyLM\n",
    "#     n_samples=500,\n",
    "#     seed=42,\n",
    "#     batch_size=16,\n",
    "#     pos_from_tail=3,    # digit 위치가 real_len-3 이라는 네 규칙\n",
    "#     device=None,\n",
    "#     verbose=5,\n",
    "# ):\n",
    "#     model.eval()\n",
    "#     if device is None:\n",
    "#         device = next(model.parameters()).device\n",
    "\n",
    "#     rng = random.Random(seed)\n",
    "#     n_samples = min(n_samples, len(ds))\n",
    "#     idxs = rng.sample(range(len(ds)), n_samples)\n",
    "\n",
    "#     total = 0\n",
    "#     hit_m3 = 0  # logits at p (=-3)로 digit 맞춘 횟수\n",
    "#     hit_m4 = 0  # logits at p-1 (=-4)로 digit 맞춘 횟수\n",
    "\n",
    "#     shown = 0\n",
    "\n",
    "#     for s in range(0, n_samples, batch_size):\n",
    "#         batch_idxs = idxs[s:s+batch_size]\n",
    "#         batch = [ds[i] for i in batch_idxs]\n",
    "#         coll = data_collator(batch)\n",
    "\n",
    "#         input_ids = coll[\"input_ids\"].to(device)         # (B,L)\n",
    "#         attention_mask = coll[\"attention_mask\"].to(device)\n",
    "#         labels = coll[\"labels\"].to(device)               # (B,L)\n",
    "\n",
    "#         # forward\n",
    "#         out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         logits = out.logits                               # (B,L,V)\n",
    "\n",
    "#         real_len = attention_mask.sum(dim=1)              # (B,)\n",
    "#         p = (real_len - pos_from_tail).long()             # digit 토큰 위치 (B,)\n",
    "#         p = torch.clamp(p, min=0, max=input_ids.size(1)-1)\n",
    "\n",
    "#         bsz = input_ids.size(0)\n",
    "#         for b in range(bsz):\n",
    "#             pb = int(p[b].item())\n",
    "#             if pb == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # 정답 digit token id (labels에서 가져오는 게 가장 안전)\n",
    "#             gold = int(labels[b, pb].item())\n",
    "#             if gold not in DIGIT_IDS:\n",
    "#                 # 혹시 label 마스킹/템플릿 변화로 digit이 여기 없으면 스킵\n",
    "#                 continue\n",
    "\n",
    "#             # (1) -4 후보: logits[p-1]에서 DIGIT_IDS 중 argmax\n",
    "#             scores_m4 = logits[b, pb-1, DIGIT_IDS]\n",
    "#             pred_m4 = DIGIT_IDS[int(torch.argmax(scores_m4).item())]\n",
    "\n",
    "#             # (2) -3 후보: logits[p]에서 DIGIT_IDS 중 argmax\n",
    "#             scores_m3 = logits[b, pb, DIGIT_IDS]\n",
    "#             pred_m3 = DIGIT_IDS[int(torch.argmax(scores_m3).item())]\n",
    "\n",
    "#             total += 1\n",
    "#             hit_m4 += (pred_m4 == gold)\n",
    "#             hit_m3 += (pred_m3 == gold)\n",
    "\n",
    "#             if shown < verbose and (pred_m4 != gold or pred_m3 != gold):\n",
    "#                 shown += 1\n",
    "#                 print(f\"[ex] idx={batch_idxs[b]}  real_len={int(real_len[b])}  p(real_len-3)={pb}\")\n",
    "#                 print(f\"     gold={gold}({tokenizer.decode([gold])}) | \"\n",
    "#                       f\"pred_m4={pred_m4}({tokenizer.decode([pred_m4])}) | \"\n",
    "#                       f\"pred_m3={pred_m3}({tokenizer.decode([pred_m3])})\")\n",
    "\n",
    "#     acc_m4 = hit_m4 / max(total, 1)\n",
    "#     acc_m3 = hit_m3 / max(total, 1)\n",
    "\n",
    "#     print(f\"\\nChecked {total} valid samples\")\n",
    "#     print(f\"Acc using logits at p-1 (=-4): {acc_m4:.4f}\")\n",
    "#     print(f\"Acc using logits at p   (=-3): {acc_m3:.4f}\")\n",
    "\n",
    "#     if acc_m4 > acc_m3:\n",
    "#         print(\"=> 결론: digit 예측 위치는 대부분 p-1 (= real_len-4) 쪽이 맞습니다.\")\n",
    "#     else:\n",
    "#         print(\"=> 결론: digit 예측 위치가 p (= real_len-3) 쪽일 가능성이 있습니다. (템플릿/라벨 구조 재확인 필요)\")\n",
    "\n",
    "\n",
    "# train_ds = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "# check_minus4_vs_minus3(\n",
    "#     model=model,\n",
    "#     ds=train_ds,\n",
    "#     data_collator=data_collator,\n",
    "#     n_samples=500,\n",
    "#     batch_size=8,\n",
    "#     pos_from_tail=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17daeb07",
   "metadata": {},
   "source": [
    "- 근데 이렇게 되면 4,5 선지에 따른 macro-f1이 달라지지 않나...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "673cda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, label_pos_from_tail=3):\n",
    "    \"\"\"\n",
    "    eval_pred:\n",
    "      - (predictions, label_ids) 튜플 형태가 가장 흔함\n",
    "      - predictions: preprocess_logits_for_metrics가 반환한 (B, 5)\n",
    "      - label_ids: (B, L) with -100 ignored\n",
    "    반환: {\"accuracy\": ..., \"macro_f1\": ...}\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "    preds_t = torch.as_tensor(preds)\n",
    "    pred_cls = torch.argmax(preds_t, dim=-1).cpu().numpy().astype(np.int64)  # (B,)\n",
    "\n",
    "    labels_t = torch.as_tensor(labels)\n",
    "\n",
    "    not_ignored = (labels_t != -100)\n",
    "    rev = torch.flip(not_ignored, dims=[1])\n",
    "    last_true_from_end = torch.argmax(rev.int(), dim=1)\n",
    "    has_any = not_ignored.any(dim=1)\n",
    "\n",
    "    seq_len = labels_t.size(1)\n",
    "    real_len = seq_len - last_true_from_end\n",
    "    real_len = torch.where(has_any, real_len, torch.full_like(real_len, seq_len))\n",
    "\n",
    "    pos_label = (real_len - label_pos_from_tail).clamp(min=0, max=seq_len - 1)\n",
    "    batch_idx = torch.arange(labels_t.size(0), device=labels_t.device)\n",
    "    gold_tok = labels_t[batch_idx, pos_label].cpu().numpy().astype(np.int64) \n",
    "\n",
    "    gold_cls = gold_tok - DIGIT_IDS[0]  \n",
    "\n",
    "    valid = (gold_cls >= 0) & (gold_cls < 5)\n",
    "    pred_cls = pred_cls[valid]\n",
    "    gold_cls = gold_cls[valid]\n",
    "\n",
    "    acc = (pred_cls == gold_cls).mean() if len(gold_cls) > 0 else 0.0\n",
    "\n",
    "    f1s = []\n",
    "    for c in range(5):\n",
    "        tp = np.sum((pred_cls == c) & (gold_cls == c))\n",
    "        fp = np.sum((pred_cls == c) & (gold_cls != c))\n",
    "        fn = np.sum((pred_cls != c) & (gold_cls == c))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_f1 = float(np.mean(f1s)) if len(f1s) > 0 else 0.0\n",
    "\n",
    "    return {\"accuracy\": float(acc), \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb886d9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62b08981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5da98f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 전에는 반드시 지워야 하는것 -> 'input_ids', 'attention_mask'\n",
    "# 아래 예시처럼\n",
    "train_dataset = tokenized_dataset[\"train\"].remove_columns([\"id\", \"label\"])\n",
    "eval_dataset = tokenized_dataset[\"validation\"].remove_columns([\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fbd52",
   "metadata": {},
   "source": [
    "1.\tbitsandbytes 로딩 성공(여기서 에러 나면 먼저 해결)  ￼\n",
    "2.\t4bit로 모델 로드\n",
    "3.\tk-bit 학습 준비 + LoRA 어댑터 장착\n",
    "4.\tSFTTrainer에 data_collator/metrics 연결\n",
    "5.\t짧은 max_length(1024) + batch1로 100~200 step만 돌려서 OOM/학습 정상동작 확인\n",
    "6.\t그 다음에 길이 늘리기(1536→2048), grad_accum 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89d89553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|> 151643\n",
      "eos_token: <|im_end|> 151645\n",
      "use_cache: False\n",
      "grad_ckpt: True\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"eos_token:\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "# pad_token 없으면 eos로 대체\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 학습 안정/메모리 옵션\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"use_cache:\", model.config.use_cache)\n",
    "print(\"grad_ckpt:\", model.is_gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e4e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90520d06f9fb45a29cf0ec268416f558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # V100이면 fp16 추천\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21574c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer에 pad_token이 이미 있는 상태라면\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2b8624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id: 151643\n",
      "model.config.pad_token_id: 151643\n",
      "gen.pad_token_id: 151643\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer.pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"model.config.pad_token_id:\", model.config.pad_token_id)\n",
    "print(\"gen.pad_token_id:\", model.generation_config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a841c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Attention proj만\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Attention + MLP까지 (성능 더 노리되 trainable 조금 증가)\n",
    "# \"gate_proj\", \"up_proj\", \"down_proj\" -> FFN\n",
    "# target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# target_modules = [\"q_proj\", \"k_proj\"]\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdd49a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"../../qwen-sft-results\",\n",
    "    \n",
    "    # 데이터 및 배치 설정\n",
    "    num_train_epochs=1,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    #  학습률 및 Optimizer (V100)\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                  # V100 필수\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_32bit\",  #  32bit 사용 (안정성) / paged_adamw_8bit -> 더 안된다면 이걸로!\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 검증(Eval) 및 저장(Save) 전략 수정\n",
    "    eval_strategy=\"steps\",      # Epoch 대신 Steps 단위로 검증\n",
    "    eval_steps=20,              # 50 스텝마다 검증 (데이터 양에 따라 조절하세요)\n",
    "    \n",
    "    # 실제 사용 시 주석 해제\n",
    "    save_strategy=\"steps\",      # 검증 주기와 맞춰서 저장 전략도 steps로\n",
    "    save_steps=20,              # 50 스텝마다 저장 시도\n",
    "    \n",
    "    # 저장 용량 관리\n",
    "    save_total_limit=2,         # 체크포인트를 딱 2개만 남기고 옛날 건 자동 삭제!\n",
    "    load_best_model_at_end=True, # 학습 끝나면 \"가장 성능 좋았던 모델\"을 자동으로 로드\n",
    "    metric_for_best_model=\"eval_loss\", # 무엇을 기준으로 최고를 뽑을지 (accuracy 추천)\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # 기타 로깅\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # 시험용이라면???\n",
    "    # max_steps = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a28d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de8d90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 1:00:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.066489</td>\n",
       "      <td>0.826230</td>\n",
       "      <td>0.789266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.053145</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.830697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>0.849180</td>\n",
       "      <td>0.856548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.053688</td>\n",
       "      <td>0.859016</td>\n",
       "      <td>0.869705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.054575</td>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.827479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.062586</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.824387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.059562</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.049281</td>\n",
       "      <td>0.845902</td>\n",
       "      <td>0.854691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.050084</td>\n",
       "      <td>0.865574</td>\n",
       "      <td>0.872092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.050142</td>\n",
       "      <td>0.862295</td>\n",
       "      <td>0.867443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70574fa",
   "metadata": {},
   "source": [
    "- learning rate를 1e-54나 5e-5로 설정하는게 더 좋을듯..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5d294",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4c4536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869 entries, 0 to 868\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     869 non-null    int64 \n",
      " 1   id             869 non-null    object\n",
      " 2   paragraph      869 non-null    object\n",
      " 3   problems       869 non-null    object\n",
      " 4   question_plus  44 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 34.1+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd17f2b3577a4f55ab42a52167809462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e069b6c6ba704360a65cda756512e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build messages:   0%|          | 0/869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fa5553f55b4e639e1612e508b80a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serialize to text:   0%|          | 0/869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "test_df.info()\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)\n",
    "test_df[\"choices_len\"] = test_df[\"choices\"].apply(len)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Model load\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "adapter_path = \"../../qwen-sft-results/checkpoint-216\"# 실제 변경\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "def build_test_messages(example):\n",
    "    sys_msg = get_system_message(example, SYSTEM_PROMPTS, SYSTEM_PROMPT_POLICY)\n",
    "    user_msg = get_user_message(example, USER_PROMPTS, USER_PROMPT_POLICY)\n",
    "\n",
    "    return {\n",
    "        \"id\": example[\"id\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def to_test_text(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, \n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "test_ds_msg = test_dataset.map(\n",
    "    build_test_messages,\n",
    "    batched=False,\n",
    "    desc=\"Build messages\",\n",
    ")\n",
    "test_ds_text = test_ds_msg.map(\n",
    "    to_test_text,\n",
    "    batched=False,\n",
    "    desc=\"Serialize to text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cde11a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'choices_len', 'messages', 'text'],\n",
       "    num_rows: 869\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 총 100개의 샘플에 대해 무작위 테스트를 시작합니다...\n",
      "\n",
      "[1/100] ID: generation-for-nlp-1613\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[2/100] ID: generation-for-nlp-2422\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '지' -> '1'로 대체\n",
      "[3/100] ID: generation-for-nlp-51\n",
      "▶ Raw Output : '소의 획일화가 불가피하다고 주장하는 이들의 관점을 비판하며, 그들의 주장이 적절하지 않다고 말하고 있어.\\n\\n### 정답\\n3\\n\\n### 설명\\n3번은 ⓒ를 고려할 때, \"획일화된 장소에 식상함을 느낀 사람들이 장소의 선택권을 요구했다는 점\"을 근거로 제시하고 있다는 내용을 포함하고 있지만, 지'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[4/100] ID: generation-for-nlp-2723\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[5/100] ID: generation-for-nlp-990\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[6/100] ID: generation-for-nlp-330\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[7/100] ID: generation-for-nlp-243\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[8/100] ID: generation-for-nlp-7\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[9/100] ID: generation-for-nlp-36\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[10/100] ID: generation-for-nlp-319\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[11/100] ID: generation-for-nlp-711\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[12/100] ID: generation-for-nlp-324\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[13/100] ID: generation-for-nlp-1730\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[14/100] ID: generation-for-nlp-88\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[15/100] ID: generation-for-nlp-702\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[16/100] ID: generation-for-nlp-1584\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[17/100] ID: generation-for-nlp-326\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[18/100] ID: generation-for-nlp-115\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[19/100] ID: generation-for-nlp-1554\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[20/100] ID: generation-for-nlp-350\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[21/100] ID: generation-for-nlp-1193\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[22/100] ID: generation-for-nlp-288\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[23/100] ID: generation-for-nlp-25\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[24/100] ID: generation-for-nlp-2204\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[25/100] ID: generation-for-nlp-1709\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[26/100] ID: generation-for-nlp-340\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[27/100] ID: generation-for-nlp-127\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[28/100] ID: generation-for-nlp-471\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[29/100] ID: generation-for-nlp-462\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[30/100] ID: generation-for-nlp-2074\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[31/100] ID: generation-for-nlp-1038\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[32/100] ID: generation-for-nlp-154\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[33/100] ID: generation-for-nlp-1784\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[34/100] ID: generation-for-nlp-928\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[35/100] ID: generation-for-nlp-224\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[36/100] ID: generation-for-nlp-312\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[37/100] ID: generation-for-nlp-609\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[38/100] ID: generation-for-nlp-1516\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[39/100] ID: generation-for-nlp-1429\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[40/100] ID: generation-for-nlp-2361\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[41/100] ID: generation-for-nlp-83\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[42/100] ID: generation-for-nlp-1382\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[43/100] ID: generation-for-nlp-2285\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '심' -> '1'로 대체\n",
      "[44/100] ID: generation-for-nlp-193\n",
      "▶ Raw Output : '나 )의 주제와 관련된 인식을 드러내는군.\\n5. (다 )는 ‘사람의 마음’에 대한 사유를 통해 ‘자기 중심적 사고’를   비판하고,  타인의 관점에서 바라보는 태도를 강조하는군.\\n\\n### 정답\\n5\\n\\n### 설명\\n(다 )는 ‘사람의 마음’에 대한 사유를 통해 ‘자기 중심'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[45/100] ID: generation-for-nlp-351\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[46/100] ID: generation-for-nlp-621\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[47/100] ID: generation-for-nlp-1757\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[48/100] ID: generation-for-nlp-304\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[49/100] ID: generation-for-nlp-1595\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[50/100] ID: generation-for-nlp-219\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[51/100] ID: generation-for-nlp-489\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[52/100] ID: generation-for-nlp-55\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[53/100] ID: generation-for-nlp-354\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[54/100] ID: generation-for-nlp-38\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[55/100] ID: generation-for-nlp-633\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[56/100] ID: generation-for-nlp-997\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[57/100] ID: generation-for-nlp-61\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[58/100] ID: generation-for-nlp-1480\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[59/100] ID: generation-for-nlp-236\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[60/100] ID: generation-for-nlp-66\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '로' -> '1'로 대체\n",
      "[61/100] ID: generation-for-nlp-135\n",
      "▶ Raw Output : ' 배제하세요.\\n</think>\\n\\n### 문제 해결 과정\\n\\n1. **지문 내용 정리**  \\n   - **주제**: 바젤위원회가 제정한 BIS 비율 규제와 그 변화, 국제 금융 시장에서의 규범적 성격.  \\n   - **핵심 내용**:  \\n     - BIS 비율은 국제법적 구속력이 없지만, 국제 금융 시장에서 신뢰를 통해 자발적으로'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[62/100] ID: generation-for-nlp-1015\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[63/100] ID: generation-for-nlp-406\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[64/100] ID: generation-for-nlp-2160\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[65/100] ID: generation-for-nlp-315\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[66/100] ID: generation-for-nlp-1286\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '게' -> '1'로 대체\n",
      "[67/100] ID: generation-for-nlp-380\n",
      "▶ Raw Output : '로운 사건을 기다리며 길을 따라 걷던 김달채는 우산을 이용해 사람들의 반응을 살펴보는 취미를 갖게 된다.\\n4. 김달채는 우산을 이용해 사람들의 반응을 살펴보는 취미를 갖게 되고, 그로 인해 시위 현장에 갔다가  우산 케이스를 보여주며  갑작스럽게'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[68/100] ID: generation-for-nlp-152\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[69/100] ID: generation-for-nlp-1754\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '의' -> '1'로 대체\n",
      "[70/100] ID: generation-for-nlp-21\n",
      "▶ Raw Output : ' 하지 못하고 있다.\\n2. ‘영익’은 가족의 상황을 알고서도 제 생각을 분명히 하지 못하고 있다.\\n3. ‘영익’은 가족의 상황을 알고서도 제 생각을 분명히 하지 못하고 있다.\\n4. ‘영익’은 가족의 상황을 알고서도 제 생각을 분명히 하지 못하고 있다.\\n5. ‘영익’은 가족의'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[71/100] ID: generation-for-nlp-116\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[72/100] ID: generation-for-nlp-1259\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[73/100] ID: generation-for-nlp-285\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '.' -> '1'로 대체\n",
      "[74/100] ID: generation-for-nlp-2\n",
      "▶ Raw Output : ':1로 비교하세요.\\n4. 외부 지식은 배제하고, 지문에 명시된 내용만으로 판단하세요.\\n</think>\\n\\n4. (가 )는 유서의 특성과 의의를 설명하였고 ,  (나 )는 유서 편찬 에서 특정 학문의 수용 양상을 시기별로 소개하였다 .'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[75/100] ID: generation-for-nlp-1674\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[76/100] ID: generation-for-nlp-1013\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[77/100] ID: generation-for-nlp-970\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[78/100] ID: generation-for-nlp-143\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[79/100] ID: generation-for-nlp-961\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[80/100] ID: generation-for-nlp-239\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[81/100] ID: generation-for-nlp-199\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[82/100] ID: generation-for-nlp-276\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "[83/100] ID: generation-for-nlp-1570\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '은' -> '1'로 대체\n",
      "[84/100] ID: generation-for-nlp-180\n",
      "▶ Raw Output : '은 도교의 실용적 측면을 강조하고 ,  ㉡은 도교의 철학적 측면을 강조한다.\\n2. ㉠은 도교의 철학적 측면을 강조하고 ,  ㉡은 도교의 실용적 측면을 강조한다.\\n3. ㉠은 도교의 실용적 측면을 강조하고 ,  ㉡은'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[85/100] ID: generation-for-nlp-86\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[86/100] ID: generation-for-nlp-1117\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '았' -> '1'로 대체\n",
      "[87/100] ID: generation-for-nlp-6\n",
      "▶ Raw Output : '�히는 고증과 의견을 덧붙이는 안설을 포함한 (가 )의 유서 편찬 방식이 반영되었겠지 .\\n3. 서학을 수용한 (나 )의 유서 편찬 방식과 달리,  (가 )의 유서는 서학을 수용하지 않았기 때문에 \\U000f0854임원경제지 \\U000f0855는 서학을 수용하지 않았'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[88/100] ID: generation-for-nlp-980\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[89/100] ID: generation-for-nlp-422\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n4'\n",
      "▶ Extracted  : 4\n",
      "--------------------------------------------------\n",
      "[90/100] ID: generation-for-nlp-11\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[91/100] ID: generation-for-nlp-424\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[92/100] ID: generation-for-nlp-610\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '거' -> '1'로 대체\n",
      "[93/100] ID: generation-for-nlp-179\n",
      "▶ Raw Output : ' 선택지\\n1. 도는 일정하게 고정되어 있지 않기 때문에 때와 상황에 따라 유연하게 변화하는 것이라고 파악했다.\\n2. 도가 가변적이기 때문에 통치술도 고정되어서는 안 된다고 주장했다.\\n3. 도가 가변적이기 때문에 도가 일정한 곳에만 있지 않게 되고 ,  그래야만 도가 모든 사물의 존재와 본질의 근거'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[94/100] ID: generation-for-nlp-171\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[95/100] ID: generation-for-nlp-29\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[96/100] ID: generation-for-nlp-260\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n2'\n",
      "▶ Extracted  : 2\n",
      "--------------------------------------------------\n",
      "[97/100] ID: generation-for-nlp-361\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n3'\n",
      "▶ Extracted  : 3\n",
      "--------------------------------------------------\n",
      "⚠️ [경고] 이상한 값 추출됨: '�' -> '1'로 대체\n",
      "[98/100] ID: generation-for-nlp-128\n",
      "▶ Raw Output : ' 없는 정보는 배제하세요.\\n</think>\\n\\n### 문제 해결 과정\\n\\n1. **지문 요약 및 핵심 정보 정리**:\\n   - 아버지는 작은 가게를 운영하며 생존을 위해 노력하고 있다.\\n   - 아버지는 캐러멜을 한 개 먹은 아들에 대해 화를 내지만, 결국 캐러멜을 주며 용서한다.\\n   - 소주 두 병이 �'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "[99/100] ID: generation-for-nlp-151\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n5'\n",
      "▶ Extracted  : 5\n",
      "--------------------------------------------------\n",
      "[100/100] ID: generation-for-nlp-2359\n",
      "▶ Raw Output : '<think>\\n\\n</think>\\n\\n1'\n",
      "▶ Extracted  : 1\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ 테스트 완료!\n",
      "추출된 정답 분포: ['4', '1', '1', '4', '2', '3', '5', '2', '3', '5', '2', '3', '3', '1', '4', '1', '2', '5', '1', '4', '4', '3', '5', '5', '3', '3', '4', '1', '2', '1', '3', '2', '4', '1', '4', '1', '4', '2', '1', '1', '4', '3', '2', '1', '4', '1', '2', '5', '5', '3', '3', '3', '2', '3', '2', '3', '4', '2', '3', '5', '1', '2', '4', '1', '1', '1', '1', '5', '2', '1', '4', '4', '2', '1', '5', '4', '4', '2', '3', '3', '3', '3', '1', '1', '4', '2', '1', '1', '4', '5', '2', '2', '1', '5', '1', '2', '3', '1', '5', '1']\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "shuffled_ds = test_ds_text.shuffle(seed=42) \n",
    "sample_ds = shuffled_ds.select(range(sample_size))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"총 {sample_size}개의 샘플에 대해 무작위 테스트를 시작합니다...\\n\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, ex in enumerate(sample_ds):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # 생성 (Generate)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,      \n",
    "            do_sample=False,         # Greedy Search (가장 확률 높은 것 선택)\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # 입력 토큰 이후의 생성된 부분만 디코딩\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        #  정답 추출 로직 (작성하신 로직 적용)\n",
    "        try:\n",
    "            # 앞뒤 공백 제거 후 맨 마지막 글자\n",
    "            pred = gen_text.strip()[-1] \n",
    "            \n",
    "            # 유효성 검사\n",
    "            if pred not in ['1', '2', '3', '4', '5']:\n",
    "                print(f\"⚠️ [경고] 이상한 값 추출됨: '{pred}' -> '1'로 대체\")\n",
    "                pred = '1'\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [에러] 파싱 실패 ({e}) -> '1'로 대체\")\n",
    "            pred = '1'\n",
    "\n",
    "        results.append({\n",
    "            \"id\": _id,\n",
    "            \"prediction\": pred\n",
    "        })\n",
    "\n",
    "        # 6. 눈으로 확인하기 (로그 출력)\n",
    "        print(f\"[{i+1}/{sample_size}] ID: {_id}\")\n",
    "        print(f\"▶ Raw Output : {repr(gen_text)}\") # repr()을 쓰면 \\n이 확인 가능\n",
    "        print(f\"▶ Extracted  : {pred}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 최종 결과 확인\n",
    "print(\"\\n✅ 테스트 완료!\")\n",
    "print(f\"추출된 정답 분포: {[r['prediction'] for r in results]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdab4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전한 Generate 추론 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [18:47<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 완료! 총 869개 결과 생성됨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "infer_results = [] # 변수명 통일 (infer_results 대신 results 사용)\n",
    "\n",
    "print(\"안전한 Generate 추론 시작...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for ex in tqdm(test_ds_text):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,      \n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        gen_ids = outputs[0][input_len:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        pred = '1' # 기본값 (최악의 경우 1번 찍기)\n",
    "        \n",
    "        # 텍스트 전체에서 뒤집어서(reversed) 검사\n",
    "        for char in reversed(gen_text):\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                pred = char\n",
    "                break # 숫자를 찾으면 즉시 중단\n",
    "        \n",
    "        # 결과 저장\n",
    "        infer_results.append({\n",
    "            \"id\": _id,\n",
    "            \"answer\": pred\n",
    "        })\n",
    "\n",
    "print(f\" 완료! 총 {len(infer_results)}개 결과 생성됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "99fb557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e6cfd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>generation-for-nlp-1609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>generation-for-nlp-1512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>generation-for-nlp-1382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>generation-for-nlp-702</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>generation-for-nlp-1404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id prediction\n",
       "0       generation-for-nlp-0          5\n",
       "1       generation-for-nlp-1          5\n",
       "2       generation-for-nlp-2          4\n",
       "3       generation-for-nlp-3          5\n",
       "4       generation-for-nlp-4          3\n",
       "..                       ...        ...\n",
       "864  generation-for-nlp-1609          1\n",
       "865  generation-for-nlp-1512          1\n",
       "866  generation-for-nlp-1382          3\n",
       "867   generation-for-nlp-702          4\n",
       "868  generation-for-nlp-1404          1\n",
       "\n",
       "[869 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Logits 기반 추론 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [18:50<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료! 총 869개 결과 생성됨\n",
      "📊 Logits 성공: 869 (100.0%)\n",
      "📊 Fallback 사용: 0 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 이걸로 해보면...??? ###\n",
    "\n",
    "DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'\n",
    "THINK_END_ID = 151668  # </think>\n",
    "\n",
    "def get_answer_from_logits(outputs, input_len):\n",
    "    \"\"\"\n",
    "    output_scores에서 </think> 이후 첫 digit의 logits 확인\n",
    "    \"\"\"\n",
    "    if not hasattr(outputs, 'scores') or not outputs.scores:\n",
    "        return None\n",
    "    \n",
    "    generated_ids = outputs.sequences[0]  # (total_len,)\n",
    "    \n",
    "    # </think> 위치 찾기\n",
    "    think_end_positions = (generated_ids == THINK_END_ID).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    if len(think_end_positions) == 0:\n",
    "        # </think>가 없으면 생성된 토큰 중 첫 digit 찾기\n",
    "        for i in range(input_len, len(generated_ids)):\n",
    "            token_id = generated_ids[i].item()\n",
    "            if token_id in DIGIT_IDS:\n",
    "                step_idx = i - input_len\n",
    "                if 0 <= step_idx < len(outputs.scores):\n",
    "                    step_logits = outputs.scores[step_idx][0]  # (V,)\n",
    "                    digit_logits = step_logits[DIGIT_IDS]  # (5,)\n",
    "                    return digit_logits.argmax().item() + 1  # 1~5\n",
    "        return None\n",
    "    \n",
    "    # </think> 이후 첫 digit 토큰 찾기\n",
    "    think_end_pos = think_end_positions[-1].item()\n",
    "    \n",
    "    for i in range(think_end_pos + 1, len(generated_ids)):\n",
    "        token_id = generated_ids[i].item()\n",
    "        if token_id in DIGIT_IDS:\n",
    "            # 이 토큰이 생성된 step의 logits 확인\n",
    "            step_idx = i - input_len\n",
    "            if 0 <= step_idx < len(outputs.scores):\n",
    "                step_logits = outputs.scores[step_idx][0]  # (V,)\n",
    "                digit_logits = step_logits[DIGIT_IDS]  # (5,)\n",
    "                # 가장 높은 확률의 digit 선택\n",
    "                predicted = digit_logits.argmax().item() + 1  # 1~5\n",
    "                return predicted\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_pred_fallback(text):\n",
    "    \"\"\"\n",
    "    logits에서 실패 시 텍스트 파싱 (fallback)\n",
    "    \"\"\"\n",
    "    if \"</think>\" in text:\n",
    "        after_think = text.split(\"</think>\")[-1]\n",
    "        for char in after_think:\n",
    "            if char in ['1', '2', '3', '4', '5']:\n",
    "                return char\n",
    "    \n",
    "    for keyword in [\"정답:\", \"정답은\", \"Answer:\"]:\n",
    "        if keyword in text:\n",
    "            after_keyword = text.split(keyword)[-1]\n",
    "            for char in after_keyword:\n",
    "                if char in ['1', '2', '3', '4', '5']:\n",
    "                    return char\n",
    "    \n",
    "    for char in reversed(text):\n",
    "        if char in ['1', '2', '3', '4', '5']:\n",
    "            return char\n",
    "    \n",
    "    return '1'\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 추론 실행 (개선 버전)\n",
    "# ==========================================\n",
    "infer_results = []\n",
    "logits_success = 0\n",
    "fallback_used = 0\n",
    "\n",
    "print(\"🚀 Logits 기반 추론 시작...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for ex in tqdm(test_ds_text):\n",
    "        _id = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,       \n",
    "        )\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        \n",
    "        # 1차 시도: logits 기반\n",
    "        pred = get_answer_from_logits(outputs, input_len)\n",
    "        \n",
    "        if pred is not None:\n",
    "            logits_success += 1\n",
    "            pred_str = str(pred)\n",
    "        else:\n",
    "            # 2차 시도: 텍스트 파싱\n",
    "            fallback_used += 1\n",
    "            gen_ids = outputs.sequences[0][input_len:]\n",
    "            gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            pred_str = parse_pred_fallback(gen_text)\n",
    "        \n",
    "        infer_results.append({\n",
    "            \"id\": _id,\n",
    "            \"answer\": pred_str\n",
    "        })\n",
    "\n",
    "print(f\"✅ 완료! 총 {len(infer_results)}개 결과 생성됨\")\n",
    "print(f\"📊 Logits 성공: {logits_success} ({logits_success/len(infer_results)*100:.1f}%)\")\n",
    "print(f\"📊 Fallback 사용: {fallback_used} ({fallback_used/len(infer_results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e54898d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db46b70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>generation-for-nlp-1609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>generation-for-nlp-1512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>generation-for-nlp-1382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>generation-for-nlp-702</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>generation-for-nlp-1404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id prediction\n",
       "0       generation-for-nlp-0          5\n",
       "1       generation-for-nlp-1          5\n",
       "2       generation-for-nlp-2          4\n",
       "3       generation-for-nlp-3          5\n",
       "4       generation-for-nlp-4          3\n",
       "..                       ...        ...\n",
       "864  generation-for-nlp-1609          1\n",
       "865  generation-for-nlp-1512          1\n",
       "866  generation-for-nlp-1382          3\n",
       "867   generation-for-nlp-702          4\n",
       "868  generation-for-nlp-1404          1\n",
       "\n",
       "[869 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2d276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
