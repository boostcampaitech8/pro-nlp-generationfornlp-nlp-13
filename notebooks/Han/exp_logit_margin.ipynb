{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4451fa0e",
   "metadata": {},
   "source": [
    "### 1순위 선택지와 2순위 선택지의 로짓 격차(Margin) 평균 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7652e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..')) \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data.preprocessor import parse_problems_column, add_choices_len\n",
    "from src.prompt.prompt_builder import PromptBuilder, PromptConfig\n",
    "from src.training.model_loader import ModelConfig, load_model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd57f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configs(cfg_dict: Dict[str, Any]) -> tuple:\n",
    "    model_cfg_dict = cfg_dict[\"model\"].copy()\n",
    "    model_cfg_dict[\"use_gradient_checkpointing\"] = False\n",
    "    model_cfg = ModelConfig(**model_cfg_dict)\n",
    "    \n",
    "    prompt_dict = cfg_dict[\"inference\"][\"prompt\"]\n",
    "    prompt_cfg = PromptConfig(\n",
    "        policy=prompt_dict[\"policy\"],\n",
    "        mode=\"test\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    inference_cfg = cfg_dict.get(\"inference\", {})\n",
    "    \n",
    "    return model_cfg, prompt_cfg, inference_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03129bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "        cfg_dict = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf8e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg, prompt_cfg, inference_cfg = create_configs(cfg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b88d4b",
   "metadata": {},
   "source": [
    "### Validation data 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb6987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from ../../data/train.csv...\n",
      "Loaded 2031 rows\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/train.csv'\n",
    "\n",
    "print(f\"\\nLoading data from {data_path}...\")\n",
    "df = pd.read_csv(data_path)\n",
    "df = parse_problems_column(df)\n",
    "df = add_choices_len(df)\n",
    "print(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f6617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data (valid_ratio=0.1, seed=42)...\n",
      "Train: 1827 rows\n",
      "Valid: 204 rows\n"
     ]
    }
   ],
   "source": [
    "valid_ratio = cfg_dict[\"data\"][\"valid_ratio\"]\n",
    "seed = cfg_dict[\"data\"][\"seed\"]\n",
    "\n",
    "print(f\"\\nSplitting data (valid_ratio={valid_ratio}, seed={seed})...\")\n",
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=valid_ratio,\n",
    "    stratify=df[\"choices_len\"],\n",
    "    random_state=seed,\n",
    ")\n",
    "print(f\"Train: {len(train_df)} rows\")\n",
    "print(f\"Valid: {len(valid_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1697472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsat_df 데이터 270개 준비 완료\n"
     ]
    }
   ],
   "source": [
    "lsat_df = pd.read_csv(\"./review_autosave.csv\")\n",
    "lsat_df = lsat_df[lsat_df['keep'] == True]\n",
    "\n",
    "print(f\"lsat_df 데이터 {len(lsat_df)}개 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e03e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# 1. 'choices' 컬럼의 문자열을 리스트 객체로 일괄 변환\n",
    "# 혹시 모를 에러(이미 리스트인 경우 등)를 방지하기 위해 간단한 조건문을 추가합니다.\n",
    "lsat_df['choices'] = lsat_df['choices'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f03c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat_df['choices_len'] = lsat_df['choices'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ea2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat_df = lsat_df.drop(['group_id', 'keep'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d14cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.concat([valid_df, lsat_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82868ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99335b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choices_len\n",
       "5    394\n",
       "4     80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['choices_len'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1689df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cfg = PromptConfig(\n",
    "        policy=cfg_dict[\"prompt\"][\"policy\"],\n",
    "        mode=\"test\",\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11225af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptBuilder ready!\n"
     ]
    }
   ],
   "source": [
    "builder = PromptBuilder(prompt_cfg)\n",
    "print(\"PromptBuilder ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "584c9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text: str, k: int) -> str:\n",
    "    numbers = re.findall(rf'[1-{k}]', str(text))\n",
    "    return numbers[-1] if numbers else \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933b014",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab1913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe0de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../../outputs/reading/final_model...\n",
      "Loading Base Model for Inference: Qwen/Qwen3-14B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c25480acbca4275b6214de9810ea079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA Adapter from: ../../models/qwen3_14B_eng_aug2/final_model_from_serverA\n",
      "Model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adapter_path = \"../../outputs/reading/final_model\"\n",
    "\n",
    "print(f\"Loading model from {adapter_path}...\")\n",
    "model = load_model_inference(model_cfg, \"../../models/qwen3_14B_eng_aug2/final_model_from_serverA\")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5d6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from Qwen/Qwen3-14B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6939cbe8f3c4d5db1a73492c21280f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa03998f54349558b4716032e533cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ac6049c8614f9d8cbf43aa21e33806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef330421d14e758a716f7572062e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {model_cfg.model_name_or_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_cfg.model_name_or_path,\n",
    "    trust_remote_code=model_cfg.trust_remote_code,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb1a90",
   "metadata": {},
   "source": [
    "## Test 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6378b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9de8d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_only_probs_and_margin(step_logits: torch.Tensor, tokenizer, k: int) -> Dict[str, Any]:\n",
    "    digit_tokens = [str(i) for i in range(1, k + 1)]\n",
    "    digit_token_ids = []\n",
    "\n",
    "    for digit in digit_tokens:\n",
    "        encoded = tokenizer.encode(digit, add_special_tokens=False)\n",
    "        if len(encoded) == 1:\n",
    "            digit_token_ids.append(encoded[0])\n",
    "        else:\n",
    "            digit_token_ids.append(encoded[0])\n",
    "\n",
    "    digit_logits = torch.tensor([step_logits[tid].item() for tid in digit_token_ids])\n",
    "    digit_probs = torch.softmax(digit_logits, dim=-1)\n",
    "\n",
    "    top2_values, top2_indices = torch.topk(digit_probs, k=min(2, k))\n",
    "\n",
    "    digit_top1 = str(top2_indices[0].item() + 1)  # 1-indexed\n",
    "    digit_top2 = str(top2_indices[1].item() + 1) if k >= 2 else \"N/A\"\n",
    "\n",
    "    if k >= 2:\n",
    "        digit_margin = (top2_values[0] - top2_values[1]).item()\n",
    "    else:\n",
    "        digit_margin = 0.0\n",
    "\n",
    "    return {\n",
    "        \"digit_probs\": digit_probs.tolist(),\n",
    "        \"digit_margin\": digit_margin,\n",
    "        \"digit_top1\": digit_top1,\n",
    "        \"digit_top2\": digit_top2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd40d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_row_with_retry(\n",
    "    row_dict: Dict[str, Any],\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str,\n",
    "    generated_text: str,\n",
    "    max_new_tokens: int = 30,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    첫 번째 예측의 확률이 낮을 때, 재고려를 유도하는 프롬프트를 추가하여 재생성\n",
    "    \"\"\"\n",
    "    output = builder.build_message(row_dict)\n",
    "    messages = output[\"messages\"]\n",
    "    \n",
    "    # 재시도 프롬프트 추가\n",
    "    retry_assistant = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": generated_text\n",
    "    }\n",
    "\n",
    "    retry_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"다시 한번 신중하게 생각해서 답변해주세요. 다른 접근 방식으로 다시 풀어보세요.\"\n",
    "    }\n",
    "    messages.append(retry_assistant)\n",
    "    messages.append(retry_message)\n",
    "    \n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(device)\n",
    "\n",
    "    k = int(row_dict[\"choices_len\"])\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs.sequences[0][input_len:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 끝에서 2번째 step의 logits 사용 (답변 digit이 나오는 위치)\n",
    "    step_logits = outputs.scores[-2][0]\n",
    "\n",
    "    top5_values, top5_indices = torch.topk(step_logits, k=5)\n",
    "    probs_full = torch.softmax(step_logits, dim=-1)\n",
    "    top5_candidates = []\n",
    "    for rank, (logit_val, token_id) in enumerate(zip(top5_values, top5_indices)):\n",
    "        top5_candidates.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"token_id\": token_id.item(),\n",
    "            \"token\": tokenizer.decode([token_id.item()]),\n",
    "            \"logit\": logit_val.item(),\n",
    "            \"prob_full_vocab\": probs_full[token_id].item(),\n",
    "        })\n",
    "\n",
    "    digit_info = digit_only_probs_and_margin(step_logits, tokenizer, k)\n",
    "    digit_margin = digit_info[\"digit_margin\"]\n",
    "    digit_probs = digit_info[\"digit_probs\"]\n",
    "\n",
    "    predicted_answer = extract_answer(generated_text, k=k)\n",
    "    gold = str(row_dict[\"answer\"])\n",
    "\n",
    "    return {\n",
    "        \"id\": row_dict[\"id\"],\n",
    "        \"choices_len\": k,\n",
    "        \"answer\": gold,\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"is_correct\": predicted_answer == gold,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"is_retry\": True,  # retry 여부 표시\n",
    "\n",
    "        \"top5_candidates\": top5_candidates,\n",
    "\n",
    "        \"digit_probs_1_to_k\": digit_probs,  \n",
    "        \"digit_margin_top1_minus_top2\": digit_margin,\n",
    "        \"digit_top1\": digit_info[\"digit_top1\"],\n",
    "        \"digit_top2\": digit_info[\"digit_top2\"],\n",
    "\n",
    "        \"prompt\": prompt_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "157dc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_row_with_top5(\n",
    "    row_dict: Dict[str, Any],\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str,\n",
    "    max_new_tokens: int = 30,\n",
    ") -> Dict[str, Any]:\n",
    "    output = builder.build_message(row_dict)\n",
    "    messages = output[\"messages\"]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(device)\n",
    "\n",
    "    k = int(row_dict[\"choices_len\"])\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs.sequences[0][input_len:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 끝에서 2번째 step의 logits 사용 (답변 digit이 나오는 위치)\n",
    "    step_logits = outputs.scores[-2][0]\n",
    "\n",
    "    top5_values, top5_indices = torch.topk(step_logits, k=5)\n",
    "    probs_full = torch.softmax(step_logits, dim=-1)\n",
    "    top5_candidates = []\n",
    "    for rank, (logit_val, token_id) in enumerate(zip(top5_values, top5_indices)):\n",
    "        top5_candidates.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"token_id\": token_id.item(),\n",
    "            \"token\": tokenizer.decode([token_id.item()]),\n",
    "            \"logit\": logit_val.item(),\n",
    "            \"prob_full_vocab\": probs_full[token_id].item(),\n",
    "        })\n",
    "\n",
    "    digit_info = digit_only_probs_and_margin(step_logits, tokenizer, k)\n",
    "    digit_margin = digit_info[\"digit_margin\"]\n",
    "    digit_probs = digit_info[\"digit_probs\"]\n",
    "\n",
    "    predicted_answer = extract_answer(generated_text, k=k)\n",
    "    gold = str(row_dict[\"answer\"])\n",
    "\n",
    "    return {\n",
    "        \"id\": row_dict[\"id\"],\n",
    "        \"choices_len\": k,\n",
    "        \"answer\": gold,\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"is_correct\": predicted_answer == gold,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"is_retry\": False,  # retry 여부 표시\n",
    "\n",
    "        \"top5_candidates\": top5_candidates,\n",
    "\n",
    "        \"digit_probs_1_to_k\": digit_probs,  \n",
    "        \"digit_margin_top1_minus_top2\": digit_margin,\n",
    "        \"digit_top1\": digit_info[\"digit_top1\"],\n",
    "        \"digit_top2\": digit_info[\"digit_top2\"],\n",
    "\n",
    "        \"prompt\": prompt_text,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_normal(\n",
    "    df: pd.DataFrame,\n",
    "    builder: PromptBuilder,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: torch.nn.Module,\n",
    "    device: str,\n",
    "    max_new_tokens: int,\n",
    "    desc: str = \"Processing\",\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        row_dict = row.to_dict()\n",
    "        result = generate_for_row_with_top5(\n",
    "            row_dict=row_dict,\n",
    "            builder=builder,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f45e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running inference on VALID set\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Generation:   0%|          | 0/474 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Valid Generation: 100%|██████████| 474/474 [13:53<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Accuracy: 0.9008 (427/474)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Running inference on VALID set\")\n",
    "print(\"=\" * 80)\n",
    "valid_gen_df = process_normal(\n",
    "    df=eval_df,\n",
    "    builder=builder,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    desc=\"Valid Generation\",\n",
    ")\n",
    "\n",
    "valid_acc = valid_gen_df['is_correct'].mean()\n",
    "print(f\"\\nValid Accuracy: {valid_acc:.4f} ({valid_gen_df['is_correct'].sum()}/{len(valid_gen_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7dab0b",
   "metadata": {},
   "source": [
    "### 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dcb0a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 분석 결과 ===\n",
      "맞춘 문제의 Margin 평균: 0.9148\n",
      "틀린 문제의 Margin 평균: 0.5422\n",
      "두 그룹 간의 격차: 0.3726\n",
      "\n",
      "=== 그룹별 상세 통계 ===\n",
      "            count      mean       std       min       25%       50%       75%  \\\n",
      "is_correct                                                                      \n",
      "False        47.0  0.542171  0.300088  0.079724  0.262475  0.547935  0.830488   \n",
      "True        427.0  0.914816  0.197331  0.038727  0.954650  0.994378  0.998993   \n",
      "\n",
      "                 max  \n",
      "is_correct            \n",
      "False       0.998627  \n",
      "True        0.999995  \n"
     ]
    }
   ],
   "source": [
    "# 정답 그룹과 오답 그룹 분리\n",
    "correct_mask = valid_gen_df['is_correct'] == True\n",
    "incorrect_mask = valid_gen_df['is_correct'] == False\n",
    "\n",
    "# 2. 그룹별 digit_margin 평균 계산\n",
    "avg_margin_correct = valid_gen_df.loc[correct_mask, 'digit_margin_top1_minus_top2'].mean()\n",
    "avg_margin_incorrect = valid_gen_df.loc[incorrect_mask, 'digit_margin_top1_minus_top2'].mean()\n",
    "\n",
    "# 3. 결과 출력\n",
    "print(f\"=== 분석 결과 ===\")\n",
    "print(f\"맞춘 문제의 Margin 평균: {avg_margin_correct:.4f}\")\n",
    "print(f\"틀린 문제의 Margin 평균: {avg_margin_incorrect:.4f}\")\n",
    "print(f\"두 그룹 간의 격차: {avg_margin_correct - avg_margin_incorrect:.4f}\")\n",
    "\n",
    "# 4. (선택 사항) 통계적 유의성 확인을 위한 기초 통계량 확인\n",
    "summary = valid_gen_df.groupby('is_correct')['digit_margin_top1_minus_top2'].describe()\n",
    "print(\"\\n=== 그룹별 상세 통계 ===\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
