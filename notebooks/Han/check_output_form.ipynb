{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d35a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data.preprocessor import parse_problems_column, add_choices_len\n",
    "from src.prompt.prompt_builder import PromptBuilder, PromptConfig\n",
    "from src.training.model_loader import ModelConfig, load_model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61e53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configs(cfg_dict: Dict[str, Any]) -> tuple:\n",
    "    model_cfg_dict = cfg_dict[\"model\"].copy()\n",
    "    model_cfg_dict[\"use_gradient_checkpointing\"] = False\n",
    "    model_cfg = ModelConfig(**model_cfg_dict)\n",
    "    \n",
    "    prompt_dict = cfg_dict[\"inference\"][\"prompt\"]\n",
    "    prompt_cfg = PromptConfig(\n",
    "        policy=prompt_dict[\"policy\"],\n",
    "        mode=\"test\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    inference_cfg = cfg_dict.get(\"inference\", {})\n",
    "    \n",
    "    return model_cfg, prompt_cfg, inference_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0f9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config_ax.yaml\", \"r\") as f:\n",
    "    cfg_dict = yaml.safe_load(f)\n",
    "\n",
    "model_cfg, prompt_cfg, inference_cfg = create_configs(cfg_dict)\n",
    "\n",
    "adapter_path = inference_cfg[\"adapter_path\"]\n",
    "output_path = inference_cfg[\"output_path\"]\n",
    "output_logits_path = inference_cfg[\"output_logits_path\"]\n",
    "test_data_path = inference_cfg[\"test_data_path\"]\n",
    "max_new_tokens = inference_cfg.get(\"max_new_tokens\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bfcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess test data.\"\"\"\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    test_df = parse_problems_column(test_df)\n",
    "    test_df = add_choices_len(test_df)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb5bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Loading test data from ./data/test.csv...\n",
      "Loaded 869 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "print(f\"Loading test data from {test_data_path}...\")\n",
    "test_df = load_test_data(\"../../\" + test_data_path)\n",
    "print(f\"Loaded {len(test_df)} rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbe17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287e3f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from skt/A.X-4.0-Light...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {model_cfg.model_name_or_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_cfg.model_name_or_path,\n",
    "    trust_remote_code=model_cfg.trust_remote_code,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16cfbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./models/A.X-4.0-Light/final_model...\n",
      "Loading Base Model for Inference: skt/A.X-4.0-Light\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f442811e7774578a9bc0338c8efa9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA Adapter from: ../.././models/A.X-4.0-Light/final_model\n",
      "Model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model from {adapter_path}...\")\n",
    "model = load_model_inference(model_cfg, \"../../\" + adapter_path)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8760d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptBuilder ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "builder = PromptBuilder(prompt_cfg)\n",
    "print(\"PromptBuilder ready!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efa7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = test_df.iloc[0].to_dict()\n",
    "\n",
    "k = int(row_dict[\"choices_len\"])\n",
    "\n",
    "output = builder.build_message(row_dict)\n",
    "messages = output[\"messages\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    25,      9,  27010,  22290,   1122,   7517,  81183,   3800,   1251,\n",
       "          49185,   8379,    506,    879,     54,    239,    308,  10803,  14181,\n",
       "           7517,  11804,    920,  40652,  28274,   2518,    506,  11806,  64660,\n",
       "           1694,     54,    239,  24223,   9200,   8254,  70532,  93225,     52,\n",
       "          29650,    579,   3597, 101115,  25067,  68241,   5226,  29020,     54,\n",
       "             27,     25,      8,  12961,  40652,    239,  16662,  12882,   6468,\n",
       "          15230,   2399,    746,  12615,  41230,    804,   1253,    298,   2319,\n",
       "          11535,  41230,   1025,   2178,  29988,  18272,    462,   4118,   1025,\n",
       "           6051,  19609,    349,    261,  41230,    878,    308,    791,     54,\n",
       "           2319,   2276,  68896,   1877,  50623,  60960,  28347,   3606,   1211,\n",
       "           1253,    261,   2319,   4392,    298,   1336,   6222,  64016,  87449,\n",
       "          10585,  14104,    804,   1253,    261,   2319,   2276,   5981,  27906,\n",
       "            298,   2681,    434,  21156,   3619,   3397,  64436,  47450,  20846,\n",
       "          50949,    447,   1304,    261,   2681,    434,  24598,   3606,   1211,\n",
       "             54,    261,   2733,  96737,  17961,   2573,   7986,    261,   1336,\n",
       "           2876,  89911,  19630,   3316,   2217,   2319,   2276,    893,   9059,\n",
       "          22137,    341,  19753,    298,  41328,    472,    791,     54,    261,\n",
       "          10008,   1538,   6468,  24966,  22151,   3510,  90188,   8254,   8786,\n",
       "           1408,  39504,   3537,  22151,  16267,     52,    261,  18884,   4007,\n",
       "           1408,  18173,   3537,   2548,    261,  32222,   5999,   3002,     52,\n",
       "            261,  64436,   2630,   4400,  19753,   4435,  11105,   1386,    261,\n",
       "           2366,  43644,  10585,   9296,   1622,  11070,  83295,    472,    791,\n",
       "           1253,   4416,  10585,    366,   2319,   2025,  23245,  25067,   2421,\n",
       "           7381,    632,  24334,    261,  81173,   9296,   1494,  18340,     54,\n",
       "            261,   2319,   2276,  35455,  24334,   7798,    261,   7381,   1972,\n",
       "            261,  22151,  17060,    356,  24334,   7798,   7381,   3000,  85139,\n",
       "            472,    791,   1253,    298,    610,   2441,   1368,  30512, 101115,\n",
       "           2416,    485,  24334,  38391,    472,   2236,     52,    298,  23245,\n",
       "          61037,   2867,  43818,   3492,    323,  12258,   7164,   5337,    261,\n",
       "          83295,   3082,    791,     54,    261,   2844,   1098,   2441,   1368,\n",
       "          96549,   1879,  90188,    261,   3606,    485,  24334,  12671,    472,\n",
       "            791,     54,    261,   2117,   7381,   1460,   8563,    703,  13179,\n",
       "           1400,  25682,   1197,  24334,  12671,    809,  68300,   8323,  19502,\n",
       "            893,  74526,   1253,   9731,   2319,   2276,  47601,    295,   3920,\n",
       "           2319,   6222,  60960,  41230,   6919,  42950,   3082,    261,    791,\n",
       "           1253,    261,   1912,   4738,  43317,   1877,  10783,   6757,    353,\n",
       "          11070,  14443,  10627,  33847,  14443,  37692,  16009,   1622,  22151,\n",
       "           2437,  25734,   3928,   5823,   3626,   1468,     54,    261,   8023,\n",
       "           9013,    323,   4338,  10579,   1828,   4966,  33639,   5414,   4849,\n",
       "            261,    921,  16234,  16690,  15488,   4966,  33639,  88502,   9296,\n",
       "           1972,    261,   2319,   2276,    298,   3233,  33639,  24688,  16770,\n",
       "            139,  39042,    673,    472,    791,     54,    261,   3857,  10585,\n",
       "          12266,   6139,   9683,   6449,   8817,  68896,   1877,   1622,  22151,\n",
       "           2437,  10585,   1828,  28347,    298,  52132, 100954,    893,   2165,\n",
       "          11209,    791,     54,    261,   3233,  14443,  11960,  13064,    261,\n",
       "          17193,    434,  31497,   4619,  27281,   6815,   9620,   1972,    261,\n",
       "           6468,  47959,   2786,    261,  97578,    261,  50623,  10585,   6027,\n",
       "          23930,   3187,   6468,   7336,  68300,  28347,  25734,    261,  15116,\n",
       "             54,   1151,    239,  12961,   7381,    239,  12012,  67853,  29084,\n",
       "          97208,   2053,    261,   1915,     71,    239,    239,  12961,   2538,\n",
       "            346,    239,     57,     54,   1538,   6468,  24966,  22151,   3510,\n",
       "           4338,   1622,  11070,  83295,    472,    791,   1253,    239,     58,\n",
       "             54,   1622,  22151,   5601,  10585,    366,   2319,   2025,  70196,\n",
       "          54287,  11360,   6027,  64395,   1253,    239,     59,     54,   2319,\n",
       "           2276,   2733,  59808,  11082,  17961,   2573,   7986,  23245,   1336,\n",
       "           2876,    261,  89911,  37305,    472,    791,     54,    239,     60,\n",
       "             54,  90188,   8254,  20002,     52,    261,  39504,   1972,    261,\n",
       "          18884,   4007,   1972,    261,  83114,  90188,   3037,  65129,    298,\n",
       "           5509,   4400,  22151,  16267,    804,     54,    239,     61,     54,\n",
       "           2319,   2276,   6468,  20357,    809,   5981,  27906,   2681,    434,\n",
       "          21156,   5509,    261,  23415,   1336,   6222,  64016,   9265,   1211,\n",
       "             54,    239,    239,  12961,   1941,   4087,  91259,    239,     57,\n",
       "             54, 101081,  11211,  10212,   7696,   6647,   4829,   2232,     54,\n",
       "            239,     58,     54,  40210,  22505,   2518,     48,  68684,     55,\n",
       "          27239,     55,  23601,     55,   8580,     55,    387,    346,    810,\n",
       "             49,    344,  18326,  13516,  29460,     54,    239,     59,     54,\n",
       "           1786,   2538,   2921,    579,   5051,   3146,  47642,  18311,  16715,\n",
       "            261,     57,     66,     57,    341,  30332,   2232,     54,    239,\n",
       "             60,     54,    579,  24814,  36361,  20783,    579,   3597,  44481,\n",
       "           1879,   2538,   1443,   5917,   2232,     54,    239,     61,     54,\n",
       "           1770,  17053,  45318,   5375,   2538,    346,  14823,  19993,  87104,\n",
       "             54,    239,    239,  69382,    261,     57,    134,     61,    746,\n",
       "           7718,    624,   7865,    463,  18631,   2232,     54,   1622,   2417,\n",
       "           2276,  18631,   1386,  15107,     54,    239,  27834,     66,     27,\n",
       "             25,     10,     61,     27]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9851e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "generated_ids = output_ids.sequences[0][inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.convert_ids_to_tokens(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea9ac79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 텍스트:\n",
      "<|im_start|><|system|>당신은 논리적인 **텍스트 분석 및 독해 전문가**입니다.\n",
      "이 문제는 오직 **제공된 지문 내의 정보**만으로 풀어야 합니다.\n",
      "당신의 외부 배경지식을 배제하고, 철저하게 지문에 명시된 내용에 근거하여 판단하십시오.<|im_end|><|im_start|><|user|>### 지문\n",
      "사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다.   독서의 즐거움에는 여러 가지가 있겠지만 그 중심에는 ‘소통의  즐거움’이 있다. 독자는 독서를 통해 책과 소통하는 즐거움을 경험한다.  독서는   필자와 간접적으로 대화하는 소통 행위이다.  독자는 자신이 속한   사회나 시대의 영향 아래 필자가 속해 있거나 드러내고자 하는  사회나 시대를 경험한다.  직접 경험하지 못했던 다양한 삶을  필자를 매개로 만나고 이해하면서 독자는 더 넓은 시야로 세계를   바라볼 수 있다.  이때 같은 책을 읽은 독자라도 독자의 배경 지식이나 관점 등의 독자 요인,  읽기 환경이나 과제 등의 상황  요인이 다르므로,  필자가 보여 주는 세계를 그대로 수용하지  않고 저마다 소통 과정에서 다른 의미를 구성할 수 있다. 이러한 소통은 독자가 책의 내용에 대해 질문하고 답을  찾아내는 과정에서 가능해진다.  독자는 책에서 답을 찾는  질문,  독자 자신에게서 답을 찾는 질문 등을 제기할 수 있다.   전자의 경우 책에 명시된 내용에서 답을 발견할 수 있고,   책의 내용들을 관계 지으며 답에 해당하는 내용을 스스로  구성할 수도 있다.  또한 후자의 경우 책에는 없는 독자의  경험에서 답을 찾을 수 있다.  이런 질문들을 풍부히 생성 하고 주체적으로 답을 찾을 때 소통의 즐거움은 더 커진다. 한편 독자는 ㉠다른 독자와 소통하는 즐거움 을 경험할 수도  있다.  책과의 소통을 통해 개인적으로 형성한 의미를 독서 모임 이나 독서 동아리 등에서 다른 독자들과 나누는 일이 이에 해당 한다.  비슷한 해석에 서로 공감하며 기존 인식을 강화하거나  관점의 차이를 확인하고 기존 인식을 조정하는 과정에서,  독자는   자신의 인식을 심화 ･확장할 수 있다.  최근 소통 공간이 온라인 으로 확대되면서 독서를 통해 다른 독자들과 소통하며 즐거움을   누리는 양상이 더 다양해지고 있다.  자신의 독서 경험을 담은  글이나 동영상을 생산･ 공유함으로써,  책을 읽지 않은  타인이  책과 소통하도록 돕는 것도 책을 통한 소통의 즐거움을 나누는  일이다. \n",
      "\n",
      "### 질문\n",
      "윗글의 내용과 일치하지 않는  것은?\n",
      "\n",
      "### 선택지\n",
      "1. 같은 책을 읽은 독자라도 서로 다른 의미를 구성할 수 있다.\n",
      "2. 다른 독자와의 소통은 독자가 인식의 폭을 확장하도록 돕는다.\n",
      "3. 독자는 직접 경험해 보지 못했던 다양한 삶을 책의 필자를  매개로 접할 수 있다.\n",
      "4. 독자의 배경지식,  관점,  읽기 환경,  과제는 독자의 의미 구성에   영향을 주는 독자 요인이다.\n",
      "5. 독자는 책을 읽을 때 자신이 속한 사회나 시대의 영향을  받으며 필자와 간접적으로 대화한다.\n",
      "\n",
      "### 문제 해결 가이드라인\n",
      "1. 지문을 끝까지 읽고 핵심 정보를 정리하세요.\n",
      "2. 질문이 요구하는 정보(수치/인물/원인/결과/요지 등)가 무엇인지 정확히 확인하세요.\n",
      "3. 각 선택지가 지문의 어느 부분과 일치하는지 1:1로 대조하세요.\n",
      "4. 지문과 모순되거나 지문에 근거가 없는 선택지는 제외하세요.\n",
      "5. 가장 확실한 근거를 가진 선택지 번호 하나만 선택하세요.\n",
      "\n",
      "정답은 1~5 중 하나의 정수로만 출력하세요. 다른 글자는 출력하지 마세요.\n",
      "정답:<|im_end|><|im_start|><|assistant|>5<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "full_text = tokenizer.decode(output_ids.sequences[0], skip_special_tokens=False)\n",
    "print(f\"전체 텍스트:\\n{full_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
