import sys
import os
import yaml
import pandas as pd
import numpy as np
import torch
from ast import literal_eval
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from src.models.dual_model import DualModel
from src.data.data_loader import DataLoader 
from src.data.classifier import QuestionClassifier
from src.data.prompt_formatter import PromptFormatter
from src.data.dataset_processor import DatasetProcessor
from src.data.tokenizer_wrapper import TokenizerWrapper
from src.data.collator import CollatorFactory
from src.models.model_loader import ModelLoader
from src.models.lora_config import LoraConfigFactory
from src.training.metrics import get_preprocess_logits_for_metrics, get_compute_metrics
from src.training.base_trainer import BaseSFTTrainer
from src.models.knowledge_model import KnowledgeModel
from src.models.inferential_model import InferentialModel
from src.utils.seed import set_seed



def load_configs():
    """
    config ë””ë ‰í† ë¦¬ì˜ YAML íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ì„¤ì • ì •ë³´ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        data_cfg, model_cfg, inference_cfg ì„¤ì •ì„ ë‹´ì€ íŠœí”Œ
    """
    with open("config/data_config.yaml", "r") as f:
        data_cfg = yaml.safe_load(f)
    with open("config/model_config.yaml", "r") as f:
        model_cfg = yaml.safe_load(f)
    with open("config/inference_config.yaml", "r") as f:
        inference_cfg = yaml.safe_load(f)
    return data_cfg, model_cfg, inference_cfg

DIGIT_IDS = [16, 17, 18, 19, 20]  # '1'~'5'
THINK_END_ID = 151668  # </think>

def get_answer_from_logits(outputs, input_len):
    """
    output_scoresì—ì„œ </think> ì´í›„ ì²« digitì˜ logits í™•ì¸
    """
    if not hasattr(outputs, 'scores') or not outputs.scores:
        return None
    
    generated_ids = outputs.sequences[0]  # (total_len,)
    
    # </think> ìœ„ì¹˜ ì°¾ê¸°
    think_end_positions = (generated_ids == THINK_END_ID).nonzero(as_tuple=True)[0]
    
    if len(think_end_positions) == 0:
        # </think>ê°€ ì—†ìœ¼ë©´ ìƒì„±ëœ í† í° ì¤‘ ì²« digit ì°¾ê¸°
        for i in range(input_len, len(generated_ids)):
            token_id = generated_ids[i].item()
            if token_id in DIGIT_IDS:
                step_idx = i - input_len
                if 0 <= step_idx < len(outputs.scores):
                    step_logits = outputs.scores[step_idx][0]  # (V,)
                    digit_logits = step_logits[DIGIT_IDS]  # (5,)
                    return digit_logits.argmax().item() + 1  # 1~5
        return None
    
    # </think> ì´í›„ ì²« digit í† í° ì°¾ê¸°
    think_end_pos = think_end_positions[-1].item()
    
    for i in range(think_end_pos + 1, len(generated_ids)):
        token_id = generated_ids[i].item()
        if token_id in DIGIT_IDS:
            # ì´ í† í°ì´ ìƒì„±ëœ stepì˜ logits í™•ì¸
            step_idx = i - input_len
            if 0 <= step_idx < len(outputs.scores):
                step_logits = outputs.scores[step_idx][0]  # (V,)
                digit_logits = step_logits[DIGIT_IDS]  # (5,)
                # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ digit ì„ íƒ
                predicted = digit_logits.argmax().item() + 1  # 1~5
                return predicted
    
    return None


def parse_pred_fallback(text):
    """
    logitsì—ì„œ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ íŒŒì‹± (fallback)
    """
    if "</think>" in text:
        after_think = text.split("</think>")[-1]
        for char in after_think:
            if char in ['1', '2', '3', '4', '5']:
                return char
    
    for keyword in ["ì •ë‹µ:", "ì •ë‹µì€", "Answer:"]:
        if keyword in text:
            after_keyword = text.split(keyword)[-1]
            for char in after_keyword:
                if char in ['1', '2', '3', '4', '5']:
                    return char
    
    for char in reversed(text):
        if char in ['1', '2', '3', '4', '5']:
            return char
    
    return '1'

def to_test_text( example, tokenizer):
    text = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=True, 
    )
    return {"text": text}

def main():
    """
    ì „ì²´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸(ì„¤ì • ë¡œë“œ, ëª¨ë¸ ì´ˆê¸°í™”, ë°ì´í„° ì²˜ë¦¬, ì¶”ë¡ , ì €ì¥)ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # 1. Config ë¡œë“œ, seed ì„¤ì •, wandb ì´ˆê¸°í™”
    data_cfg, model_cfg, inference_cfg = load_configs()

    set_seed(42)
    
    # 2. Dual Model ì´ˆê¸°í™”
    print(">>> Loading Dual Models...")
    # dual_model = DualModel(
    #     inferential_ckpt=inference_cfg['models']['inferential']['checkpoint_path'],
    #     knowledge_ckpt=inference_cfg['models']['knowledge']['checkpoint_path'],
    #     device_map='auto'
    # )
    k_model = KnowledgeModel(inference_cfg['models']['knowledge']['checkpoint_path'])
    i_model = InferentialModel(inference_cfg['models']['inferential']['checkpoint_path'])
    tokenizer = k_model.get_tokenizer()

    # 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    loader = DataLoader(data_cfg['data']['test_csv'])
    test_df = loader.load_and_flatten()
    classifier = QuestionClassifier()
    test_df = classifier.classify_dataset(test_df)

    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")

    # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° Dataset ë³€í™˜
    prompt_formatter = PromptFormatter()
    processor = DatasetProcessor(prompt_formatter)
    test_dataset = processor.process(test_df, is_test=True)

    test_ds_text = test_dataset.map(
        to_test_text,
        fn_kwargs={"tokenizer": tokenizer} # ì—¬ê¸°ì— ì „ë‹¬í•  ì¸ìë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë„£ìŒ
    )
    print(test_ds_text)
    # 4. ì¶”ë¡  ì‹¤í–‰
    print(">>> ì¶”ë¡  ì‹œì‘!")
    infer_results = []
    logits_success = 0
    fallback_used = 0

    print("ğŸš€ Logits ê¸°ë°˜ ì¶”ë¡  ì‹œì‘...")

    k_model = k_model.get_model()
    i_model = i_model.get_model()
    k_model.eval()
    i_model.eval()
    with torch.inference_mode():
        for ex in tqdm(test_ds_text):
            _id = ex["id"]
            text = ex["text"]
            

            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=4096,
            ).to("cuda")

            if ex['question_type'] == 'knowledge':
                outputs = k_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    return_dict_in_generate=True,
                    output_scores=True,       
                )
            else:
                outputs = i_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    return_dict_in_generate=True,
                    output_scores=True,       
                )

            input_len = inputs["input_ids"].shape[-1]
            
            # 1ì°¨ ì‹œë„: logits ê¸°ë°˜
            pred = get_answer_from_logits(outputs, input_len)
            
            if pred is not None:
                logits_success += 1
                pred_str = str(pred)
            else:
                # 2ì°¨ ì‹œë„: í…ìŠ¤íŠ¸ íŒŒì‹±
                fallback_used += 1
                gen_ids = outputs.sequences[0][input_len:]
                gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
                pred_str = parse_pred_fallback(gen_text)
            
            infer_results.append({
                "id": _id,
                "answer": pred_str
            })

    # 5. ê²°ê³¼ ì €ì¥
    output_path = inference_cfg['paths']['output']
    pd.DataFrame(infer_results).to_csv(output_path, index=False)
    print(f">>> ê²°ê³¼ ì €ì¥ ì™„ë£Œ! : {output_path}")

if __name__ == "__main__":
    main()