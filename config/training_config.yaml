training:
  # 공통 설정
  common:
    do_train: true
    do_eval: true
    max_seq_length: 1024
    lr_scheduler_type: "cosine"
    weight_decay: 0.01
    logging_steps: 100
    save_strategy: "epoch"
    eval_strategy: "epoch"
    save_total_limit: 1
    save_only_model: true
    report_to: "none"
    metric_for_best_model: "f1"
    greater_is_better: true
  
  # 4지선다(지식형) 학습 설정
  knowledge:
    output_dir: "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/result/knowledge_decoder"
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    # gradient_accumulation_steps: 4
    num_train_epochs: 3
    learning_rate: 2e-5
  
  # 5지선다(독해형) 학습 설정
  inferential:
    output_dir: "/data/ephemeral/pro-nlp-generationfornlp-nlp-13/result/inferential_decoder"
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    # gradient_accumulation_steps: 4
    num_train_epochs: 3
    learning_rate: 2e-5