model:
  model_name_or_path: "Qwen/Qwen3-8B"
  use_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  compute_dtype: "float16"
  device_map: "auto"
  use_gradient_checkpointing: true
  trust_remote_code: true

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: "all-linear"
  bias: "none"
  task_type: "CAUSAL_LM"

trainer:
  output_dir: "./models/qwen3_8B"
  max_seq_length: 2048
  packing: false

  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4

  learning_rate: 0.0002
  lr_scheduler_type: "cosine"
  optim: "paged_adamw_32bit"
  weight_decay: 0.01
  warmup_ratio: 0.05
  max_grad_norm: 1.0

  fp16: true
  bf16: false

  eval_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 10
  eval_steps: 40
  save_steps: 40

  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: "eval_macro_f1"
  greater_is_better: true

  report_to: "wandb"
  seed: 42
  remove_unused_columns: true

data:
  train_path: "./data/train.csv"
  test_path: "./data/test.csv"
  valid_ratio: 0.1
  seed: 42
  do_split: true

prompt:
  templates_dir: "./src/prompt/templates"
  verbose: false
  policy:
    system:
      4: "v2"
      5: "v1"
    user:
      4: "v2"
      5: "v1"

tokenizer:
  train:
    max_length: 2048
    padding: false
    truncation: true
    add_generation_prompt: false
  gen:
    max_length: 2048
    padding: "max_length"
    truncation: true
    add_generation_prompt: true

evaluator:
  enabled: true
  output_path: "./outputs/valid_predictions.csv"
  max_new_tokens: 100
  return_logits: false

wandb:
  enabled: true
  project_name: "knowledge-prompt-compare"
  entity: "pro-nlp-generationfornlp-nlp-13"
  run_name: "guidelines_step_by_step"

inference:
  model:
    use_gradient_checkpointing: false
  
  adapter_path: "./models/qwen3_8B/final_model"
  test_data_path: "./data/train_logic.csv"
  output_path: "./logic/logic_submission_test_with_v1.csv"
  max_new_tokens: 100
  do_sample: false
  
  prompt:
    policy:
      system:
        4: "v2"
        5: "v1"
      user:
        4: "v2"
        5: "v1"

dpo:
  sft_adapter_path: "./models/qwen3_8B/final_model"

  trainer:
    output_dir: "./outputs/qwen3_8B_dpo"

    num_train_epochs: 2
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 4

    learning_rate: 0.000005
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1

    beta: 0.1
    label_smoothing: 0.0
    loss_type: "sigmoid"

    fp16: true
    bf16: false

    eval_strategy: "steps"
    save_strategy: "steps"
    logging_steps: 5
    eval_steps: 20
    save_steps: 20

    load_best_model_at_end: true
    metric_for_best_model: "eval_loss"
    greater_is_better: false
    save_total_limit: 2

    max_length: 2048
    max_prompt_length: 1820
    max_completion_length: 30

    remove_unused_columns: false
    report_to: "wandb"
    seed: 42

  data:
    train_path: "./data/dpo_train.jsonl"
    eval_path: "./data/dpo_eval.jsonl"
    seed: 42
    csv_dir: "./data/dpo_outputs"
    margin_threshold: 0.995
    eval_ratio: 0.1