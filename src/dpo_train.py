import yaml
import argparse
from pathlib import Path
from typing import Dict, Any

import torch
from transformers import AutoTokenizer

from src.training.model_loader import ModelConfig
from src.training.dpo_trainer import DPOTrainerConfig, build_dpo_trainer_option3
from src.data.dpo_data_loader import DPODataConfig, load_dpo_dataset
from src.utils.seed import set_seed
from src.utils.wandb import wandb_init, wandb_finish


def main(
    model_cfg: ModelConfig,
    dpo_cfg: DPOTrainerConfig,
    dpo_data_cfg: DPODataConfig,
    sft_adapter_path: str,
    wandb_cfg: Dict[str, Any] = None,
):

    set_seed(dpo_cfg.seed)
    print(f"Random seed set to: {dpo_cfg.seed}")

    if wandb_cfg and wandb_cfg.get("enabled", False):
        wandb_init(
            config={
                "model": model_cfg.__dict__,
                "dpo": dpo_cfg.__dict__,
                "dpo_data": dpo_data_cfg.__dict__,
                "sft_adapter_path": sft_adapter_path,
            },
            project_name=wandb_cfg.get("project_name", "qwen3-dpo"),
            run_name=wandb_cfg.get("run_name") or f"dpo_{Path(dpo_cfg.output_dir).name}",
            entity=wandb_cfg.get("entity"),
        )

    print(f"Loading tokenizer from {model_cfg.model_name_or_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_cfg.model_name_or_path,
        trust_remote_code=model_cfg.trust_remote_code,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"pad_token not found, set to eos_token: {tokenizer.eos_token}")

    print("Loading DPO datasets...")
    datasets = load_dpo_dataset(dpo_data_cfg)

    print(f"Train dataset: {len(datasets['train'])}")
    if "validation" in datasets:
        print(f"Validation dataset: {len(datasets['validation'])}")

    print(f"Building DPO trainer with SFT adapter: {sft_adapter_path}")
    trainer = build_dpo_trainer_option3(
        dpo_cfg=dpo_cfg,
        model_cfg=model_cfg,
        sft_adapter_path=sft_adapter_path,
        tokenizer=tokenizer,
        train_dataset=datasets["train"],
        eval_dataset=datasets.get("validation"),
    )

    print("=" * 60)
    print("DPO Training Start")
    print("=" * 60)
    trainer.train()

    final_model_path = Path(dpo_cfg.output_dir) / "final_model"
    trainer.save_model(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))

    if wandb_cfg and wandb_cfg.get("enabled", False):
        wandb_finish()

    print("-" * 60)
    print("DPO Training Complete")
    print(f"Final model saved at: {final_model_path}")
    print("-" * 60)


def create_configs(cfg_dict: Dict[str, Any]):
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }

    model_dict = cfg_dict["model"].copy()
    if "compute_dtype" in model_dict and isinstance(model_dict["compute_dtype"], str):
        model_dict["compute_dtype"] = dtype_map.get(
            model_dict["compute_dtype"],
            torch.float16
        )

    model_cfg = ModelConfig(**model_dict)
    dpo_cfg = DPOTrainerConfig(**cfg_dict["dpo"]["trainer"])
    dpo_data_cfg = DPODataConfig(**cfg_dict["dpo"]["data"])
    sft_adapter_path = cfg_dict["dpo"]["sft_adapter_path"]

    return model_cfg, dpo_cfg, dpo_data_cfg, sft_adapter_path


def load_config(config_path: str) -> Dict[str, Any]:
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DPO Training for Qwen3-14B")
    parser.add_argument(
        "--config",
        type=str,
        default="./config.yaml",
        help="Path to DPO config YAML file"
    )
    args = parser.parse_args()

    raw_cfg = load_config(args.config)
    model_cfg, dpo_cfg, dpo_data_cfg, sft_adapter_path = create_configs(raw_cfg)

    main(
        model_cfg=model_cfg,
        dpo_cfg=dpo_cfg,
        dpo_data_cfg=dpo_data_cfg,
        sft_adapter_path=sft_adapter_path,
        wandb_cfg=raw_cfg.get("wandb"),
    )
